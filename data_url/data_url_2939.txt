For early releases we can have this hard coded.
The precision should be distinguished from fidelity of gate in documentation, i.e. it is the max precision that the system can handle in terms of what it is trying to do, not the actual precision for the operation.
Is it hardcoded now somewhere?
Currently we use floats and the details of the accuracy are obscured from the user.  This is likely fine since the accuracy tends to not be an important factor for users at this level of control.
We should close until we have a concrete case where the accuracy matters to end users (and then it might just be important to inform the users, not use a type thank captures the degree of precision that is achievable).
