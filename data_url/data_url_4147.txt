ProjectQ would be made even better by keeping its public interfaces agnostic to complex-valued physically nonobservable probability amplitudes. The compiler already can be used this way, as is demonstrated by its compatibility with the genuine quantum hardware of the IBM back end. However, reimplementing the unit tests in a probability amplitude agnostic form would be a huge step toward total interchangeability of genuine quantum hardware and simulators, within an open source quantum compiler.
Virtually the full set of unit tests could be immediately applied to genuine quantum hardware. It would encourage "hygiene" in quantum interface and quantum program design in an open source community that has to rely on simulators to prepare for better genuine quantum hardware.
If you approve, I would like to contribute a PR for this.  Do you have any comments, design suggestions, ideas, or approaches you would like me to follow?
The main reason for not being amplitude agnostic is for the tests to complete within a reasonable amount of time (e.g., direct access to probabilities without having to sample).
And I don't see any downsides in having some of the unit tests rely on specific features of the simulator, especially since we currently don't have hardware on which we could run a general implementation of, e.g., Shor's algorithm.
Thank you for the response. The benefit I see in amplitude agnostic tests is that we might expect them to "just work" on true quantum hardware, in addition to simulators, without modification.
For example, instead of an IBM back end unit test set, and everything else running on the simulator, it might be possible to apply virtually the full set of unit tests on top of the IBM back end, and switch out the back end throughout with a single configuration option.
It's totally understandable if this not a priority or a change you'd like to see, for the moment. If there's nothing else to discuss, we can close the issue.
The user front-end assumes nothing more than a physical quantum computer can do, i.e., one can only write gate operations and measurement. Additional information by specific back-ends need to be accessed via eng.backend.X which I think is a nice separation...
Access to amplitudes makes it much easier to check, for example, that the global phase of a subroutine is correct. If one uses such a subroutine in a with Control block, this global phase becomes a relative phase which is physically relevant. When compiled for a specific case, e.g., as a standalone subroutine, such a global phase (Ph gate) is automatically removed as it is not relevant....
Damian, I wasn't really aware of that eng.backend.X separation, which is nice. Also, yes, strictly for a simulator, it's easier to and faster to make precise checks. That all makes sense.
Part of the difficulty I personally found with trying integrate Qrack as an optional simulator for ProjectQ is that Qrack was designed to force the user not to rely on direct complex amplitude checks. For example, by default, we apply a randomized global phase factor on initialization and on measurement. (This can be turned off in constructors.) I wonder if some other simulators won't also go this route, because it gives a different kind of consistency checking. It's been easier to know that we're not relying on nonobservables in a breaking way.
Of course, having to accommodate both approaches, to try swap out the simulator everywhere in ProjectQ with a compilation option, we've triangulated a number of bugs that were easier to miss, doing it our way. (Thank you!)
Obviously, the current ProjectQ method works. I understand the separation you mention, now, and it's been a good thing for Qrack. It might be "6 and half-a-dozen," ultimately.
