Minimal code example:
import xacc
import numpy as np 
lower_bounds = np.zeros(2)
upper_bounds = np.ones(2)
# Creating a set of initial parameters that 
 # *should* be in violation of the boundary conditions
# provided above, expecting the opt. to throw an error.
initial_params = np.ones(2) * -1.
# Using the adam algorithm for this example, but
# bug occurs for other mlpack optimizers as well
algoName = 'adam'
 m_options = {
"algorithm": algoName, 
"lower-bounds": lower_bounds.tolist(), 
"upper-bounds": upper_bounds.tolist(), 
"initial-parameters": initial_params.tolist(),
"maxeval": 500}
optimizer = xacc.getOptimizer("mlpack", m_options)
# Creating a fake function that returns (1 - iterable) 
  # and a gradient of (1-iterable)**2 to simulate fake
 # data returns. We'd *expect* this optimization to terminate
# before even running due to the violation of boundary conditions
# with our initial set of parameters. However, it keeps pushing
 # the values of x well lower than the min. boundary of 0.0.
  itr = 0.
 def foo(x):
global itr 
print(x)
itr += 0.1
return (1. - itr), [(1. - itr)*(1. - itr),(1. - itr)*(1. - itr)]
optimizer.optimize(foo, 2) 
I've quickly checked this and indeed we don't support generic lower-bounds and upper-bounds in mlpack.
One difficulty is that it looks like not all optimizers in mlpack support boundary conditions.
e.g. the Adam optimizer doesn't have any parameters for boundary conditions:
https://ensmallen.org/docs.html#adam
@amccaskey what do you think?
