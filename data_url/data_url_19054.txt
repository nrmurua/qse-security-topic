Currently memory in optimization is strongly increasing with every iteration. Using a memory profiler this tracks down to redifining tf.Variables in the propagation of method in tensorflow:

and b) dt =tf.constant

It therefore seems that there is memory leakage due to the change to tf.Variables as introduced in  46be03f by @lazyoracle  what has been the main point of changing the definition. If there was no  specific reason, except for explicitely having to watch constant in the gradient tape I would suggest to revert tf.Variables to tf.constant.
The original motivation for switching to tf.Variable was that it allowed the flexibility to directly access the whole suite of Gradient based optimizers defined in tf.keras.optimizer. I can go back and check if there is another way to continue that integration without fully rewriting parts of the tensorflow related codebase.
We should integrate the tf.Variable with the Paramatermap and C3Obj, such that only the elements that need gradients are made into variables, the others are made tf.constant
Possibly closed in #46
