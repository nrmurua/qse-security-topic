Transpiling circuits seems to take excessive amounts of memory to the point where doing things in parallel can cause all of the memory to be used, freezing ones system.  In this example, I am transpiling 20Q circuits over many repeated applications of a unitary.  When using 4 cpus, each process continues to take up memory until all memory is consumed, and swap space starts to be taken, after which point the system freezes.
@mtreinish pointed out that this might be a Jupyter variable caching issue, but it is reproducible from the script below direct from Python.  one could argue to turn off parallel execution, or reduce it, but as the depth of the circuits grows, this would not solve anything, and likely such issues would be hit with a single process at some point (which I have yet to explore).
The script below is at O2, and there is a big difference between running at O1 and O2 in terms of memory, where the former easily executes within the confines of a 64Gb machine, where the latter kills it every time.
The script and circuits are given below.  One can replace a call to ibm_prague with the newly added fake backend version (#9369) and hopefully get the same result.
20Q_DTC_100cycles.zip
Memory usage at 20Q should not be blocking me from testing circuits on hardware on a 64Gb machine.
No response
As a follow up, if I use the exact same circuits and backend, but change the entangling gate from cz to cx it no longer takes up all of my memory.  This is also inline with other results, where I can transpile larger circuits, e.g. width 50 or 100, on systems with cx or ecr gates and not run into problems.
If I leave out the coupling_map and initial_layout, while keeping cz in the basis gates then optimization_level=2 takes only ~400Mb per processes verses 8+Gb in the original example.  The transpilation time is also dramatically reduced.  In my case, the circuits are already linearly mapped, so I can use the mapomatic inflate routine to do the actual layout.  Others are obviously not so lucky.
