Quantum networks, as a generalization of quantum channels to higher order maps, is a powerful tool in the study of quantum systems. It is important in simulating complex quantum systems, such as in the analysis of causal structures.
To simulate a quantum network, we need to be able to represent the quantum network as a Choi operator with tensor structures. The QuantumNetwork class should store the information on the labeling of all input and output systems, which identifies the leads of the Choi operator tensor. The link_product function is also required, which takes two QuantumNetwork objects and return a new QuantumNetwork object that represents the link product of the inputs.
The overall goal of this class is to help people construct and manipulate quantum networks in a more intuitive way, where they can be done as simple as the construction and manipulation of tensor networks.
@renatomello could you please advise at least for the link product?
For demonstration, I've implemented a demo QuantumNetwork class here. The use case of the class can be found in this jupyter notebook.
The key differences between QuantumNetwork and the current implementation of the Channel class are
During writing the class, I also found two issues in the current Channel class:
Regarding the QuantumNetwork class. There are still some issues that need to be resolved before we implement it.
The Channel class has methods named to_choi, to_liouville, and to_pauli_liouville. Those three return a matrix representation of the Channel in a given representation. For more representation, one can use the superoperator transformations in qibo.quantum_info.superoperator_transformations.
The Channel class has methods named to_choi, to_liouville, and to_pauli_liouville. Those three return a matrix representation of the Channel in a given representation. For more representation, one can use the superoperator transformations in qibo.quantum_info.superoperator_transformations.
Yes, sure. I noticed these methods. But what I suggest is that we may need to override the matrix method in Channel, as in the SpecialGate class. One possible option is to return self.to_choi().
This matrix method is defined in the parent class Gate, which is callable from Channel. I feels like at least this method should return something and raise a warning, or it should return a NotImplementedError. The current version raises an AttributeError, which is not what I expected.
@Canoming PR #1116 implements your suggestion of a NotImplementedError and directs the user to the other methods.
@Canoming in your example of how the QuantumNetwork class would work, does the partition parameter admit 0 labelling or does it start from 1?
@Canoming in your example of how the QuantumNetwork class would work, does the partition parameter admit 0 labelling or does it start from 1?
Hi, the partition in the QuantumNetwork works like the shape in np.ndarray. As said in my notebook:
partition: the partition of the Choi operators. For a Choi operator with partition
, the tensor (np.ndarray) is of the shape (n,m,n,m).
In the case of quantum channels, setting partition = (n,m) means the dimension of the input system is n and the dimension of the output system is m. As a tuple the label starts from 0. If the name partition makes the confusion, maybe we can call it the shape. I chose the name partition to distinguish it from the shape in numpy.
The main difference from the tensor networks is that, in quantum networks, the indexes always come in pairs. The link product link is equivalent to the tensor contraction of tensor networks, but it always contracts between pairs of indexes. Here I'm trying to hide the duplication of indexes within the class. All the function calls from outside should only work on the partition but not the shape of the data.
In the case of quantum channels, setting partition = (n,m) means the dimension of the input system is n and the dimension of the output system is m. As a tuple the label starts from 0. If the name partition makes the confusion, maybe we can call it the shape. I chose the name partition to distinguish it from the shape in numpy.
The name partition is usually designated to subsystems of the global system, e.g. in entanglement theory. If it means a shape, then it cannot assume the value 0 by definition, unless one wants to define a tensor with a 0 dimensional component. Am I understanding this correctly?
In the case of quantum channels, setting partition = (n,m) means the dimension of the input system is n and the dimension of the output system is m. As a tuple the label starts from 0. If the name partition makes the confusion, maybe we can call it the shape. I chose the name partition to distinguish it from the shape in numpy.
The name partition is usually designated to subsystems of the global system, e.g. in entanglement theory. If it means a shape, then it cannot assume the value 0 by definition, unless one wants to define a tensor with a 0 dimensional component. Am I understanding this correctly?
Yes, the number in the tuple means the dimension of each system. For example partition[0] gives the dimension of the first system. We should assume all the ints are positive.
@Canoming could you explain to me what the QuantumNetwork.full method works and what is intended to do?
@Canoming what happens in the link product when expr is None? Shouldn't it default to a specific expression? If yes, which one?
Also, you defined the order of the partition for when three conditions are met (expr is None or self._channle_expr(expr) is not None or self._inv_expr(expr) is not None). However, these are not all the possible scenarios. What happens when none of these conditions are met? Should it default to another way of doing the product or should it raise an error?
Thanks.
@Canoming could you explain to me what the QuantumNetwork.full method works and what is intended to do?
@renatomello The QuantumNetwork.full is used to convert a pure process (for example, a pure state, an unitary operator) to the Choi operator. As all the operations between QuantumNetworks are defined in terms of Choi operators, every QuantumNetwork should store its Choi operator. But I try to allow the construction of QuantumNetworks from state vectors and unitary operators. If the construction is from a "pure process", where pure is True, the full() method will convert it to a semidefinite operator.
@Canoming what happens in the link product when expr is None? Shouldn't it default to a specific expression? If yes, which one?
Also, you defined the order of the partition for when three conditions are met (expr is None or self._channle_expr(expr) is not None or self._inv_expr(expr) is not None). However, these are not all the possible scenarios. What happens when none of these conditions are met? Should it default to another way of doing the product or should it raise an error?
Thanks.
The expr for link product is one of the issues I haven't sorted out yet. Ideally, the link product should accept every valid expression as in einsum. But, as for now, I only hard-coded these 3 cases. I'm not sure about what is the best way to convert the expressions to specify which axis needs to be summed over. I may need to read deeper in numpy to figure out how they implement the einsum function. Also, I think it would be great if we can schedule a meeting to discuss how we should choose the order of the systems. That would affect the efficiency of the contraction, and it can be hard to change after the pull request is merged.
Also, I think it would be great if we can schedule a meeting to discuss how we should choose the order of the systems. That would affect the efficiency of the contraction, and it can be hard to change after the pull request is merged.
What are you calling order here?
Also, the __matmul__ and the link methods (and consequently overwriting the @ operation) will not work without defining an einsum subscript to default to when the subscript is None. (Note: subscript is what you called expr).
An extra question: In your implementation, QuantumNetwork.applyis  returning a ndarray instead of a QuantumNetwork object. Is that supposed to be the case?
Also, I think it would be great if we can schedule a meeting to discuss how we should choose the order of the systems. That would affect the efficiency of the contraction, and it can be hard to change after the pull request is merged.
What are you calling order here?
@renatomello My concern about the "order" comes from different ways to duplicate the indexes. For example, in the simplest case, where the network is a channel, we may consider a n dimensional input space and a m dimensional output space. What we need to determine is what is the shape of the underlying array. There can be four orders:
First, the choice of the order (n,m) or (m,n) is different between the math community and the physics community, and we need to decide which convention should we follow.
Second, the duplication (a,b,c) -> (a,a,b,b,c,c) or  (a,b,c) -> (a,b,c,a,b,c) may affect the efficiency of the contraction. When we contract the networks, the axis with the same dimension will be summed over. The choice of my current order follows the choi_matrix produced by the current Channel class, and the duplication of the indexes follows the default behavior of the np.ndarray.reshape method. But I'm not sure if they are good for our purposes.
Also, the __matmul__ and the link methods (and consequently overwriting the @ operation) will not work without defining an einsum subscript to default to when the subscript is None. (Note: subscript is what you called expr).
For the default behavior of the @ operation, I think we only need to define it for the channels and super-channels, which are the case where the partition is of length 2 and 4. These are the most common conditions, and the default behavior is physically meaningful. For other conditions, we may through an error and ask users to use the link method and specify the subscript.
An extra question: In your implementation, QuantumNetwork.applyis returning a ndarray instead of a QuantumNetwork object. Is that supposed to be the case?
I wrote the apply method for compatibility concerns. The numpy_backend expects all operators are np.ndarrays. The apply method allows to convert a np.ndarray to a np.ndarray.
@renatomello My concern about the "order" comes from different ways to duplicate the indexes. For example, in the simplest case, where the network is a channel, we may consider a n dimensional input space and a m dimensional output space. What we need to determine is what is the shape of the underlying array. There can be four orders:
First, the choice of the order (n,m) or (m,n) is different between the math community and the physics community, and we need to decide which convention should we follow. Second, the duplication (a,b,c) -> (a,a,b,b,c,c) or (a,b,c) -> (a,b,c,a,b,c) may affect the efficiency of the contraction. When we contract the networks, the axis with the same dimension will be summed over. The choice of my current order follows the choi_matrix produced by the current Channel class, and the duplication of the indexes follows the default behavior of the np.ndarray.reshape method. But I'm not sure if they are good for our purposes.
As a first implementation, we can just stick to one order and add complexity to the class implementation if and when it is needed.
Nothing prevents us from doing follow-up PRs.
For the default behavior of the @ operation, I think we only need to define it for the channels and super-channels, which are the case where the partition is of length 2 and 4. These are the most common conditions, and the default behavior is physically meaningful. For other conditions, we may through an error and ask users to use the link method and specify the subscript.
Do you have the subscripts for the length 4 ready? Then I can already add to the code and raise NotImplementedError for other lengths.
@renatomello Sorry about the late reply.
As a first implementation, we can just stick to one order and add complexity to the class implementation if and when it is needed. Nothing prevents us from doing follow-up PRs.
Sure, I agree with that.
Do you have the subscripts for the length 4 ready? Then I can already add to the code and raise NotImplementedError for other lengths.
In the case of the length of the first object is 4, we may use the subscript ijklabcd, jkbc -> ilad (see my updated code), which assumes the first object is a super-channel, which convert the second object (a channel) to another channel.
For the general subscripts, we should always repeat the subscript twice. That is to say, when a user writes ijk, we need to replace it with ijkabc, where abc repeats the same pattern as the user input. Do you have a better idea of how to implement that? I'm still looking for a practical method to implement it.
But anyway, in the end, we'd better implement the summation without the candy of einsum. I can try to rewrite after I get myself familiar with how the einsum is written in numpy
