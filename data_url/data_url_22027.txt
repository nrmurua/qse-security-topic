Using Artiq 3 on a KC705, connected to a 100 MB/s router, the following causes the core device to panic roughly 1 every 1e4 times. I have also seen similar panics for kernel functions that are running continuously, but calling the master via RPCs. I guess this may be related to #837
Experiment pseudocode:
Error in core log (the indices are different every panic):
Can we get the device to print backtraces on panic?
Yes. But this is likely not necessary in this particular case.
I don't think #837 is relevant here, the TCP stack must not panic no matter what sort of garbage it is fed with. I fuzz it regularly to achieve that.
How do you know this is in the TCP stack?
I don't know, but that's where almost all of the slicing lives; most of the rest is in the RPC code and we haven't touched that.
@sbourdeauducq I have thought about it and making the core device print backtraces is a good idea and is easy, because that code is already in libstd (and we only need to tweak it slightly for our embedded environment). This will however bloat the runtime even further (which I personally don't consider a problem whatsoever).
Sure go ahead, we have 1+GB RAM (though flash is more limited). As long as it can be easily disabled for running on the block-RAM of the RTM FPGA...
How much BRAM do you have on RTM for the firmware and what do you intend to fit into it?
Not much, it's a XC7A15T. Just satman (drive si5324, process drtio aux packets), no smoltcp or anything particularly big or complex.
There is nothing else needing a lot of BRAM on that FPGA, so almost everything can be allocated to the CPU if needed.
No panics with the following experiment at current master over 20000 runs:
@whitequark what are you waiting for? more data?
@cjbe it might be useful to check whether this still occurs after all the smoltcp changes.
@jordens A way to reproduce and/or a backtrace.
@jordens @whitequark I did try to reproduce this with the master branch as of a few days ago, and I could not.
I am going to upgrade a running experiment to a current version of Artiq next week - I will report if I see this again.
@cjbe Did you see anything?
@whitequark I have not been able to reproduce this problem with the current version of Artiq.
