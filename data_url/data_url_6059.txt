Run a list of the same circuit with a seed_simulator fixed gives different results.
The problem is not reproducible with BasicAer
When checking the job.result().to_dict(), it looks like the seed_simulator is taken into account for the first circuit.
Obtain the same results for all circuits in the set
I do not think this is the desired behavior here. When seeded, the simulator should return the same results given the same input. This is different than all inputs being identical due to using the same seed. That is to say that the seed should seed the rng used for the ensemble of circuits.
As @nonhermitian said what you are expecting is not the correct seed behavior.
If you set a seed it is for the entire qobj, not a single circuit. So each circuit in qobj will return the same results each time you execute the qobj. Individually each circuit uses a different, but deterministic seed, derived from the seed_simulator value.
Sorry for the confusion, that exactly what i said, for a specific seed_simulator fixed, i obtain different results for the same circuits in the qobj on Aer simulator. This is not expected right?
This is the expected result.  If you run the same batch of circuits again, you get the same output as you have posted.
Ok, i see @nonhermitian thanks, just out of curiosity, why you don't use the same seed for all circuit, not just for the qobj or the batch, because it seems more plausible to me? Also, on BasicAer simulator, i got the same results for each circuit in the batch.
Because you want each circuit to perform independent random sampling based on the amplitudes of the statevector.  If the seed is the same for all, then this is not independent random sampling.  The basicaer issue you show above is a bug, and should be reported.
The only requirement of the seed is that the ENTIRE output is deterministic for a fixed qobj and fixed seed. You could do it the way you say, but to what purpose? If you set the same seed for every circuit then there is literally no reason to execute 10 identical circuits because they all would have the same outcomes and just take 10 times the simulation time. For more realistic use cases there would also be a problem with biasing error for noise models and measure sampling which would be an issue for noisy simulations (in randomized benchmarking or tomography for example)
Oh, i understand better now thanks for your explanations @nonhermitian and @chriseclectic
