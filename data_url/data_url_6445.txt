Currently, qsimcirq users who want to run on GPU must compile the code locally, as the wheels are built on Github Actions runners which do not have the CUDA SDK. Compared to the CPU workflow of pip install qsimcirq, this is incovenient; it would be preferable if GPU support was built into the released wheels.
In order to provide this support, we need to investigate the following options (listed from most to least preferable):
Hello, I was wondering if there is a tutorial on how to use GPU acceleration with qsim circuit simulations in Python (I have a parameterized circuit written in Cirq which is optimized in a python script, and I would like to carry out the circuit evaluations on an NVIDIA GPU). I was also wondering if there is any benchmarking data for what kind of speedup one could expect compared to regular CPU computation.
We have a tutorial for using GPU acceleration on GCP here: GPU-based quantum simulation on Google Cloud. If you have your own GPU-enabled device that you'd like to use instead, you can follow the instructions in steps 5 and 6 to locally build and verify the GPU version of qsim.
The goal of this issue (which is currently incomplete) is to remove step 5 from the process, so that any GPU-enabled device could simply run pip install qsimcirq and have the GPU version ready to go.
As for benchmarking data: #449 will add detailed info to the site, but on larger circuits (25+ qubits) we've observed GPU simulations completing ~5x faster than CPU simulations of the same circuit.
Complicating factors identified for implementing option (2) described in the first comment:
