I have been working with @kylegulshen on a PR to forest-benchmarking that changes some unit tests which use the QVM. Strangely, there were some tests that failed when the full test suite was run, but not when run alone. There certainly could be other issues at play here, but on a whim I decided to change the QVM's random seed for one of the failing tests from 1 to 2 and suddenly things began working, which seemed alarming to me. I sense that there could be something deeper going on, so I just wanted to get this issue in writing and maybe spur some investigation.
I suspect that I set the test too stringent so that passing was sensitive to the particular seed. If that's the case, does changing the seed from 1 to 2 in particular still "seem alarming"? I would think the only surprising thing is that initializing, setting the seed, and then running a test produces different results than initializing, doing some work, setting the same seed, and then running the same test.
@kylegulshen So I originally felt that both things were alarming (the change 1 -> 2, and the ordering issue). I assumed that the test was tailored to the random seed, but if that is not the case then I am less concerned with the first point. But, that still leaves the order of operations issue. I originally thought that maybe it was a pytest fixture scope issue, but even changing the QVM's connection fixture to "function"-level (meaning that it is recreated for each unit test) did not fix the issue, and therefore pointed me in the direction of filing a QVM issue.
Closing as is inactive
