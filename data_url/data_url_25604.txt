This issue goes hand in hand with the import dependencies issue #674.
The current implementation of Covalent dispatcher requires that the dispatcher's environment satisfies all of a workflow's package dependencies. However, different workflows may have possibly conflicting dependencies. Worse, some of their deps might conflict with Covalent's own dependencies. For instance, our own Quantum Chemistry tutorial is incompatible with the version of numpy required by Covalent.  In addition, in a on-prem deployment with a central Covalent server it is impractical, not to mention really bad practice, for each end user to modify the server's environment; the server should ideally run in a lean and fixed environment.
To avoid this limitation, Covalent must process a workflow without unpickling any workflow-specific data. These include:
The following snippet fails when run from covalent-client:
covalent logs:
Indeed, the Covalent server currently handles the raw (unserialized) workflow data in several places, and covalent-server doesn't have pandas.
The encoded-workflow-processing-exp branch (work in progress but basically complete modulo sublattices) eliminates the intermediate unpickling mentioned above; in particular, workflows are now submitted to the server purely using JSON-serialization. If you install that branch using pip install -e into covalent-server and uncomment the commented-out line, then the workflow runs successfully from covalent-client:
While a proper design doc is forthcoming, the basic idea is that all data related to the workflow is now decoded and processed in the covalent-client conda environment and not in covalent-server. For example, post-processing (which requires executing the workflow function) is offloaded to the Dask cluster running in covalent-client.
If one instead replaces the "workflow_executor" line with
then one gets the following output:
In this case, the dispatcher has halted workflow execution just before postprocessing. One can then postprocess the workflow "offline" (without the Covalent server running) by calling res.post_process():
Hey @cjao  thanks for the detailed issue. Is there any way to relax the assumption of client side get result having the same environment? Ideally we want those to be different (at-least just to look at the result).
I suspect we can somehow already get the "return" electron/type by post processing while preprocessing happens and just set the result to that. Maybe I am missing something. Thoughts @kessler-frost ?
Hi @santoshkumarradha , the workflow is constructed by the client, so the client environment is necessarily able to understand the type of the decoded result.
Perhaps you are referring to the possibility that the client environment changes after workflow submission. In that case, the client can still view a string or JSON representation of the result (if the result is JSON-serializable); results are encoded as TransportableObjects, which have grown two properties: object_string and json.
Indeed true it's in the client side, but need not be the same client nor have the same environment. But if there is a string representation, that's good, but I may be missing something, don't we need postprocessong to happen for us to know what a lattice result is even for getting the string representation ? If so, where is this post processing happening for the string ?
The string representation of the workflow result is indeed computed during post-processing. However, the post-processing doesn't have to take place client-side. The only requirement is that the post-processing environment satisfies the dependencies of the workflow. For instance, one can perform the post-processing on a different computer; internally, post-processing simply becomes another task that can be run using any executor.
Is there any way to relax the assumption of client side get result having the same environment? Ideally we want those to be different (at-least just to look at the result).
@santoshkumarradha So, to the best of my understanding, the only assumption on the client side is that it should have sufficient packages installed to understand each node's result and the final lattice's result; it is not necessary for the client to have all the packages installed that were used during the actual execution, even if it is doing post-processing on the client-side (assuming there are no non-electron executions happening inside the lattice definition).
Is that what you were asking or am I misunderstanding something?
To elaborate on @kessler-frost's reply:
There are several types of dependencies:
Each class of deps is dealt with differently.
In develop, the dispatcher unpickles workflow data in several places:
The first two steps are now moved outside the server process and can be run using any executor whose environment satisfies the dependencies. As for the last point, the PR modifies the SDK so that the server can reconstruct a Lattice purely using json.loads().
The basic design of Covalent already allows the dispatcher to manage a workflow without being fully aware of the workflow data; the essential information is the input-output relationships between tasks, in the words, the structure of the transport graph and not the raw contents of each task.
To actually process a workflow without requiring its dependencies, the Covalent server must observe two basic principles:
The pickle module is not secure. Only unpickle data you trust.
It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. > Never unpickle data that could have come from an untrusted source, or that could have been tampered with.
Safer serialization formats such as json may be more appropriate if you are processing untrusted data. See Comparison with json.
Presently, the Covalent server violates the first principle in several ways:
It also unpickles data during several steps of the workflow processing pipeline:
The proposed change moves all unpickling and pickling out of the server process. In addition, the server process only handles data types defined by Covalent:
Executors now accept serialized inputs, including the underlying callable of a task, its inputs, and any call_before/call_after deps. In addition, they return output encoded as a TransportableObject. This is achieved by a wrapper function which performs the following steps in the executor's environment:
A consequence of the above is that the workflow result is returned and stored in Result._result as a TransportableObject. Calling Result.result automatically attempts to deserialize the result; the new property Result.encoded_result returns the encoded result.
The UI displays TransportableObjects using their their JSON form (if the underlying object is JSON-serializable) or their string representations (TransportableObject(obj) stores str(obj) together with the base64-encoded, pickled form of obj)
Lattices always store their underlying workflow function in serialized form (a TransportableObject). During build_graph, or when the lattice is called as a regular Python function, the workflow inputs are passed to a deserialized copy of the workflow function. All inputs are serialized before storing them in the lattice or transport graph nodes.
Postprocessing, which involves deserializing and executing the workflow function on the deserialized workflow inputs, becomes a task to be run in a designated executor whose environment satisfies the workflow dependencies. Lattices now expose a workflow_executor property analogous to the existing executor property. In addition to the usual executors, workflow_executor supports a special value called "client". Setting workflow_executor="client" causes the dispatcher to halt workflow execution just before preprocessing and set the workflow status to Pending Postprocessing. The client can take the Result, which contains only encoded node outputs, and postprocess the workflow client-side by calling Result.post_process(). This returns the deserialized workflow result.
Since postprocessing is now decoupled from the main workflow execution, the Result has acquired some new statuses: POSTPROCESSING, PENDING_POSTPROCESSING, FAILED_POSTPROCESSING. The last one would be encountered if the workflow_executor specified by the user doesn't actually satisfy the workflow dependencies, or if the postprocessing fails for any other reason; for instance, a remote executor might time out if the workflow contains a long sleep() statement. Whatever the reason, one can still reattempt postprocessing client-side.
Sublattices are also deserialized and built using the workflow_executor. They are presently handled by deserializing the sublattice and calling dispatch_sync from the Covalent server. This issue is a bit more involved than postprocessing since the server needs the sublattice transport graph to execute the sublattice. If the graph-building executor were to simply return the transport graph as a TransportableObject, the Covalent server would need to unpickle the return value and violate the second Basic Principle. This is robustly handled by the next change:
For all intents and purposes, lattices can now be reconstructed from a JSON string purely using json.loads(). This is accomplished by teaching TransportGraph, and associated metadata objects (Deps and Executors), to serialize themselves to JSON and rehydrate themselves from JSON using json.loads(). This task is made easier by the fact that we are serializing most non-JSONable data as TransportableObjects, whose data attributes consist entirely of strings. The main benefits of this change are:
Assuming that a workflow doesn't use the local executor, the Covalent server no longer unpickles any data.
Each executor needs to understand TransportableObject. This presently means a dependency on Covalent. An easy mitigation would be to split the PyPI Covalent package into covalent-server and a covalent-sdk, the latter of which would only contain core data types. The TransportableObject abstraction allows us to extend the data encoding scheme in the future,
@FyzHsn The ongoing work to persist Result using the new data store is essentially unaffected and maybe even simplified a bit using the Transport Graph's new method to serialize itself to JSON. For instance,
returns the transport graph in node-link form, with the additional benefit that all metadata objects are in their JSON representations. Thus one can save and load the data using json.dump and json.load.
