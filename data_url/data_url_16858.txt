When the C++ state vector simulation is run, the state vector is returned as a JSON string to qiskit. There, the string is parsed using a JSONEncoder and validated against a schema using marshmallow (_deserialize in https://github.com/Qiskit/qiskit-terra/blob/f17f8f073e1d76d4b3573878c58415e6f9e46542/qiskit/validation/fields/custom.py#L49 ). When it is returned using Result.get_statevector(), the state vector is serialized again in ( https://github.com/Qiskit/qiskit-terra/blob/f17f8f073e1d76d4b3573878c58415e6f9e46542/qiskit/validation/fields/custom.py#L43 ) and formatted in https://github.com/Qiskit/qiskit-terra/blob/c3cdd355d3314889a1cff6597f0b5509a537d309/qiskit/result/postprocess.py#L176 .
In summary, the state vector goes through following transformations: JSON string -> JSON -> python list of complex numbers -> python list of 2-valued lists -> numpy array of complex numbers.
These transformations have a significant impact on the runtime when many qubits are simulated (23+). Result.get_statevector() should return a numpy array directly instead of going through _serialize.
It seems that some of the transformations are not necessary. The deserialization in custom.py should return a numpy array and the serialization should be skipped when calling Result.get_statevector(). In addition, the deserialization is slow for large state vectors as marshmallow does not reserve memory for the list containing the state vector prior to appending the variables returned by _deserialize. This can be fixed by loading the JSON string using an object_hook ( https://docs.python.org/3/library/json.html ), i.e. directly creating a numpy array while parsing the JSON string instead of relying on marshmallow.
There are several efforts in progress on this front. The first is just a band-aid to make this use case work: #3181 the second is: #3383 which is the start of reworking the current provider interface to decouple json/ibmq api from the providers interface. The third effort I don't have a link for because I'm still drafting an rfc for the effort, but is a v2 providers interface that doesn't use qobj and the ibmq api semantics at all as the interface to providers, but instead is just python objects.
Coming over from the aer issue - here's a quick test to demonstrate the problem (and its severity). TL;DR is that a temporary workaround could be replacing raw_result.get_statevector(i) with direct access np.asarray(raw_result.results[i].data.statevector). It appears that Result.get_statevector is essentially equivalent to running Result.to_dict and then accessing that dict manually (that is to get one statevector result - so if there were a loop over circuits and get_statevector is used within the loop it is REALLY bad).
I was using master branch for these tests.
Results on my machine:
Test code:
Confirmed the problem is in Result.data() - removing the to_dict() call improves the performance of get_statevector immensely (will likely improve get_unitary(...) even more, and get_counts/memory(...) equivalently but I haven't tested).
Removing the to_dict() from data(...) and updating the rest of the Result module to account for the change should fix the performance issues of the get_statevector/unitary(...) methods. So just change self.data(...)['counts'] to self.data(...).counts and self.data(...)['statevector'] to self.data(...).statevector (and so on...).
Problems will arise though if client-side code is relying on the Result.data(...) method to return a dict (even though the client could just call to_dict() on the ExperimentResult(Data) themselves) so there could be issues there. Still definitely solutions but they may be slightly more complicated.
This has been fixed now. Since we moved to avoiding marshmallow/forcing serialization and using python objects for the back and forth between terra and backends, aer has been updated to just move the output into a numpy array and return that object in the results.
