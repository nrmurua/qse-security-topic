Currently, many Qobj operations are tagged as "tidied up" operations, which means that if the global option auto_tidyup is set to True (its default), then small values (< 1e-15) are truncated from the output matrices.  This generally ends up with "neater" looking matrices when output; there are far fewer numbers like 1 + 4.516584e-17j reported, when that's basically just 1.
There are, however, a couple of problems because of this.
The first is speed; after every (very simple)_ operation, such as matmul or add, the tidier runs on the matrix.  This typically has O(nnz) complexity for sparse matrices, and O(n * n) complexity for a dense square matrix with n rows.  This adds very significant overhead on to simple operations.
Second is numerical precision.  By dropping small terms, we are effectively turning numerical imprecision into a "feature" as opposed to a bug.  Because QuTiP 5 uses the new data-layer to do most of its operations, rather than constructing intermediary Qobj instances, many mathematical operations will not give the same result if the data layer is used directly compared to if Qobj is used at every step.
For example, let's say we have three random Qobj:
Now, in general,
because a + b + c will invoke the tidyup code twice, whereas the right-hand side will not invoke it at all.  This means that despite the fact that Qobj.__add__ uses the data-layer addition underneath, there will be numerical differences using the QuTiP default settings.
I suspect these sort of problems are behind the recent test instabilities on dev.major with regards to tolerances.
My vote is for a removal of auto_tidyup, or at the very least, changing the default value of the option to False.  We still want nice values for display, so perhaps we could consider having an option to produce a tidied up copy when calling __repr__, instead?
Interested to hear what others think about this too.
The way I see it, tidyup is to remove 0 elements from the sparse matrix, keeping nnz low when making the liouvillian etc.
So it should do nothing on Dense data layer.
If we do
The matrix is empty, but it has still nnz elements saved without tidyup. It need to be periodically cleaned to keep sparse matrix sparse. We could lower the number method calling it, but we should keep it at key points.
The problem is recognising what a true 0 is from a small value.  In your example on the new data layer, there will already be exactly 0 items explicitly stored (and data.add_csr is going to get faster and smarter in another PR, too) without tidyup, but much more concerningly 1e-15 * qutip.rand_herm(5) will end up in a zero matrix, which is clearly absolutely wrong.
The argument from Liouvillian doesn't work here - even before the new data layer, liouvillian never called the tidyup code because it accessed the Qobj.data field directly, rather than used Qobj.  Since it then goes into CQobjEvo in most of the solvers, which also don't touch the tidyup code, it doesn't get called.
I don't think there's any safe way to know if small values should actually be zero, or if they're just small values.  It makes sense that we want to maintain as much sparsity as possible when values are truly zero, but I think having the QuTiP default be to attempt to tidy up after every addition, multiplication and matrix multiplication is quite unsafe, and for people who aren't doing very very sparse calculations, it's actually a slow down.
QobjEvo used to tidyup once right before 'compiling'. We could tidyup values smaller than 1e-15 * norm to differentiate small object to numerical error.
In usual operations, matmul, add etc. if you can make them smarter then and keep a reasonable sparsity without tidyup, it could replace the Qobj's auto_tidyup. And in situation where this is a slowdown, Dense should be used.
We could change the default to False for the beta and see how users reacts, but I would not remove the option.
Switching auto_tidyup to being a relative tolerance not an absolute tolerance I think is a really good idea.  It raises the cost of it a bit by making it a two-pass operation (and for sure let's use the max norm, not the trace norm!), but I think it's a sensible compromise.  We could even have two options - tidyup_atol and tidyup_rtol to have both, and have tidyup_atol default to 0.
Also, fine point about QobjEvo - I'd forgotten that it internally called tidyup.
I'm certainly in favour of swapping the default to False for the next major release.  I know people may still want the option, so not a good idea to remove it completely, but I think having the default be False is safer numerically.
Just some information. There were indeed issues about this, e.g. #1247 . For new users, this could lead to some confusing behaviours. In those cases, relative tolerance definitely helps.
Ah perfect example, thanks very much Boxi!
I just got bit by auto_tidyup for the second time and was looking to see what issues there were about it here.... In my case, I was not careful to put parentheses around small physical constants scaling an operator and had the operator tidied up to zero. Since this was one term in a larger Hamiltonian, it was pretty tricky to debug where the problem was coming from. One idea I had was that perhaps qutip could optionally warn if auto_tidyup turned a matrix into the zero matrix. The idea here of a relative tidy up might be a better solution though.
@jakelishman Do you think we can make this relative auto_tidyup threshold a good first issue for new contributors?
Oh, sorry I forgot to reply to you Boxi.  In theory it's not such a difficult change, but it does have quite some performance implications - doing it properly will involve writing Cython, and we need to make sure that there's no major regressions.  The swap to relative amplitudes necessarily makes the code ~twice as slow (from a one-pass to two-pass algorithm), but we do need to take care that it's as fast as possible, because this code is called all the time.
Okay, involving writing Cython itself already makes it unsuitable for most new contributors I guess.
I think even if the time of tidy up doubles it still won't be a significant bottleneck for solvers? Matrix multiplication itself will be O(dim^3) in the worst case, this one is at most O(n^2). But I do agree that we need to be very careful with such a low-level function.
If you're concerned about the solvers, a) they skip tidyup til the end anyway and b) technically the computational complexity of CSR * dense vector is identical to tidyup (though tidyup is a little more cache efficient), but really it's the constant factors that could kill you for small systems.  For example, the "naive" way of implementing a relative tolerance would take the absolute value of a complex number, but that involves a floating-point square root, which is a very slow operation.  That's likely partly why the current version compares real and imaginary components separately, even though the sparsity structure is only improved if both go to zero.  In a two-pass operation you'd sqrt twice for every entry (naively - all the square roots are very avoidable), and I'd start to worry that that really could dominate small system operations.
Or maybe you should just ignore me when assigning "good first issues" - I'm probably too opinionated about performance characteristics without enough experience at managing other people's code!
Well, your opinion is definitely important, especially regarding core :) Performance is a top requirement there.
Technically I think there is no need for square root at all because we can just compare the squared value to the squared tolerance. But I get your point that constant factor may dominate the performance of a small system.
It's best to avoid Cython in "good first issues", so this one won't be on my list anyway.
