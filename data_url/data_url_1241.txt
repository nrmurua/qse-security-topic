For systems L=12 or similars, the density matrix of a complex128 data type should occupy roughly 260Mb in RAM, yet I see that the process increases the RAM memory usage to about 1.5-1.7Gb when access to the density matrix on a given moment step is required. This becomes problematic for bigger system sizes.
I have created a minimal example code below
I have seen that, when dm=r.density_matrix() is called, this returns the density matrix as a np.array (in this case of type np.complex128). But here is the thing: for n=12 qubits as in the example, the density matrix only occupies ~ 260Mb in RAM for the np.complex128 data type, whereas the loading operation rises the whole RAM usage to about 1.7GB! I think the whole object r returned by the generator is quite heavy, but since I only want to obtain dm, and nothing else, I would like to avoid such a big memory blow up. This is more problematic if one goes to systems of n>12 of course, because it limits a lot such step by step procedures. So, is this a bug in the Cirq implementation of the simulator or am I missing something? How can one avoid such memory blow-up, for instance, if we just want to access one of the protected variables of the r object? (in this specific case, r._merged_sim_state.target_tensor gives the density matrix unreshaped).
Cirq version
v1.1.0
The simulation state requires the DM plus three buffers of the same size, and is maintained throughout the simulation. The simulation runs lazily under generator methods like simulate_moment_steps, so the whole thing stays in memory while you're iterating. Then calling density_matrix() on the step result creates a copy of the DM. So at a minimum, there are five copies of the DM in memory during your iteration above. The other 500 MB may be GC not run yet or other misc stuff.
Note density_matrix() has an optional copy parameter defaulting to True. If you set it to False, that eliminates a copy, so might reduce the garbage created.
@95-martin-orion @tanujkhattar looking with fresh eyes, it seems like this is an unnecessary copy. reshape IIRC does a copy for you. Seems like the default value for copy should be False, or perhaps not even have the parameter at all and allow the user to make their own copy. I know we looked at this previously but I can't remember why I put it there.
https://github.com/quantumlib/cirq/blob/bc1e2d3f98a622080b7431775eadcc95f34aa8f7/cirq-core/cirq/sim/density_matrix_simulator.py#L299-L300
Okay looking a bit further, SparseSimulatorStep.state_vector defaults to copy=False. So I think the density matrix step result has the wrong default there. But even still, I don't see a point to the parameter on either of these step results since the user can just create their own copy.
This probably deserves its own issue if you want to create it. I think the current issue can be closed per the explanation in the previous comment.
The simulation state requires the DM plus three buffers of the same size, and is maintained throughout the simulation
Why does the simulator require 3 additional copies of the DM? This is just repeating the same huge array, and in the case of systems of L~14 qubits or similars, things start to become very expensive in memory. Is there a work around for this to work only with a single realization of the DM to be loaded in RAM?
I agree with the point that the argument "copy" seems to be not necessary, as the user can decide whether to copy the DM in memory or not.
They're used for efficient calculation of channel operations. I'm not super familiar with the code, but it's here; you can see all the buffers. https://github.com/quantumlib/Cirq/blob/master/cirq-core/cirq/protocols/apply_channel_protocol.py. It is more efficient to maintain them than regenerate them each channel. I'm not aware of a workaround.
Why does the simulator require 3 additional copies of the DM?
qsim does something similar for state vector simulation, but the same concept applies: these buffers provide the necessary space for performing operations on the state. For example, one buffer might contain an operation to be applied to the state, and another might be used to store the output since we can't do the operation in-place.
It seems Memory leak has not been evidenced, but we may improve on the memory usage in the DM instead.
This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 30 days
Issue closed due to inactivity.
Issue closed due to inactivity.
