I think our current approach of using MLE in factories to estimate fit parameters is not the best approach to estimating what we want to learn which is the ZN limit.
I am not sure we are currently meeting the assumptions needed to use MLE and our current approach is really an answer to "what is the best truth I can assign to these model parameters" which is not exactly what we want to learn.
It is also true that a function of an estimate (current approach) is not the same as an estimate of a function.
I think a Bayesian approach would better fit the question we are asking, eg. "What is the best estimate I can make for the zero noise limit?"
We would need to depend on a python stats package to assist with this, but I don't think that's too bad here.
I have written a brief summary of both current and proposed Bayesian solution with some LaTeX math here:
BAYESAN.pdf
I did consult with @cgranade to make sure I got the details right on the write up :)
Thanks @crazy4pi314 !
Bayesian ZNE has always been one of our "dreams" both for research and for Mitiq. I completely agree that it would very appropriate for the task of estimating extrapolations.
However, I also think that the current MLE approach is not unjustified and has its own advantages:
My view on this issue (this is just my personal opinion), is that it would be very good to add Bayesian estimation as you suggested but is should probably be an additional feature and not a replacement of the current MLE methods. For example it could take the form of a new BayesFactory class in mitiq.zne.inference .
Two comments on the pdf notes:
Thanks for the nerdsnipe, @crazy4pi314! If I could chime in, @andreamari, Bayesian stats for quantum applications are something I've worked in for several years now.
To that effect, the approach that @crazy4pi314 laid out above is a strict reduction in the assumptions required by comparison with MLE methods. The argument to that effect comes down to that the decision to pick MLE over any other frequentist method is identical to the choice of which prior to use in a Bayesian framework, as per the Wald's complete class theorem (huge thanks to @csferrie for teaching me that one). From that perspective, Bayesian reasoning gives an explicit framework for tracking that kind of framework.
In this case, that's useful for dealing with the function-of-estimate versus estimate-of-function dichotomy that comes up in cases like this one, operational tomography, spectral density estimation, and other cases where predictions about future data need to be made using a finite data record.
Thanks @cgranade !
I am definitely not an expert, but can we say that MLE roughly corresponds to a particular case of a Bayesian estimation where a uniform prior distribution is assumed for the model parameters?
In this sense I agree that I said something wrong when I wrote that MLE "requires minimal assumptions": it actually requires more since it picks a specific prior. What I wanted to say is that, by piking a flat prior, the initial ignorance about the model parameters is maximized. But I think this is a minor issue.
The main point is that there is a trade-off between the generality of the inference framework and the simplicity of the theory/code implementation. Fitting a line and taking the intercept is a very intuitive way of doing zero-noise extrapolation which any user and contributor of Mitiq would understand. On the other hand it would be awesome to have at disposal a more advanced Bayesian method which, by construction, it would take into account physical constraints and would also output a full probability distribution for the zero-noise limit.
For this trade-off reason I think it would be really nice to have both methods in Mitiq.
