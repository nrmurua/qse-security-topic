If a dataset_db has grown too large, new dashboards cannot sync the datasets and so are unusable.
This is a known bug, but has not been addressed; see e.g. #1614 (comment)
The actual error comes from within python asyncio (via sipyco) and is identical to this one: #671.
Perhaps sync_struct could be more intelligent about trying to sync large datasets and chunk data accordingly? Or at the least, give a sensible error when the dashboard is unable to sync the dataset_db.
Dashboards start,. datasets sync and dashboard is usable
Do you really need to broadcast 100MB of datasets to every dashboard? What is the actual use scenario here?
Mostly this happened because I'd accidentally let the dataset_db grow too large, and the error message wasn't particularly illuminating on why a new dashboard couldn't connect. There's no way of correcting this in a reasonable timescale with the master still running, since obviously deleting datasets manually would take a while.
Essentially my feeling is that it's a bug for it to behave in the current manner, and I would be perfectly happy with an error message from the master saying that the dataset db is too large to function correctly so that other users don't get bitten by this and have to spend time debugging a cryptic error message.
