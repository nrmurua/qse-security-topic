Qobj instantiation and mathematical operations have a large overhead, mostly because of handling the dims parameter in tensor-product spaces.  I'm proposing one possible way to speed this up, while also gaining some additional safety and knowledge about mathematical operations on tensor-product spaces.
The steps:
As of QuTiP 4.5 (and all previous versions), Qobj instantiation is slow and this permeates through to all operations on Qobj.  Matrix multiplication, scalar multiplication, addition and so forth all need to instantiate new Qobj instances, and the time penalty for this is on the order of ~50µs per object.  This results in more and more code than needs to bypass Qobj for speed, and in some cases (e.g. qutip.control) makes the use of Qobj prohibitively expensive.  This obviously is not ideal, since Qobj is our primary data type.
The majority of this time loss is due to inferring the type of an object from its dimensions, and on unnecessary copying of data at initialisation.  This is exacerbated by operations often instantiating an out parameter as out = Qobj(), and then doing things like out.dims = ..., out.data = ....  This causes runtime checks to be done at every stage, so the penalty of initialisation can sometimes be paid several times over in simple operations (a particularly notable example is in the implicit promotion of scalars to operators in addition, taking over 500µs to execute 1 + qutip.qeye(2)).  A lot of this can be completely avoided, however, simply by instantiating the objects using all known information, not relying on inference.
In particular, various operations know what the type of their outcome is by a simple lookup table: addition is only defined between operations of the same type and maintains that type, whereas the adjoint has the mapping
If this information is supplied to Qobj.__init__ (and the fact that it need not copy data we've created specially for it...), we can hugely slash the overhead of mathematical operations while maintaining their safety.
The issues start to come once we look at matrix multiplication and tensor-product spaces.  The tensor allows us to construct objects which are a mixture of several different types, and matrix multiplication wants to be able to contract scalar product spaces so that bra * ket gives a scalar.
The current dimension handling in QuTiP is simple and intuitive until tensor-product structures are considered.  At this point, it starts to become more complicated.  In particular, the type of a Qobj is tied to its dimensions, but it becomes difficult to define this once there is tensor product structure.  Some of this is because QuTiP allows us to construct objects which do not have a really rigourous mathematical backing to them, such as I . |g> - the tensor product of an operator and a ket.  QuTiP assigns this a type 'oper', though the way it reaches this decision is more like:
Such objects do have a use.  Let's say we have a system with two computational qubits and one ancillary qubit, we've performed a calculation on it and ended up in some state |x> = |a>.|b>.|c>, and we want to extract the computational subspace when the ancilla bit is projected onto |0>.  We can do this in a mathematically rigourous way with
which will always return a density matrix.  Alternatively, we can instead define the operator (note proj() to create |g><g| has become dag() to simply make <g|) as
which gets us what we wanted.
This is not necessarily common, but it is useful in some circumstances.
There have been some cases of complaint about the handling of tensor-product spaces in QuTiP in the past (see this discussion in the Google group), but these largely revolve around people not liking the idea that we enforce the tensor structure to be maintained at all.  My reading of these issues is that some people would like to see dims removed, or make mathematical operations effectively ignore it.
Personally, I think enforcing the tensor product structure catches a whole lot of potential issues in code when working with objects in different Hilbert spaces, and so far I've not actually seen any examples where I think frequent overriding of the dims is necessary.  I'd argue that the Qobj constructor taking a dims parameter is sufficient for any use-case which needs to manually set the dimensions because they're passing in an object constructed outside of QuTiP functions.
Removing dims also makes a lot of operations harder to do.  Various places in the code permute the order of the tensored spaces, and dropping dims means that the user has to "remember" the tensor product structure themselves so that they can pass it in, and we can then know what to reorder.  Clearly this is undesirable - the dims are a non-computable property of the object, and therefore should be carried around as a data attribute on the class.
The alternative in the Google group that's sometimes suggested is to keep dims for these use-cases, but make it more of a suggestion, so that any two objects which satisfy left.shape == right.shape should be compatible for addition-like operations, and ones which satisfy left.shape[1] == right.shape[0] should be compatible for matrix-multiplication-like operations.  Again, I personally would tend to reject this on the grounds that enforcing the tensor-product structure is respected is a core part of what Qobj does; it ensures that the operations are mathematically possible, and that operations between different Hilbert spaces are not mixed.
First off, we can sidestep some of these issues by improving library code which creates Qobj instances.  Operations like Qobj.__add__ already know exactly what the output dimensions are, what the the type must be, and other things like if Hermiticity has been preserved.  We move away from the outdated style of
to one which passes all the information in one go:
This is more verbose, but significantly faster.  With no other changes to the code, doing this can save around one-quarter of the overhead on several Qobj operations.  Moving to the new data-layer types also gets large improvements in instantiation time.
This is fine, except for matrix multiplication of tensor structures.  In these, like in the example above, the matrix multiplication can cause tensor structures to contract, and so they then become incompatible with their previous Hilbert spaces.  If we instead maintain a list of 'type' and introduce a 'scalar' type, such objects can sensibly be broadcast back up to the correct size when needed, treating the spaces containing as identities of the correct dimension.  I envisage that this may have some nice use-cases within qip, for example a gate on a single qubit could be represented by a two-by-two matrix with all other dimensions scalars, rather than requiring the whole Hilbert space to be represented all the time.  Optimisations can be done using only the required elements of the subspace, and only broadcast up to the full representation once at the end.
Further, we can ease the burden of parsing the dimensions in the first place.  I haven't fully attempted this yet so I don't have full details on this, but I imagine there is some internal information we can keep after a single parsing pass that will make other operations simpler.  This is particularly true of super-operators, which often care about the input and output shapes of the spaces, necessitating several more calls to np.prod.  Since this information has to be computed at type-inference time, it's easy to save it and remove the overhead.  I'd propose having this type be internal, something like dimensions.Parsed and storing it as a protected attribute on Qobj instances.
If we instead maintain a list of 'type' and introduce a 'scalar' type, such objects can sensibly be broadcast back up to the correct size when needed, treating the spaces containing as identities of the correct dimension. I envisage that this may have some nice use-cases within qip, for example a gate on a single qubit could be represented by a two-by-two matrix with all other dimensions scalars, rather than requiring the whole Hilbert space to be represented all the time.
I like this idea. That one always has to use expand_operator to fill all the identities is a waste of time. It would be excellent if Qobj can handle this internally without really constructing the identities!
If we instead maintain a list of 'type' and introduce a 'scalar' type, such objects can sensibly be broadcast back up to the correct size when needed, treating the spaces containing as identities of the correct dimension. I envisage that this may have some nice use-cases within qip, for example a gate on a single qubit could be represented by a two-by-two matrix with all other dimensions scalars, rather than requiring the whole Hilbert space to be represented all the time.
I like this idea. That one always has to use expand_operator to fill all the identities is a waste of time. It would be excellent if Qobj can handle this internally without really constructing the identities!
Yep, I agree with this. This could make lazy computation a lot neater ! Right now, I am writing the method which can multiply tensors without multiplying the expanded tensor (if the rest are identities). This could be a part of the Qobj product itself if we had such a lazy expansion built in.
Looks like you two could have a productive conversation. It'd be good to avoid duplication and make the best use of everyone's time, even for the computer ;)
Sorry, I accidentally clicked close...
Sorry, I should have mentioned in the issue text - this is an aide memoire more than anything else; just notes that I'd mentioned off-hand in a meeting today which seemed like a good starting point for something off-topic from my current project.
It's partly fleshed out method of upgrading the type and dimensions parameters that has been something people have wanted for a while, but nobody had (and still nobody has) time to do it properly.  Some exact details still need to be designed, and it's open for comments in the mean time so hopefully more eyes can spot if there are problems with this.
I do have one specific comment regarding dims when dealing with QIP. Maybe we could do something where dims (or a structure built on top of Qobj)  also handles entanglement. When, we have entanglement, the tensor structure is not as useful anymore since there is no tensor decomposition in that case!
Knowing the tensor structure is still necessary for enforcing the Hilbert spaces are of the correct dimensions, even if the resulting state can't be written as a product of states on individual spaces.  I think there's a lot more design to do to work out a sensible data structure which can really take advantage of this lazy representation, but my morning-after reaction to this is that it's very non-trivial without a lot of code duplication.
It's easy enough to imagine how it will work for operators of structure [scalar, oper, scalar] * [scalar, scalar, oper] -> [scalar, oper, oper] - in this case the final result is fully expanded in the last two states, and not in the first.  It's much trickier dealing with [oper, scalar, scalar] * [scalar, scalar, oper] -> [oper, scalar, oper].  In this case, since there's a subspace in between them, you need a specifically "lazy" Kronecker product.  I suspect that this would have to be supported via either a completely separate tensor mechanism, or the mechanisms which currently underpin the data-layer kron would have to be revisited.
At least at first, I think that's a very large undertaking, and it would really need a lot of careful design to ensure that we don't slow anything down, or balloon the amount of code to be maintained.
To @sarsid
Well, I do feel keeping the original dimension information useful. [2,2] means that I have a two-qubits ket state, whether entangled or not. I can, for instance, trace out one qubit and study the other. Otherwise, this system information has to be stored somewhere else and operation like partial trace will be more clumsy.
To really let dims reflect the current entanglement situation seems unfeasible for such a fundamental data class. I don't even know if there is an easy way to do that since entangled states can still be disentangled by further evolution.
Knowing the tensor structure is still necessary for enforcing the Hilbert spaces are of the correct dimensions, even if the resulting state can't be written as a product of states on individual spaces. I think there's a lot more design to do to work out a sensible data structure which can really take advantage of this lazy representation, but my morning-after reaction to this is that it's very non-trivial without a lot of code duplication.
It's easy enough to imagine how it will work for operators of structure [scalar, oper, scalar] * [scalar, scalar, oper] -> [scalar, oper, oper] - in this case the final result is fully expanded in the last two states, and not in the first. It's much trickier dealing with [oper, scalar, scalar] * [scalar, oper, oper] -> [oper, scalar, oper]. In this case, since there's a subspace in between them, you need a specifically "lazy" Kronecker product. I suspect that this would have to be supported via either a completely separate tensor mechanism, or the mechanisms which currently underpin the data-layer kron would have to be revisited.
At least at first, I think that's a very large undertaking, and it would really need a lot of careful design to ensure that we don't slow anything down, or balloon the amount of code to be maintained.
Both your and Boxi's comments point to the fact that this probably needs to be something on top of Qobj as a QIP-state object rather than part of Qobj itself! Once the details on these changes are fleshed out, I shall certainly look into this. It could potentially be very useful to have a slightly more customized/optimized QIP state layer on top of Qobj.
I had some more thoughts about this while responding in the Google group (see: this post and associated email chain).  I think perhaps a good solution is to move away from keeping track of the "dimensions" of each space, to keeping track of what "basis" each space is represented by.  This actually allows more safety, and I think is probably a more natural way of thinking about the underlying physics for most people.
This discussion is now superceded by the more concrete implementation discussion in #1421 - if you've got further comments, please add them there!
