Hi @jcmgray, I've been having some trouble using the TNOptimizer for complex-valued networks using tensorflow backend. I'm attaching an example where optimizing a small PEPS crashes if we try to do boundary MPS contraction.
It seems like the tensor_split method somehow uses a double dtype even if the original PEPS is set to be complex, which causes TF to get confused.
Is there something simple I'm missing?
This is using the tensor_2d branch for the PEPS stuff of course.
Yeah tensorflow is kinda picky about dtype, which is annoying, like here, because:
I'll add an explicit check for tensorflow and complex, unless I think of something else cleaner.
(By the way you might also have to use cutoff=0.0 for autodiff, as none of the compilers/autodiff computational graphs like dynamic shapes.)
Some other issues:
Slightly harder:
I've fixed svd for tensorflow and complex, but the 'tf + QR + complex + autodiff' problem remains.
It may be of interest that the latest (dev) versions of torch now support complex data - torch generally is more convenient to work with auto-diff wise as it doesn't require (slow and badly scaling) compilation of the computational graph to work pretty efficiently (unlike jax and tensorflow).
Ok so one way to skirt around the 'tf + QR + complex + autodiff' problem is override the autoray lookup of linalg.qr for tensorflow:
The following now works for me:
