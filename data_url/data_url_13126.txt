After running the calibration experiments and seeing the entire list of results, a user might want to know how each Strategy performed across all BenchmarkProblems
Function name subject to change.
Please assign this to me
@natestemen @andreamari
There are couple more similar issues #2014 and #2013 regarding calibration logs. I would like to suggest something better than just adding error values and noise level. Anyway there could potentially many other benchmarks/strategies and maintaining this kind of code https://github.com/unitaryfund/mitiq/blob/master/mitiq/calibration/calibrator.py#L104-L149 is gonna be painful because we will have different sets of columns for strategies/benchmarks. And this group_by_strategy/group_by_problem is gonna make the calibration logging even less maintainable.
What if we have only these two kinds of tables?
The database like table
and this is the group_by (cartesian product) version of the same table and it is easy to transpose it if necessary
Another benefit of using such kind of logging is that the code for the machine readable report will be almost the same (only rendering will be different)
