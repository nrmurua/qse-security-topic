The tests test_overoptimization_case and test_congestion  fail if the seed value is not explicitly set to seed=13, indicating that their success relies on having a very specific sequence of random numbers generated, e.g. setting seed=123456 makes the test fail.  The issue here is that this sequence is directly tied to how the random numbers are generated under the hood.
Currently, the random numbers are generated via NumPy, which uses RandomKit.  Generating a set of randoms using a different method, for example the C++ random lib as done in #1789, makes the tests fail, even for the same seed value.
Whats more, #1789 fixed a bug (at least I think it is a bug) where the random number generator is being reset at each call to layer_permutation.  Thus every layer was drawing the same random numbers each time.  Now there is a single random number generator attached to the swap mapper instance.  This again breaks the tests.
The tests of course have their worth, but their success or failure should not dependent on having an exact sequence of randoms.  Perhaps their is another metric for the circuit that can be used to gauge success, rather than explicit DAG equivalence.
âœ… Passes all tests with default seed value 999 for test_congestion and seed value 19 for test_overoptimization_case. Fails for the seed values shown below. Issue pointed by OP of having the test success heavily dependent on the seed of the random number generated still holds true. Issue reproducible!
Functions test_overoptimization_case and  test_congestion present in --> test/python/transpiler/test_stochastic_swap.py
Will update comment/add more comments if cause is found/fixed!
Python version 3.9.7
qiskit-terra version: 0.19.1
I'm closing this issue based on the discussion we had in #7658 (comment) Basically while these tests are quite fragile and sensitive the exact rng and implementation they still have value (albeit a bit obtusely) in finding real bugs. This test issue caught real bugs in #7658 which would cause non-deterministic behavior in some cases so I think we'll just leave the tests as is (doing exact circuit comparisons). That being said some of these mapper tests could probably be re-written to be a bit easier to work and be easier to debug (especially everything in the test_mappers.py), but that is independent of this issue.
