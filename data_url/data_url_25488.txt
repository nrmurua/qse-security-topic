The dask executor plugin cannot reliably handle simultaneous dispatches, and fails in particular for many sublattice workflows (e.g. the vary_distance workflow in the quantum chemistry tutorial as of v0.177.0rc0). This is due to a confluence of factors:
Covalent tries to share a single asynchronous Dask Client among all the tasks within a workflow that execute in the same Dask cluster. This client is cached (in the _address_client_map) upon initialization by the first task, and retrieved from cache by subsequent tasks.
Each workflow runs in its own thread in the workflow_pool. Because different threads share the same memory space, the same _address_client_map is shared by all workflows.
When one retrieves a result from a Dask Future, the Future checks whether the client that submitted the task is operating in asynchronous mode. Only when future.client.asynchronous is True can the future be awaited on.
Client.asynchronous is actually a property that compares the current event loop with the event loop that created the client. If the client was created from a different thread (in particular, from a different workflow), the asynchronous property is False. Thus, the current thread would need to recreate the Client to obtain an awaitable future. But then the asynchronous property would be False when queried by other threads. This potentially leads to a race condition if the current thread is preempted between lines 111 and 115, since the asynchronous property could revert to False when checked in line 115.
This was less of a problem before because different workflows were dispatched in separate processes, not threads, so Dask clients weren't shared between workflows.
Proposal: get rid of the workflow_pool and run workflows as asyncio tasks on the FastAPI event loop. As a side benefit, we can dispense with the hack for starting the global workflow_pool and tasks_pool.
This is through explanation of what I was experiencing. Thanks @cjao
