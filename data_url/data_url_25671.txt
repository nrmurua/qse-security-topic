This issue is similar to Issue #638: when using PyTorch with Covalent, the gradients are not evaluated properly. Note that this is not unique to the Dask executor, but the same thing happens with the local executor as well. This issue is also seen in the MNIST classification tutorial, where the loss stays more or less constant over multiple training epochs.
One would see that the result is the same as the init_point.
Result should be different from init_point if gradient descent is performed properly.
No response
See #646 for MNIST. I would restructure your code so that Optimizer creation and training happen in the same electron:
I get
By the way, the electron decorator on cost does nothing; electrons within another electron are executed as ordinary python functions.
Thanks for looking into this and the detailed explanations in #646 @cjao! I'll close this issue now.
