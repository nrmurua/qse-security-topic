One of our contributors on windows got a wild lapack error from pip install -e . (or just pip install -r requirements.txt). We were able to fix it by changing requirements.txt to just be cirq~=0.10 (i.e., removing numpy and scipy). This could be a one-off, but may be general. It might be an indication to relax the aggressive pinning in our requirements.
OS: Windows
Python version 3.8 & 3.9
I'm generally against unpinning versions as I think it reduces reproducibility in the long run.
IMO the philosophy on this that makes sense to me is that we only add things to the requirements.txt that we directly use. If we had something that returned a plot from a pandas table we would add pandas, and rely on it to bring in a correct version of matplotlib. AFAIK Lapack is a native numeric library that was likely coming in from numpy. I usually use conda oto avoid things like this (native dependencies not getting pulled in) as it can better manage dependancies not in Python.
I do agree with @nathanshammah's point on this that for reproducibility we really should pin everything. This is generally done with lock files in other languages but there isn't a great option for Python. There you would have your requirements file, and at build time the compiler would generate a lock file (if there wasn't one already). I was looking into this already for speeding up testing in CI. I found this guide helpful for the pipenv tooling, and it's pretty straight forward to do with conda (just export the env after building). Probably we will have to go one of these ways at some point, just to make things more reproducible but also easier for us to manage.
I currently can't install this package with Python 3.9 because the scipy that's required (1.4.1) doesn't have a wheel for 3.9 and building from source fails with the latest setuptools.
Does mitiq actually not support scipy > 1.4.1, or can the requirements be loosened to something like ~=1.4 so new non-major versions are supported?
@dbanty Thanks for the feedback on this! It seems like we have some issues in version specs in the requirements files when it comes to versions of those packages for windows. I am also on windows, and am seeing if I can update the file now!
FWIW I was able to reproduce this, as it looks like the versions we pinned for numpy/scipy were both pretty out of date.
Pinning is good for CI reproducibility, but bad for packaging - having requirements.txt used for both ends up satisfying neither perfectly.  If you like the CI to have exact versions, you can always have a requirements-ci.txt which pins exact versions, and that's what you use in your test actions, but it's ignored by your packaging actions, since the requirements files are just a convention and have no actual power unless you give them it.
(Also, pip freeze > requirements-ci.txt is a very simple way to generate a lock file from a known-good environment, but not super well-featured if you want more.)
Pinning is good for CI reproducibility, but bad for packaging - having requirements.txt used for both ends up satisfying neither perfectly. If you like the CI to have exact versions, you can always have a requirements-ci.txt which pins exact versions, and that's what you use in your test actions, but it's ignored by your packaging actions, since the requirements files are just a convention and have no actual power unless you give them it.
(Also, pip freeze > requirements-ci.txt is a very simple way to generate a lock file from a known-good environment, but not super well-featured if you want more.)
Yeah totes, I agree on the freezing/pinning thing for CI (also a good way to have CI cache things to save time). Mostly right now I am considering specifying everything in a pyproject.toml which make it easy to say in one place req, dev req, optional req and manage version numbers in a better way. If you have thoughts on its #867
This issue had no activity for 2 months, and will be closed in one week unless there is new activity. Cheers!
