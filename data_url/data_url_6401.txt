The prompting issue is that, depending on hardware and float precision options, Qrack sometimes "fails" precision checks, upstream in compiler and networking stacks. (Generally, I've observed failed checks at one decimal place above test passing threshold, if a check fails at all, but, again, this is also hardware and float precision option dependent.) Although, sometimes failing to pass these checks, in my opinion, does not necessarily prove an overall relative reduction in accuracy or precision.
For one thing, by design, Qrack randomizes the global phase offset on initialization and on measurement, (with an option to turn this behavior off). This might be physically realistic, if one thinks of the global phase offset as random and unknowable, (which it is, theoretically). It might be moot, if the phase is "unknowable" only because it cannot have measurable physical effect, which I also think is true.
Another major factor in numerical precision is that Qrack "floors" very small amplitudes to exactly 0. Specifically, the cutoff to "floor" has been chosen by testing the residual amplitude left behind by two successive Hadamard gates on the same bit, from a permutation basis eigenstate. Ideally, two successive Hadamards on the same qubit should return the state vector to exactly its original state. In practice, from what I've seen, very few of the other available simulators take this into consideration at all, allowing the residual numerical error to compound on sequences of gates that should ideally be exactly equivalent to the identity operator. Once Qrack floors a "residue," to compensate for this, it normalizes the state again, to correct for the "probability leak" of flooring the residue. This should hopefully improve numerical stability on long sequences of gates, though this has admittedly not been adequately tested, to prove so. Judged by the observed magnitude of this residue, depending on language, compiler, and hardware, it is sometimes not possible to actually achieve the accuracy of these upstream tests, because these residues are already too large of a source of error, though many algorithms reproduce test-passes to precision. (I should specifically note, though, Python's numpy seems, in cases, to manage a really impressive float precision, beyond the bounds of direct binary float type representation.)
One result of this is, precision checks trip at barely threshold significance. (The above explains why we go through this complicated routine, which appears to hurt precision, where most of the other popular simulators I've analyzed seem like they might take the same obvious numerical approach, hence they tend to agree on tests of precision.)
We can take further steps to improve precision, though. Our normalization algorithm is "naive" and definitely not high precision, just summing probabilities from low permutation to high permutation. This can be improved, whether it's worth the overhead.
Maybe most importantly, we should also perform comparative benchmarks of numerical stability of long sequences of gates, to test whether this residue flooring and normalization actually works, definitively.
Well, first off, apologies, because the accuracy benchmark is totally wrong. It's not even close to correct. First, I need to correct the benchmark. Then, I need to get accurate data in hand.
See #176
#274 and other recent PRs go a long way towards improving the normalization feature, and even basing optimizations on it. I think it was a mistake to think of float error on complex values with intended norm = 1 as the primary driver of the requirement for normalization. Rather, the significant source of error is flooring of "noise" amplitudes, (less than norm of 1e-14 for float accuracy, and much less for double). We have cleaned up the API, improved precision against various batteries of unit tests at our disposal, and put the flooring behavior back into the library when normalization is enabled, in which case it has truly become a feature, particularly for QUnit. With that, I think I can close this issue, for now.
