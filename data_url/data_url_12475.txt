As the last sentence "One way or the other, the classical neural network easily outperforms the quantum neural network. For classical data, it is difficult to beat a classical neural network." leaved in the MNIST tutorial, I may have some wondering.
The classical model(with 37 params) has the 128 batch_size as well as the 20 epochs to achieve more than 90% accuracy. However, the quantum model only runs on 32 batch_size and 3 epochs. When I run the test by setting the 'same (32 batch_size, 3 epochs)' configuration on the classical model, the quantum model has achieved about 50~66% accuracy which outperforms the classical model (49%).
Quantum Model :


Classical Model :


Result :

Maybe we should reconsider the conclusion of MNIST tutorial (at least, both of them should have the same ability).
I think the meaning of the sentence is that fully trained classical models (even trivial MLPs) will pretty much always outperform QML models. The only reason the quantum model is trained so quickly is just for the brevity of the tutorial, the authors note "Training this model to convergence should achieve >85% accuracy on the test set". Thus both models are trained to completion, classical models prevail. After 3 epochs neither models is done training, so it's not a very meaningful statement to say one is outperforming the other.
Additionally, with your example of running 3 epochs for the classical model, I imagine there is substantial variance in the performance. Because so few gradient steps are taken, my intuition tells me the random seed plays a significant role given the newfound importance of the initial proximity to a decent local minima. A quick run of my own seems to indicate that. I ran the same conditions and got >85% test accuracy with the classical model in just 3 epochs (better than the QML model would do with many epochs.

thanks for sharing your perspective~~ it's nice~
