Running 2.4 on Windows 7, nist_qc2 gateware
From #529 I got the impression that converting kernel invariant SI time variables to mu in kernel methods should happen at compile time and thus come without any cost.
However, I see a significant difference as compared to doing the conversion on the host.
Also, pulse_mu(seconds_to_mu(...)) seems preferable to pulse(...).
Some clarification of these differences in the documentation would be helpful.
Here's a minimal example:
I've looked at six cases:
"pulse" commands, pulse_time as kernel invariant: RTIO Underflow after 36 pulses, slack = ~-2800mu
[sidenote: in Artiq 2.0, it managed 58 pulses before the underflow]
"pulse" commands, no kernel invariants: RTIO Underflow after 1 pulse, slack ~-12000mu
"pulse_mu" commands, pulse_time_mu as kernel invariant: outputs all pulses, slack ~+39us
"pulse_mu" commands, no kernel invariants: outputs all pulses, slack ~+32us
"pulse_mu2" commands, pulse_time as kernel invariant: RTIO Underflow after 54 pulses, slack ~-3000mu
"pulse_mu2" commands, no kernel invariants: RTIO Underflow after 2 pulses, slack ~-30000mu
Could this be added to 2.5 as well, please?
Updated testcase:
Unable to reproduce on latest master. All three variants underflow at 32 pulses. The LLVM IR also checks out.
