CI fails stochastically - as rough estimate, maybe around 1 in 5 CI runs that make it to the second stage - on Windows in an amplitude estimator test (TestSineIntegral.test_confidence_intervals).  The traceback from CI is:
Note that the actual failure is that the time_taken attribute of the statevector simulator is coming back as 0.0, which fails a test asserting that it's larger.  On a quick scan through the code, I see that utils.run_circuits adds a time_taken = 0.0 attribute to its output right before returning it, if the attribute doesn't already exist.  I don't know if that's relevant.
Currently I don't have a reproducer - it's stochastic in CI, and seemingly only on Windows.
Perhaps a time_taken field isn't getting put in correctly somewhere in which case that needs correcting, or perhaps the simulation runs sufficiently fast that the timing method occasionally rounds down to 0?  This latter option has some evidence in that the failure on appears on Windows, since timing is different (and I believe less granular) there.  If it is the rounding behaviour, the solution may be to remove the failing line in the test, but it would be good to know the reason first.
No response
I'm testing this now on my windows VM to see if I can recreate it. TBH though I'm not sure how much value that assertion adds to the test. I'm not sure how asserting the quantum instance storing that it measured a non-zero time for execution validates anything about the integration operation being tested, especially because as you pointed out it is dependent on the local environment to measure time.
Ok,yeah I can reliably reproduce this locally on my vm, running the test in isolation pretty much hits this locally every time. I'm thinking your guess about windows missing the precision or is rounding a small time causing it to return 0. I'm inclined to just remove that assertion from all those tests.
