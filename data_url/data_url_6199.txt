gives
I am not sure if this is the expected behavior, but it does not align with the IBMQ jobs, and the behavior is not intuitive.
There is currently no way to check the failure of an AerJob unless you know to try to grab the results ( I do not know where this is mentioned if at all).  This is different than an IBMQ job where one has the job.error_message() method for this purpose.  Also, as in the case of the above example, there are no results at all because the job error-ed before it even began.  So to say that the result had an error does not make sense given that the job didn't even start in the first place.
The unitary simulator behaves in the same way.
Well, yes, this is the expected behavior on Aer.
What you are facing is a general error, so it affects the whole job and that's the reason why it throws when someone calls  job.result(). But as far as I remember this was the behavior of IBMQ jobs as well.
I think the way to check for errors in Aer is exactly the same as in IBMQ, with the exception that we don't have job.error_message(). This is specific for IBMQ, as the BaseJob doesn't force us to implement this method, but I do agree that we should pair behaviors as much as we can even though there are specifics of running locally vs remotely that will require to diverge. We could implement job.error_message() and have this beautil error report when there is no general error but relative to exeperiment ones.
Ok cool.  Can you please point me to the documentation that says the errors must be retrieved from the job.result() method?  I don't recall seeing it.
if you take a look at  AerJob.result() the docstring says that this method mimics Python future's result() method, which says:
That's for general error.
For experiment specific errors, you have to get the results by calling: job.result() and iterate over the list of results checking which of the experiments failed and which didn't (this is what `IBMQJob.error_message() does).
As we share behavior for all the providers, maybe a Tutorial on how to properly handle jobs could help here... thought we already have one, but couldn't find it.
I think this should be fixed since 0.3.2
