I have been looking for ways to deal with large amounts of data. As far as I understand, at the moment an array is initialized that is made to store all of the data that is generated from a loop. However, for very large arrays of data, this is not a viable approach. For instance, during an overnight measurement, if I want to save all the individual traces, this could amount to over a billion datapoints, which means several GB of memory. However, this data does not need to be stored in memory, and can in fact be forgotten as soon as it is stored on the local disk.
I was therefore thinking of alternate solutions to this problem, and was hoping people could help me with ideas. I think one possible approach would be to have the data_manager somehow "know" when to use the standard approach (initialize a full array for each gettable element), and when to use a different approach, such as having buffers, like an ATS digitizer. The buffers are continuously filled, and simultaneously stored. Once a buffer has been stored, it can be safely overwritten.
As to how the data manager should "know" about what approach to choose, my preference would go out to manually having to specify it, although some sort of automated choice depending on the array size would also be possible. In any case, I think a good additional check would be to verify that the data arrays do not exceed the memory-imposed limit.
I would like to know other people's opinions on the matter, and suggestions as to how this should be tackled. How could this best be integrated with the current QCoDeS framwork, and do people know any pre-existing packages, or some tricks, to deal with this efficiently and effectively? Furthermore, is any effort spent on this issue going to be useful, or is it going to be wasted effort because it probably won't be compatible anyway with the upcoming revamping of multiprocessing/multithreading (@giulioungaretti)?
@nulinspiratie  I think arrays are often a bad idea for data, @alexcjohnson  has a different opinion :P
In any case this is a bit more complex.
a) how big your data really is? Anything less than 16 gb, then just buy more ram.
b) what is the data used for?
c) buffers are oh so '70s but the idea is sound,  buffers in hardware exist mostly for I/O so in QCoDes what I want to do is just streaming data, so that the only memory intensive things are plotting and adaptive measurements.
I think discussions are definitely üëç but coding efforts are  likely to be "wasted" (ok, maybe not wasted because you are going to learn a lot and have fun)
@giulioungaretti , I think the reason most of us physicists think in terms of arrays is because that is something we can wrap our head around. It's simply a very long "list" of numbers.
I think the issue raised by Serwan (@nulinspiratie ) is a very real one. We will be facing these issues, and we will be facing them a lot on the short term. I would be very much open to any suggestion you have on the datasaving/handling front, both in terms of the workarounds to fix it in the current array_based paradigm, as well as in the longer term streaming format.
One thing I can think of would be to do streaming data writing to the hdf5 format and only retrieve it afterwards. Altough one could argue that that is still thinking in arrays, I don't see the fundamental difference with some kind of streaming write. Unless of course you had something entirely different in mind.
@giulioungaretti The data can range anywhere between 4 GB and 20+ GB. Although the measurement computer might be able to handle this amount of memory, I would rather not risk it. Another issue with this, is that the dataset will become extremely large, which means that saving and loading the data might be quite difficult.
The data is mostly generated simply to have the raw data. It could be useful later, for instance if we realize that the analysis of the data was incorrect.
I have currently implemented a workaround by creating another datamanager on a separate server. I have now set it up such that this datamanager is connected to the measuring parameter, such that whenever a new set of traces is obtained, it will automatically store it to the harddrive (in HDF5). This actually works really well, it creates a separate folder within the datafolder, and stores the set of traces there. And since it's on a separate server, this is all done in parallel. Although it's doing what I wanted, I'm not liking the fact that the parameter directly communicates with the datamanager. I would much rather have that this was done somewhere in the loop, something along the lines of being able to dictate which measured parameter is stored using which datamanager. Another aspect I don't like about the current method is that the Loop and also the metadata being stored, have no information on how the actual traces are being stored.
