I would like to be able to get the delay pragma to work with pyQuil with noise models. For example if I have a T1 noise model and I have increasingly long delays I'd like to append noise models that depend on the delays such that I can use the QVM to correctly simulate the behavior.
To be explicit, it would be nice to be able to run the T1 or T2 experiments in forest-benchmarking on the QVM, see e.g. https://github.com/rigetti/forest-benchmarking/blob/master/examples/qubit_spectroscopy_t1.ipynb .
What if we had something like:
This could be interpreted to mean a delay of t nanoseconds on the target qubit. For instance,
would mean that we idle for 500 ns on qubit 2.
This might be a first step to having parameterized noise, where the noise could now be governed by this t parameter.
One of the "problems" with this approach is that the parameter is meaningless in terms of the unitary representation, and I'm not sure this is a pattern I'd want to have to incorporate.
(CC @ntezak @ecp-rigetti)
This makes me queasy but I can't articulate why. I'll think about it.
It makes me queasy because we are lying about the meaning of a function mapping reals to unitaries. We would have to start treating parameters specially, not just a means to an end to generating a unitary.
Would DEFGATE IDLE(%t) be compiled to the delay pragma?
Ultimately we are trying to get consistency between the way programs run on the a noisy QVM and on the QPU. So if they were two different commands it would kind of defeat the purpose.
Perhaps the queasiness could be related to the discussion of how to treat noise models on the QVM. I might write down my thoughts on that issue and send it out.
@ecpeterson @joshcombes @notmgsk @kilimanjaro @sophiaponte  any further thoughts on this problem?
I'm sorry this is so late, but: Quilt will have a DELAY primitive. Will that suffice?
