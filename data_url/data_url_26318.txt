The current implementation of RESP charge fitting is slow. Although RESP charge fitting is intended to use for postprocessing and only for single configuration, I am interested in developing cuda/openacc code to parallelize it and include it into MD steps. Could anyone point out where should I look into and how to organize the code structure that fit into the existing code? I suppose the nested for loop in qs_resp.F in principle can be parallized in the similar way as src/grid.

Could you give a summary of what the code you linked is doing?
I see that there are a lot of branches in the inner loop. So presumably this code could be optimized quite a bit. Furthermore, one could parallelize it via OpenMP.
While GPU acceleration is an option, it's significantly more work. So, I'd suggest you first optimize the CPU code - if only to develop an in-depth understanding of the problem.
I am new to this code and fortran but I guess this function is the key component to calculate the grid values for outputting the .cube file as stated in howto:resp. I wish I could contact the original author to understand thoroughly before actually optimizing it.
I am happy to start with OpenMP. @oschuett could you point me to an example of cp2k source code that use OpenMP directives and how to compile it (change the complier, flag in the installation shell script)?
Writing cube files is almost always IO limited. So, you could try to get a fast SSD or mount a tmpfs.
However, one can usually avoid working with cube files. You could e.g. use RESP_CHARGES_TO_FILE to only write out the charges.
