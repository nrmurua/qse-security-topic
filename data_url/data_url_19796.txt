Currently the measurement mitigation constructs a correction matrix by preparing all 2^N basis states. If measurement noise is independent (ie. no crosstalk) we can construct the correction matrix from 2 (or N) calibrations (each qubit in the excited state together or separately). Still need to construct a 2^N matrix although we might be able to use sparse matrix tools.
I tend to prepare 2 calibrations (all zeros and all ones basis states). What can be the reason for preferring N (each qubit in the excited state separately) calibrations?
I can't think of one really, so let's do the 2 cals. Definitely if all the measurement noise is uncorrelated they should be equivalent.
I think both pseudo_inverse and least_squares can be done without constructing 2^N over 2^N matrices. 2^N operations are required because need to go over the entire measurement vector. But as for storing matrices, it is sufficient to store N matrices of size 2x2, and their inverses. pseudo_inverse and least_squares can be applied using these matrices without explicitly computing any tensor. So there is no need to go to tools for sparse matrices.
I could believe it...try and code it up and let's see. The tensored matrices are just conceptually easier (a few lines of code), then they work the same as if we have constructed the full matrix from all the cals.
Ok I coded it. Honestly I de facto calculate the complete matrix or its inverse, only not store them. So I'm not sure if it's such a good idea. After all storing has the advantage of not repeating calculations. And there's also readability and maintenance and usage of built-in optimized numpy functions.
I mean: at any point of time, the code maintains a constant number of vector of length 2^N.
