See code demo below
The above code will stall in tfq_parallel_kernel in the t.gradient step,  with the exact stall position at
i.e. the program has trouble with lock in multiprocessing where possible deadlock happens.
Some further aspects:
This bug is specifically related to tfq instead of tf, as plain parallel_kernel with only tensorflow works well.
This bug is related to some keras model initialization in the main process (as the first line in main in the above code). Although such model has nothing to do with logic in multiprocessing or tfq_parallel_kernel, the program stalls. If we delete the first line of main (initialization on a plain Keras sequential model), the program works fine.
Any thoughts on this?
I have just read this: https://pythonspeed.com/articles/python-multiprocessing/ and pytorch/pytorch#3492. I found the following workaround works, I will test further whether this works in my real project.
The changes are:
According to pytorch/pytorch#3492 you may also add force=True as set_start_method('spawn', force=True).
