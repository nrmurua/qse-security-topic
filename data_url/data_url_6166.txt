
Note that the 1024 statevectors being produced in the above code only amount to ~256kB of memory combined.
Removing snapshot gives:

I would guess its because snapshots are stored as json data and the json library/type uses a lot more memory than the complex arrays would.
So if it is storing all 14 qubits than it is a 10x storage overhead. If only 4 qubits than many times more overhead. Would be hard to imagine
In this case it would be storing all 14 qubits: the pass that truncates the number of qubits currently treats statevector snapshot as an all-qubit instruction so won't perform the truncation if it is present.
The problem with statevector snapshot is that you can't return a statevector for part of the system (it would have to be a density matrix snapshot for reduced state).
One idea we could do is that when you add a snapshot to a circuit it implicitly uses all the registers of that circuit, and if that circuit is enlarged by a coupling map the truncation could treat the snapshot instruction as acting on the original circuit qubits for truncation purposes (even though its always treated as an all-qubit operation at execution).
The problem with this is if the coupling map causes other qubits to be used you get varying sized snapshots for the output. Ie if you had a 4-qubit circuit that wasnt remapped you would get 4-qubit vectors from snapshot. But if it remapped to use an ancilla qubit aswell you may get 5-qubit snapshot vectors etc.
All makes sense. But that still means there is an order of magnitude memory overhead for statevector simulation here.
I think that may be inevitable until we replace JSON as the serialization format. All snapshots are stored internally as json objects, but this has the cost of increased memory usage and slower performance with snapshots during runtime.
This performance could maybe be improved if someone had time to rework the data containing to use templating for the different types of snapshots rather than json objects, and not go via json->string when importing into python.
