Using (2.0.dev+563.gc4559fc) on windows, a repository scan of a 100 of these files,
which are identical except that the class name and file name were replaced by their index, takes about 55 s. 10 files takes about 5 s, 200 about 114s. Our actual experiment folder with about 84 files takes 63s or so. This can be a long time to wait before we can run any experiments if we have to restart artiq or make changes to the repository. Is there any way to speed this up or, is 0.5 s per file something we just have to live with? The numbers are about the same on the virtual machine.
I used the following script to generate the files,
@sbourdeauducq @jordens does it take a similar amount of time on your machines, or is this potentially another case of the overzealous antivirus slowing down creation of all of the worker processes that do the scanning of the experiments?
Rather slow here as well and there does not seem to be an obvious solution (e.g. recycling workers does not help much, caching won't handle dependencies or dataset-dependent defaults).
Why does recycling workers not help much? Hasn't it been clear that process startup and importing is slow?
This problem is only going to get worse as we build up more and more experimental code, and it is also something that first-time ARTIQ users will encounter rapidly and may find off-putting, so if there are straightforward ways to improve efficiency here it seems that would be very useful.
Why does recycling workers not help much?
I don't know; this is simply the first thing I had tried and it did not seem to significantly improve things.
Actually it does, I probably was doing it wrong the first time I tried.
Should be faster by an order of magnitude.
Great, thanks. Could this be added to the 2.0 release?
