The current qGAN implementation employs numerical gradients.
Employing the gradient framework would enable to run this QML algorithm with analytic gradients.
Hi @Zoufalc !
If it's not a problem, I would like to work on this - is there anything you feel I could take a look at that would help me get started?
Hi @SashwatAnagolum,
Great!
As a starting point, I would suggest the qGAN paper, as well as the gradient tutorial.
Awesome, done!
What I've got so far is that we want to replace the numeric differentiation in the optimizer used here with one that uses a function that returns analytically computed gradients - does that sound right?
Also, if you're Christa Zoufal - amazing work, especially on your publication on QNN expressiveness!
Thanks a lot :)
Yes exactly. What now needs to be done is the definition of an analytic gradient function that is passed into the optimizer.
In order to get this gradient you need to get the probability gradients.
More explicitly, if you have a look at Eq. 8 and 9 in the qGAN paper.
These equations can be rewritten (using a frequentist perception of probability) as
L_G(phi, theta) = -\sum_l p^l[log(D_phi(g^l))] 
where p^l refers to the probability to measure the respective output from the quantum generator.
Now, we can formulate the respective gradient as
d_theta L_G = -\sum_l d_theta(p^l)[log(D_phi(g^l))] 
The same applies to L_D.
To get d_theta(p^l) you can use the gradient framework.
I.e. Gradient().convert(StateFn(qc_generator), parameters_generator), where the l^th entry corresponds to d_theta(p^l).
So to enable this, the quantum generator has to use parameters.
Issue was taken care of by #1450 so closing this
