There are some inconsistencies in simulation time when executing circuits repeatedly on GPU. More specifically I tried the following benchmark:
and I get the following numbers on CPU:
10 qubits - complex128 - CPU 40 threads
20 qubits - complex128 - CPU 40 threads
25 qubits - complex128 - CPU 40 threads
Notice that in all cases the time / nreps ratio is constant, that is execution time increases linearly with nreps, as expected since we are executing the circuit sequentially. However, when executing on GPU I get the following:
20 qubits - complex128 - V100 32GB
25 qubits - complex128 - V100 32GB
27 qubits - complex128 - V100 32GB
28 qubits - complex128 - V100 32GB
29 qubits - complex128 - V100 32GB
so the single-rep time increases when we increase nreps beyond a specific number (around 50). Moreover, while it is possible to simulate a single rep of 30 qubits on the 32GB GPU, it runs OOM when nreps > 1.
All these benchmarks are using the custom operators. I still need to check if the behavior using only Tensorflow primitives.
Here are some additional benchmarks using the defaulteinsum backend:
27 qubits - complex128 - V100 32GB
28 qubits - complex128 - V100 32GB
Here the situation is similar to CPU and as expected the nreps / time ratio is constant. Therefore the issue appears only when using the GPU with custom operators.
Yes, I agree, there is something odd for the GPU, e.g. I tried your code on the titan v, and there is something I don't understand, nreps=1, nqubits=28, I get:
the wave-function is correct, so there must be some caching algorithm.
We did some checks and we conclude the following:
Fixed in #274.
