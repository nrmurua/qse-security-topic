I'd like to implement a hash for each dataset, that is unique enough for our general applications, and short enough for humans to read :)
Reasons for that:
If we think that is useful we'd need to make sure we can generate smart hashes, where it is unlikely that we have a double version.
We could:
I have no idea how unique it is though...
we can of course make a really really long string but thats hard to put in a plot title or something like that.
Any ideas/comments about that?
What about uuid?
Also: why a hash and not a timestring, perhaps with a letter for the location, e.g. qd20161109003910
The time will make it unique, and is human readable.
@MerlinSmiles , @peendebak we also use the date_time format as an implicit unique ID. However we sometimes run into the problem that there are multiple files with the same date-timestamp (some experiments generate multiple datasets per second). I guess the uuid solves this by going to a finer resolution.
Another idea (that can be incorporated with the above) would be to use a certain aspect of git-hashes. The git hashes are generally long. However if you provide enough characters (~7) to be unique the software will automatically select the right hash. We could implement the same functionality for the qcodes hashes so that if you specify yymmddhhmm (and no seconds) it will still find it because there are no other datasets corresponding to it.
tl;dr
@peendebak @eendebakpt @AdriaanRol
Yes I was looking into that uuid too, also apparently it can be shortened whith XORing parts of it and it should still be random. I had a thought to put a long and a short version in the snapshot (to not depend on a correct implementation of a shortening algorithm elsewhere).
I use timestamps currently in file / folder names and it works ok, but I might want to rename, reorganize and do stuff.
I am not very fond of timestamps for uniqueness, if you have many measurement setups and many datasets, there might be collisions.
Also a string based on characters and numbers will be much shorter (and readable) than timestamps.
The hash does not need to be a part of the filename (that's just what we do now), so you should still be free to reorganize, rename and do stuff :).
I agree that shorter generally means more readable, however I think timestamps are generally way more readable than random hashes.
It would also ensure that the timestamp is always stored, allowing automating some kind of data retrieval system.
I'd just use the git way, as we talked before. üñåÔ∏è
Where would the right place be to generate and save that hash?
upon initialization of the dataset?
And then add it to the snapshot when saving? Or an empty file which filename is the hash, for easy finding later?
One thing to be careful about with random hashes is that you need a large hash to avoid the birthday problem. That's why GUIDs/UUIDs are so big, and cryptographic hashes are typically very large.
A timestamp plus a short random string often works well, unless you're generating a very large number of data sets each second.
And supporting substring match like Git makes a lot of sense no matter how we generate the unique IDs.
unless you're generating a very large number of data sets each second.
@alan-geller Just to make sure no hidden assumptions slip into the framework, we generally do generate multiple datasets per second. I'm not worried about this creating any problems but just to be sure.
Closing this issue since the desired feature has been added in #1002 (and we no longer develop the old dataset).
