GEEP is indeed on our list of methods that we'd like to GPU accelerate. It's actually a low-hanging fruit.
However, so far we've always stuck to double precision for reasons of numerical stability. Do you have any research indicating that GEEP would be resilient  enough to run in single precision?
