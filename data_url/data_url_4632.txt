Callbacks (eg. entanglement entropy) are disabled in the first multi-GPU implementation. The main issue is that the full state vector is required at every step during the simulation, which is not available in the multi-GPU set up.
While I don't think it is possible to have a general callback implementation for multi-GPU, for the entanglement entropy in particular we could do the partial trace operation at each state piece during the simulation and merge the pieces in the end to calculate the entropies.
Before spending time on this, I would note that it is very likely that it might be useless if we use the current partial trace implementation tf.tensordot(state, tf.math.conj(state), axes=[self.partition, self.partition]). The multi-GPU approach will be used for more than 30 qubits (otherwise a single GPU can be used and will be faster). Although I never tested calculating entropies with many qubits, I believe that tensordot will most likely run out of memory or just take too long.  A potential work around is a custom operator.
I did some quick checks regarding entanglement entropy performance and as expected I am not sure how useful the current implementation will be for >30 qubits. The partial trace takes more than 4x state memory. For example for 31 qubits the custom ops need 17GB and goes up to 80GB if the callback is used. The op is faster on GPU though, so probably the best approach for multi-GPU is to do the partial trace on the state pieces. We would just have to use more logical devices due to memory.
Before implementing this, I would suggest to change the entanglement entropy API to a gate. This will give more flexibility to the user to do the calculation only in specific places in the circuit. If the calculation is done after every gate (like now), it will take a lot of time and most results aren't that useful.
Also note tf matrix diagonalization always falls back to single thread CPU.
Thanks for opening the 3 issues, I believe this one is the most relevant for the first release, i.e. ports the default features of qibo to all supported devices. The other issues are also important but can be addressed later, after the first release.
How \rho come it takes more memory? A state is 2^n complex numbers. A reduced density matrix takes 2^(n/2)x2^(n/2), which is the same.
Yes, this is correct. This implementation takes more memory because tf.tensordot(state, tf.math.conj(state)) which does the \rho = Tr_B |\psi ><\psi | calculation probably creates copies of the state. If we do this with a custom operator like we did for the gates we can certainly improve it (it should take 2^n for the state + 2^(n - 1) for the density matrix if we save it as a vector).
But this reduced density matrix is self-adjoint. So you can keep half of the numbers, so essentially 2^(n-1).
This is also true, in fact when we generalize our custom gates to work with density matrices we should use this symmetry. For the entanglement entropy we are discussing here we need to diagonalize the reduced density matrix, so it depends on the diagonalization algorithm we use. In Tensorflow's built-in diagonalization that we use here we have to pass it as a 2^(n/2) x 2^(n/2) matrix. If we change this to a different method it might be possible to diagonalize by passing the 2^(n-1) vector.
