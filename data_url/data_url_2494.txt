While trying to merge #1465, one of the decomposition tests (cirq/optimizers/decompositions_test.py::test_single_qubit_matrix_to_native_gates_fuzz_half_turns_always_one_gate) failed on input (0.8209689113482108, 0.32097649795628813) with message:
This looks like a rounding problem but most likely require some investigation.
Regardless of the issue at hand, I noticed that this test generates random set of test parameters. This seems like a dangerous practice: with this project growing if we have more tests like that which fail randomly even with very low probability p, then the test run success probability degrades as (1-p)^N, which doesn't require large N to significantly impact development sanity. Is this random test here a one-time thing, or it is a desirable practice to have in this project?
We do fuzz testing in several places. I don't think this is a bad practice, because the failures we see are actual failures.
We do have some probabilistic tests with false positive failures, but when we add those I try to keep the probability of statistical failure very low. Perhaps 1 in a million, informally. If we had a 1000 such tests, we would see a failure every 1000 runs or so. That is an acceptably low flake number.
The main issue with the randomized tests is that numpy gets very angry at them if you attempt to run the tests in parallel. The different threads compute different test parameters, and the inconsistency is considered to be an error.
You're probably right; it just might get more frustrating to work with the flaky project if it gets out of control. Fortunately there is re-run button in Travis.
On the other hand, if we suspect that there is something wrong maybe it's good idea to just test it extensively once, when creating the functionality? Out of pure curiosity I run the concerning test 100000 times and it failed 30 times. (Each case with the similar inconsistency at the order of 1e-4.)
Fixed by #1516
