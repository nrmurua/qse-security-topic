There are a few compiler passes right now related to peephole optimization:
We should unify these into one general pass which is able to walk the circuit and collect+consolidate these gates. This should rely on the DAGDependency graph otherwise it will miss some opportunities when gates commute but are not next to each other. This should also accept a filter function that tells the pass to collect those gates that meet a certain condition (I believe retworkx can already do this). The filter functions could be:
Also I think we should just combine the collection phase with the consolidation phase. We can call it CollapseChains. I haven't found a use case for collecting a chain of gates but not collapsing them into an equivalent unitary.
Yeah, retworkx has a function for this already: https://retworkx.readthedocs.io/en/stable/stubs/retworkx.collect_runs.html it's straightforward to add a DAGDependency method to use that with a filter function arg to collect the chains.
However on the collapsing side that's where all of these functions differ pretty substantially. For example, ConsolidateBlocks builds a unitary for the collected chain of 2q gates the UnitarySynthesis pass TwoQubitBasisDecomposer, Optimize1qGates uses quaternions to track the rotation in u1,u2,u3, Optimize1qDecomposition uses OneQubitEulerDecomposer. All of these passes have different tradeoffs in computational cost and the cases they cover (and how they transform the chain of nodes they get). I agree we can and should replace the collect side with a common function, but I think unifying the consolation side will be trickier because there are a lot of little details there, even if essentially they're all collecting a chain of op nodes into a unitary and decomposing that there are a lot of little details that differ between them.
I'm wondering if it just makes more sense to have an abstract CollapseChains pass that you define a filter function and a collapse function. Something like:
So the quaternion business in Optimize1qGate can be gotten rid of I think if we just collapse into a unitary and use the OneQubitEulerDecomposer on it. In which case I think it becomes the same as Optimize1qDecomposition. Why are these 2 different again?
I guess in the case of cliffords I see the point that we want to collapse it into an N-qubit Clifford rather than a N-qubit Unitary.
So the quaternion business in Optimize1qGate can be gotten rid of I think if we just collapse into a unitary and use the OneQubitEulerDecomposer on it. In which case I think it becomes the same as Optimize1qDecomposition. Why are these 2 different again?
There are 2 reasons. The first is that the quaternion pass will simplify to more pulse efficient gates U3 -> U2 or U1 which the decomposer pass doesn't do yet (this is in progress in #5554 ). But, the other aspect is runtime performance, after #5554 the results between the 2 passes will be identical for the old ibmq basis but the quaternion pass is still faster. Even with the tuning we did in #5682 the quaternion pass is about 40-50% faster than the decomposer pass for the u1, u2, u3, id, cx basis (I haven't tested the performance impact with #5554 yet, but it will make the decomposer pass slightly slower because it will do 2 calls to the decomposer to compare the results between the 2 basis sets u3 vs u3u2u1). While that's not major, and #5554 will change the preset pass managers to always use the decomposer pass, it's still enough that if you're working with the old basis having the faster pass around as an option seems worthwhile.
I like this direction but I have a few comments:
Here is a profile of the Optimize1qGatesDecomposition:

generated with:
It looks like it's spending most of the time in the decomposer. We can tune the operator constructor further, and probably removing the compose and just doing the dot product manually will speed things up more. But there is a diminishing margin there because Operator  construction is only ~1/2 as much time cumulatively as the actual decomposer calls. But we can open another issue and/or PRs to work on this separately.
Like I said before I agree we only need the decomposition pass by default moving forward (and #5554 makes that change). I just thought because the quaternion pass is still faster to produce the same result we shouldn't remove (well deprecate) it yet while it's still faster. That way people can still use it if they're working in a u1, u2, u3 basis.
As a first step on this issue I was going to add a collect_runs method to DAGDependencythat just called retworkx's collect runs method. But I think we're going to need a new different method to collect chains on a DAGDependency. The retworkx function collect_runs is about finding linear paths of nodes inside a graph where there is only successor node (it's basically just a porting of the old dagcircuit method: https://github.com/Qiskit/retworkx/blob/master/src/lib.rs#L1102-L1171 ), but the structure in DAGDependency doesn't match that expectation for finding chains in that way. For example with this circuit:
The DAGDependency graph looks like:

If we wanted to collect a chain of 1q gates there retworkx's collect_runs() function won't work because it can't find the chain of 3 u1s, instead it would basically put each 1q gate in a separate run.
There are a few compiler passes right now related to peephole optimization:
Add this one: CXCancellation
@mtreinish right. So I think we need to modify the function a bit for DAGDependency. We are interested in collecting the maximal block of gates where all gates meet a certain criteria (here gate.num_qubits < 1). So for each node, I think we need to look at its children as well as siblings (i.e. those that share the same parents). And keep going down the graph until we cannot grow the block anymore. So in your example H plus 3 U1s will be collected.
But also your example shows how this graph is better than the DAGCircuit, because the DAGCircuit won't allow us to bundle H with the U1s (they are separated by a CNOT, but the U1s actually can commute before the CNOT).
I'm also good with the suggestion above for the structure of the base pass, and providing an implementation for the common case where we want to collect and collapse into a Unitary.
Ok this is actually simpler to implement, because code already exists for it in Qiskit.
Finding the longest sequence of gates acting on some N qubits, considering commutativity, is described in Section 5.4.2 of this paper: https://arxiv.org/pdf/1909.05270.pdf
So we need to adapt a special case of this template matching algorithm in Qiskit where it considers every gate that meets the collection criteria to be a match (instead of looking at a template library which it does now).
Looking through the paper and the template matching code I'm thinking there is a performance tradeoff here we're going to need to keep in mind. While I can look at implementing the PatternMatch algorithm natively in retworkx (unless someone else would like to give it a try), which will speed it up to some degree (how much so I'm not sure because we'll still need to callback to python for anything beyond the graph structure) and we'll definitely want to do longer term, I still think there will be a tradeoff here. When I look at the complexity of the PatternMatch algorithm vs what we do today especially for the level1 preset pass managers I think will make it too slow. I think it would be a good fit for level 3 and maybe even level 2.
So I'm starting to think we're not going to want to base all of our peephole optimization passes around dag dependency from a performance perspective. For example, the tradeoff between faster (especially after my PR series, which starts in #5915 ) but less thorough optimization in Optimize1qGatesDecomposition which is using the DAGCircuit's run collection feels like a good fit for level 1, but passes based on an abstract chain collapsing pass which does optimization using DAGDependency and PatternMatch would be a better fit for higher optimization levels and will provide better optimization at the cost of runtime.
I think as a first step we should just try to make a pass for block collection that is based on the PatternMatch, which already exists in Qiskit. Once we know it works well and it can find all blocks, then we can profile performance and tune it. I agree we may have to do simple passes at lower optimization levels.
I would be willing to give this a try.
Here are a few thoughts on the block collection part of this; I will open a draft PR later today.
I agree with @levbishop's comment that collecting is generally not unique and that we should concentrate on lightweight heuristic approaches. So a reasonably simple and useful strategy seems something like this: "starting from the inputs (leafs) of DagDependency, choose heuristically largest blocks of nodes matching certain criteria". Note that when a node is (marked as) collected, some of its immediate successors can now become leafs and may also be eligible for collecting (depending on the collection criteria).
Let's associate to each node its in_degree, that is the number of its direct predecessors; a node is an input if and only if its in_degree is 0.
Below I will describe several different collection strategies.
Taking collecting Clifford gates as an example, the overall strategy to process a DagDependency could be to repeat the following until no more uncollected nodes remain:
Intuitively, extracting larger blocks of non-Clifford gates helps finding larger blocks of Clifford gates later on.
Note that in general there can be multiple consecutive blocks of commuting gates, so the second step above is repeated multiple times until nothing gets collected.
I believe one can show that the depth of the resulting circuit is necessarily equal to the total number of collected blocks.
Any comments and additional thoughts?
I also didn't quite figure out how to tie the above with collecting runs of single-qubit gates, or two-qubit gates, or multi-qubit gates. Well, for single-qubit gates, possibly the method [1] for collecting blocks of single-qubit gates, and then splitting these blocks by individual qubits, could work. But I don't see a good solution for collecting two-qubit (or multi-qubit) gates.
I am contemplating to change the API for collapse_function (proposed in #5775 (comment) and temporarily implemented in #8319) from
to
This would simplify the pass that collects blocks of CX and SWAP gates and replaces each block by a LinearFunction: the collapse_function method would only need to construct a LinearFunction from a block of linear gates, and not need to worry about iterating over all blocks or about calling dag.replace_block_with_op -- these would now be handled by CollapseChainsPass.run(). This would similarly simplify other similar passes, such as collecting blocks Clifford gates and replacing each block by a Clifford,  collecting blocks of SWAP gates and replacing each block by a Permutation, etc..
However, with this change, the collapse_function will not have a global view of the DAG, but I cannot think of a use-case where this would be helpful.
What do you think? Better yet, are there any strong objections to changing/simplifying the API as described?
I think this is fine. The use cases of collapsing I can think of don't really require a global view of the DAG. Just needs to know what to collapse into (Clifford, LinearFunction, Unitary, etc.). And some options such as the ones here, which tell whether to collapse any given chain, or not, depending on the cost trade-offs of whether collapsing would be beneficial down the road:
https://github.com/Qiskit/qiskit-terra/blob/d59f2386cacf1e822b8791ff20c1e22bda36b83d/qiskit/transpiler/passes/optimization/consolidate_blocks.py#L40
@alexanderivrii - Can this issue be closed now since #8907 was merged?
@ShellyGarion, not quite, #8907 does two things: (1) it provides a very general transpiler pass for collecting and collapsing blocks of nodes, which in theory should be able to fit all of the different block collection strategies discussed here, and (2) it provides a specific implementation of the collect function that combines blocks of nodes that match a given filter function. Thus (2) covers use-cases such as "collect blocks of SWAP gates", "collect blocks of Clifford gates", "collect blocks consisting of 2-qubit gates only", but it does not cover other use-cases such as "collect blocks with each block consisting of single-qubit and two-qubit gates over the same pair of qubits" or "collect blocks of commuting gates". So I feel that to fully close this issue we need to provide collect functions that cover these cases as well.
