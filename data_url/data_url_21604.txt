Unsigned ints aren't implemented on the core device - I seem to remember hearing a reason once for not doing it, but that has escaped me.
I am implementing an algorithm that uses unsigned integer math on the core device. Currently I'm having to use quite a few hacks to get around the differing behaviours of signed vs unsigned ints; I'm in the process of determining whether these hacks are a significant performance bottleneck, but they definitely introduce more code and hence more potential bugs in functions that I would expect to "just work" natively.
It also seems that the lack of unsigned integers has caused issues in the past (e.g. 3e84ec2)
Implement unsigned ints on the core device
See example code for some of the workarounds, some of which are more onerous than others (i.e. comparison introduces extra branching). Also there is some unexpected behaviour with 64-bit casting; this may be a bug.
I am implementing an algorithm that uses unsigned integer math on the core device.
It also seems that the lack of unsigned integers has caused issues in the past (e.g. 3e84ec2)
This is just because this code exploits the overflow. Is it also the case for your algorithm or are there additional reasons for wanting unsigned ints?
We have a particular use case that involves exploiting overflow for unsigned ints, but in general it seems we should have the ability to use unsigned ints in kernels.  This reduces the chances of subtle errors in a variety of contexts (like the DDS register values above, but also for any similar tasks), and increases flexibility for users who may have a preference for a particular algorithm implementation.  Is there substantial penalty/complexity involved with implementing unsigned ints?
This reduces the chances of subtle errors in a variety of contexts
increases flexibility for users who may have a preference for a particular algorithm implementation
When all values are positive and there is no overflow, signed ints behave exactly the same as unsigned but with one bit less, no?
When all values are positive ...
In my use case, overflows are normal and expected, so not having to write custom right shifts, comparisons, etc would be nice.
When all values are positive and there is no overflow, signed ints behave exactly the same as unsigned but with one bit less, no?
And for example with DDS FTWs, one uses all 32 bits (thus the issue with behavior there if one isn't careful).
I guess the question is -- what is the downside to implementing both signed and unsigned ints?  Does it present any major issues or cause any performance degradations?
DDS FTWs are twos complement (signed) integers. Treating them as unsigned is not helpful in the best of cases and fundamentally wrong in the worst.
I return to my previous question: what is the downside to implementing both signed and unsigned ints? Does it present any major issues or cause any performance degradations?
There won't be performance degradations, it's just more code and a more complicated type system. There would be four integer types (32/64 signed/unsigned) instead of two.
Transferred to NAC3.
