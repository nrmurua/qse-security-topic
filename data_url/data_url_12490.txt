Dear TFQ team,
Recently, I am trying to use GPU to train a hybrid quantum-classical model with CUDA and WSL2 to speed up the training process.
However, in my experiment, I found only my GPU's memory was used, but the utilization of the GPU is almost 0% and computation seemed to be done by CPU.  I would be very grateful if someone can help me to solve this issue.
BTW, I am using TensorFlow -2.3.1, Cuda 10.1, and CuDNN 7.
Hi @FanFan-94 , thanks for the question. TensorFlow Quantum does not currently support GPU acceleration (GPU acceleration in any regular tf.*** operations will not be affected and can still use the GPU).
When we first released TFQ, there weren't that many quantum circuit simulators based on GPUs that we liked performance and reliability wise (The two most important factors being $$$/FLOP + relative speedup vs CPU, and ability simulate arbitrary Cirq Circuits).
Very recently the cuQuantum SDK was announced and appears to have some compelling performance numbers and we may look into incorporating it into qsim and subsequently TFQ in the coming months if the numbers hold up. Does this help clear things up ?
Hi, @MichaelBroughton , thank you for your reply. That is very helpful :)
