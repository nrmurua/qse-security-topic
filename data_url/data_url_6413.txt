It was discussed during PR #110 ("Device-memory-only option for QEngineOCL") that allocation on the same shared device by different QEngineOCL instances is not protected against, and this might lead to problems when attempting to allocate more memory than available. However, the same problem exists and has even fewer guards against it, for QEngineOCL allocation on host memory or even use of QEngineCPU.
A warning was added in the code about device memory allocation limits not being fully handled, and the PR was accepted to unblock a QEngineOCL refactor that is interdependent on the same code. It is understood that the unsafe condition will be fixed after the QEngineOCL refactor. However, the problem is present even in QEngineCPU and QEngineOCL allocation on general RAM heap, and has no checks, there.
It has always been implicit, until now, that the end user is responsible for knowing the allocation limits of their environment. This issue needs to be addressed, in all of its forms.
This is further complicated by QUnit. One of the potential advantages of QUnit is that it might operate under the RAM limits of maximal entanglement. QUnit should maintain this capability, while handling on-demand increase in allocation. Maybe, QUnit should give a warning when maximal entanglement would exceed available resources, but only give an error or exception if the immediate demand for allocation exceeds system limits.
Largely the goal of this issue is to eliminate surprises for the end user by helping speculatively estimate memory consumption and respond safely to OOM errors in the middle of calculations.
By OpenCL standard, the API reports a maximum single array allocation per device. In long-term memory, most GPU device RAM is segmented into 2 or more pieces of equal size. While many GPUs can (commonly) allocate maybe 2x this amount, it is not deemed safe or correct to do so by the OpenCL standard. Qrack respects this standard. We have the supported option to allocate on the OpenCL "host" instead, where system general heap will be used (possibly exactly mirrored in GPU device RAM, giving no benefit). We throw on over-allocation according to the OpenCL standard, though I know an over-allocation of about 1 qubit is commonly stable.
We might, by CMake, allow over allocation on OpenCL. In fact, I'm about to implement that, and then I'll finally close this. (We respect the OpenCL standard on allocation limits by default.
