Schemes such as Dynamical Mean-Field Theory (DMFT) or Density Matrix Embedding Theory (DMET), to name just two ideas from a larger family, allow one study the properties of a much larger system by simulating a smaller "impurity" system that is coupled to a bath. Usually there is some iterative self-consistent procedure involved in computing the impurity and bath Hamiltonian.
Since these procedures are routinely used in the context of simulating fermionic systems (sometimes on a quantum computer), such routines are essential to eventual incorporate into OpenFermion. Here are some papers discussing these ideas:
https://arxiv.org/abs/1510.03859
https://arxiv.org/abs/1610.06910
https://arxiv.org/abs/1603.08443
https://arxiv.org/abs/1508.04328
Consider this thread a place to discuss how we should proceed with incorporating such routines into OpenFermion.
The overarching theory of impurity methods in condensed matter is self-energy functional theory (SFT):
https://arxiv.org/abs/1108.2183
In all cases at some point in the variational loop we have to integrate over the frequency domain of two-point correlation functions. This can be done either as an integral over the real frequency domain or as a Matsubara sum. In both cases we need to evaluate the correlation functions for thousands of time points over a relatively long time evolution to get a reasonable accuracy, this may be prohibitively expensive for near term devices.
There exists a classical trick to significantly reduce the number of imaginary frequencies one needs to sum to get an accurate result (hundreds of points instead of tens of thousand):
https://doi.org/10.1103/PhysRevB.75.035123
... but then it's not clear how it maps to sampling a smaller set of real time points for the correlation functions.
Unless this problem has been solved in a different context, this would be a critical theory bottleneck for a practical implementation of impurity methods in OpenFermion.
As a side note, there is also a cluster method which is not obviously derived from SFT which works in a discretized momentum space: the dynamical cluster approximation (DCA)
https://arxiv.org/abs/cond-mat/9903273
It would likely suffer the same problem of integrating correlation functions over the frequency domain.
Thanks for this helpful overview Pierre-Luc! You outline some nice research directions and give some nice background. Regarding the impracticality of methods in the near-term: I've had several conversations with Bela (the author of https://arxiv.org/abs/1510.03859) who seems to believe that evaluating the correlation functions at just tens of time points would already be enough to enable truly interesting simulations in some cases. Even if it would take many more than this though, OpenFermion is not explicitly a platform for near-term implementations. Many of us (myself for one) are interested in thinking about what to do with an error-corrected device. So in that context I don't see any reason why we wouldn't welcome contributions on this topic to OpenFermion today.
Also, not all embedding schemes necessitate time evolution. For instance, DMET (https://arxiv.org/abs/1603.08443) can be used in cases where one is only interested in time-independent properties of systems in their thermodynamic limits. I have slightly changed the title of this issue because I am not sure if DMET technically falls into the category of an "impurity model", although it is obviously related.
I didn't know much about DMET, thanks for the link Ryan!
There are two time-dependence notions that have to be distinguished. There is the non-equilibrium situation where the Hamiltonian itself is time-dependent and therefore macroscopic observables also vary in time. There is also the linear response regime where a system is assumed to be at equilibrium under a constant Hamiltonian and a small perturbation at some frequency is introduced. For materials this corresponds to the spectrum of electronic excitations around the Fermi surface where a lot of material engineering happens.
Bare SFT doesn't give information about time dependent quantities which would arise in non-equilibrium situations but it yields the response of a given system at equilibrium when slightly perturbed at different frequencies.  If one could directly compute (or sample?) the k'th order time derivative (d/dt)^k of the correlation function around t=0 with some kind of phase estimation scheme then it's indeed possible to recover the frequency dependent correlation functions by doing a moment expansion.  However the convergence of the moment expansion and how much accuracy is required for each derivative has to be analyzed carefully since we are inferring information about low energy physics from short time evolution. But then if the correlation functions can be extracted from a quantum subroutine they can be used as inputs in (all?) impurity methods.
From the reference you posted it seems that DMET can also be used for linear response theory but it's not clear if it's as direct as with impurity methods.
I agree with most of the discussion here that while there are some clear theoretical bottlenecks, the need for these methods at some point means that the community at large would probably benefit from having the building blocks to work with within OpenFermion.  It's such a large undertaking however, that spending some time considering independent, reusable pieces from such a project would be worthwhile, so there would be some intermediate benefit to implementing all of this.  @pldallairedemers would probably be the expert to consult on this, but just a few pieces might be for example
The closest functions that exist within OpenFermion right now are active space truncation functions that average out core electrons, but these are a shadow of what you would need for decent embedding methods that allow for fractional occupation of the embedded space.  I want to make sure we mention also that @ncrubin recently wrote up a paper on the use of DMET on quantum computers that is relevant to the discussion here:
https://arxiv.org/abs/1610.06910
It's perhaps also worth considering whether we want to lump other partitioning schemes into this umbrella, such as active space + perturbation theory.  I imagine there are some shared pieces, but Green's function vs other approaches seem to be different enough that they might need entirely separate modules.
I'll share some more thoughts and personal observations to build on Jarrod's comments.
On the big picture side of thing I view impurity solvers somewhat as higher level methods that have correlation functions both as inputs and outputs (self-energy can be determined from the correlations functions). From my understanding translation invariance of infinite systems is mostly dealt with at this level (although the full story has many subtleties). I am not aware of a general purpose open source code for impurity solvers but I would be surprised if there is no specialized implementation (e.g. DMFT) somewhere out there. For sure there exists large scale codes in academia but one would have to look at the licensing details on a case by case basis. Figuring out a good standard to precisely specify an interface to the large scale codes shouldn't be too hard, I believe most of this work could be done by simply looking at the formalism of SFT which recovers a lot of impurity methods as limit cases.
I tend to view state preparation and correlation function measurements as two separate modules. Preparing Gibbs states is useful when we want to look at temperature dependent effects like phase transition but it's not an absolute requirement, we can measure correlation functions on pretty much anything (obviously the ground state is often very interesting).
Just for reference, here are two good papers dealing with the preparation of Gibbs state:
A Quantum-Quantum Metropolis Algorithm
Thermalization in Nature and on a Quantum Computer
(An unexplored idea would also be to prepare thermal Gaussian states from a larger purified Gaussian state. I expect there is a deterministic purification procedure for this that can be implemented in linear depth with matchgates).
Now on the problem of establishing specifications for a correlation functions module I'll start from more specific issues and end with more speculative thoughts on what should be considered in a general framework. There is some theoretical work to do in order to get a clear picture of the complete approach.
The main advantage I see to working toward a general framework is that it would become possible to quickly apply new concepts and methods between different branches of physics. For example, recent ideas in quantum gravity could be used to study and understand new effects in quantum chemistry and potentially discover new classes of phenomena. The main difficulty mostly comes from the facts that quantum simulators will likely transform the workflow traditionally used in classical computations and we may need a few wrappers to interface with older codes.
As a final disclaimer I'll say that I'll be happy to advise on the theory framework but I don't expect to have time to do the actual code implementation myself. I'll try to make myself more familiar with the DMET approach.
Thanks both Jarrod and Pierre-Luc for the great discussion. I don't think Jarrod or myself will have time to dive into this project at this point. But if another contributor (ideally one with a condensed matter background) wants to work on this in the future, I think Pierre-Luc would be an awesome source of advice / feedback on the implementation.
