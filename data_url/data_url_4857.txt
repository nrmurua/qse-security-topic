I tried to reproduce the 600-qubit VQE mentioned in the whitepaper by running a script slightly modified from examples/vqe_extra_mpo.py. Modifications are:
I use a machine with 36 cores and 251GB memory run the script. The backend is set as tensorflow. But the job is killed because of being out of memory, while the whitepaper shows that a 40GB GPU is enough for a same job.
Is the out of memory error raised at jit staging time or runtime?
Also, one should set a rather large time buffer for the cotengra optimizer to search for the better tensor network contraction path, max_time=360 is not enough. The path searching for the n=600 problem may takes one hour or so to get an reasonable path. The way to check whether the path finder is good enough is focusing at the summary information of contraction path, the WRITE metric is equivalent to the memory consumption 2^WRITE up to some constant overhead
Is the out of memory error raised at jit staging time or runtime?
In my script, I suppose jit is not enabled. So the out of memory error should be at runtime.
And thanks for your suggestions, I will have another try by setting a larger max_time.
The memory cost is reduced to ~30GB when I use a max_time=3600 for finding the contraction path. Many thanks!
