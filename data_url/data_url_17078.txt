We need to improve the way we test visualization, that's out of discussion. However, it is not fully obvious how.
The way we compare images is implemented in QiskitVisualizationTestCase.assertImagesAreEqual (test/python/visualization/visualization.py). As defined, it is very unstable. How images are generated very much depend on non-controlled factors, like the available fonts. At the same time, it seems that is not sensitive enough. Tolerating some difference (in order to handle uncontrolled factors) makes relevant differences hard to detect. If we are willing to reduce the tolerance to the point that semantic changes are visible, we need to consider the CI as the "ground truth". For that, we need to save unmatching images and update the references (using PublishBuildArtifacts like in here)
For the latex drawer, comparing the latex source seems the way to go. For the matplotlib case, we should be able to mock matplotlib.figure.Figure.  But I dont know how complicated that can be.
Any other idea?
Regarding the comparison of images, where the tests pass in CI and need not pass locally (as mentioned in #2949), would it be better if it is also set in the test scripts. To elaborate, instead of setting it as an environment variable just for the CI, it could also be set in the makefile and tox.ini as well. Thus allowing for tests to not necessarily be constrained to mention any specific backend in their code. Although it will force them and future tests to use ps, dependencies and quirks such as this, regarding testing could be mentioned in the README (for example). This would also help in deterring future mishaps(for backend) regarding tests for visualizations by new contributors (such as myself).
Please do correct me if I'm wrong in my understanding of this (and #2949's) situation.
So my issue with image comparison tests, beyond their fickleness based on a lot of environmental factors, including but not limited to the mpl backend (which is hardcoded for testing in #2949), is that they're not actually testing things are correct. Image comparison tests just test the status quo which may or may not be correct. We're encoding the behavior of the current output in our reference images not actually what we view as a correct. We've had instances in the past where we've had a bug with barriers in the reference images and had no idea. Another perfect example is #3052 which if we had latex image comparison tests (or latex source comparison tests) would fail. Even though the output with #3052 is objectively more correct in pretty much every case the tests would fail. The tests do not tell us if we have a bug or not, they just indicate when we've changed something, which doesn't seem like much of a value add. Especially when weighed against their general instability.
^ ditto. We are not testing against any ground "truth", just against some status quo. Catching actual visualization bugs have always come down to a user noticing something is off, and reporting it. So why not just remove these painful tests?
As pointed out by @mtreinish and @ajavadia it appears that the main problem of visualization tests stems from the not so stable QiskitVisualizationTestCase.assertImagesAreEqual, I'd like to propose an alternative to that. Matplotlib already has a robust way of comparing images, where some of the fickle factors such as font, crop and alignment are taken care of. There seems to be two ways to go about it as mentioned here.
But this all depends on the use case. Again as pointed out, the tests needn't just detect bugs but also reacts to any changes made. Since the aim is detecting bugs, then check_figures_equal (mentioned in the link) is probably a better alternative, as it doesn't require reference data, just a reference function. (this could also prevent the repo from having to save image reference data, and having to update reference data after any changes are made)
However if the majority is inclined to get rid of the image comparison tests, another alternative could be to test the numerical data returned from plotting, rather than testing the plots themselves, when applicable.
Do tell if these sound like valid approaches to the problem.
@drholmie thanks for looking into this. The check_figures_equal seems good to me and I would be very happy if the reference images get cleaned up from the repo. Are you able to work on this?
Thanks @ajavadia! I'd love to work on it! However we should probably have a structured way of going about it. Considering there are multiple implementations regarding this issue, one of which seems to be this regarding mocking the libraries for the latex figures.
If we're tackling non latex images, that is also fine. I'd need to apply the decorators at the other tests, in the right spots and make the appropriate changes. I think going about it through multiple PRs would make things easier. I'm also open to other ways of going about this. If the method I suggested is fine I'll start working on it after confirmation.
I have no problem with https://matplotlib.org/devel/testing.html#writing-an-image-comparison-test, but i'm curious to know how would it work though. For example, we want to compare text, ignoring font.
Since check_figures_equal uses a reference function instead of a reference image, the font and other local factors would be the same when generating and comparing both images. More about the implementation of check_figures_equal can be seen in the code. Do tell if this is alright.
One question with check_figures_equal, if both figures have to be generated live before they are compared, how can we draw the reference figure without entirely replicating all the drawing logic from the visualization method in the test? So e.g. would tests look like
check_figures_equal seems better suited to cases were you have shorthand and longhand ways of performing the same operation, and want to validate that they continue to be equivalent.
image_comparison, while still requiring images in the repository, has kwargs to either standardize font versions, or remove some text elements entirely prior to comparison that sound like they target solving some of the problems we have been having. Have these been tried?
Ah yes, @kdk thank you for pointing this out. check_figures_equal would require a scenario like your example. I was more inclined to try and use both ways (check_figures_equal and image_comparison) of going about it based on the visualization test (as @kdk had mentioned later on). Some tests, if I've understood them correctly, like test_circuit_matplotlib_drawer.py can be done using check_figures_equal. For e.g:
Others like test_gate_map.py, if I've understood correctly, would benefit more from image_comparison.
In the qiskit-terra repo, the usage of image_comparison and check_figures_equal have not been tried out for visualization tests.
Do tell if I've made any mistakes in my understanding of the situation, and if the way of using both check_figures_equal and image_comparison on a case to case basis is acceptable.
ping @kdk @drholmie @1ucian0, any consensus on this?
Hey there! Sorry for the late response, have been busy with my semester examinations. However, found some time to experiment with the usage of the check_figures_equal decorator. Unfortunately, it isn't as simple as I had previously shown. My sincerest apologies for the misinformation. @kdk's example showcases the issue with check_figures_equal correctly. But image_comparison could still work out as it at least standardizes some of the parameters. If we need more control over the parameters (outside of the ones mentioned in image_comparison) then mocking the libraries as stated here would be more suitable.
Although I'd like to have a clearer picture on what the fickle parameters are(other than font and backend, which seem to be a few of the reasons for tests failing), depending on that we could make a better decision on which way to go (either image_comparison or mocking).
