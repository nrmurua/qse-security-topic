Currently tfq.convert_to_tensor uses just one core and makes use of Cirq serialization protocols. They are pretty slow for large circuits. A quick benchmark shows that more than 95% of all time spent computing in tfq.convert_to_tensor is spent in the cirq serialization logic and the protobuf SerializeToString function. Since it's unlikely we can speed either of those up quickly, perhaps we should look into parallelization of tfq.convert_to_tensor ?
Hey Michael, I am new to tfq community and I would love to work on this issue!
Welcome! Glad you’ve taken an interest. I’m optimistic we can make things a little quicker :)
@MichaelBroughton Where can I start, I went through the code in tfq.conver_to_tensor
After you've read the code you could:
Ok thanks, I will give this a shot
@MrSaral are you working on this issue? If so, please request to be assigned to it
Hey @tacho090 , Yes I am working on this. Please assign it to me.
Hi! I'm new to the TFQ community and would love to tackle this problem. Can I be assigned to this issue? I'm more than happy to work on this.
Go for it, feel free to open a PR for it
After you've read the code you could:
Hi, I've been working on this issue and I had a couple of questions. I read the benchmarks/README.md file and tried to use Bazel for benchmarking, but I ran into a lot of errors.
How would you like me to time my code? Should a custom benchmark file suffice or should I be using the existing benchmarking system (with Bazel)?
Is there any specific reference circuit you would like me to use? I want to try multiple circuits that stress the code in various ways but I'm not sure what to look for.
That's all the questions I have for now. I'm not sure if I'm supposed to be using Bazel in the first place.
