I called the Monte Carlo event probability function which has some issue with the decomposition functions for gaussian backend. I tried to solve the error by changing the tolerance level in the rectangular function in decompositions.py. This did not help and on printing the 'diffn'  in the rectangular function, values >1 were also seen. The error looks like this:
Expected behavior: monte carlo feature vectors outputted
Actual behavior: ValueError seen
Reproduces how often: The error might or might not occur when I change the n_mean or the adj matrix.
System information:
My code snippet:
events = [2, 4, 6, 8, 10, 12]
fv = np.array(similarity.feature_vector_mc(nx.Graph(test), events, 2, n_mean=8, samples=1000))
where test is the adj matrix.
I noticed that different functions have different tolerance levels.
Also, I don't think this error comes from Walrus.
Thanks @AroosaIjaz for reporting.
@trbromley, it looks like the apps layer might be generating a non-unitary interferometer?
With help from @nquesada, we tracked this problem down to the decompositions.takagi() function. This function can return a non-unitary matrix when input matrices that are highly degenerate.
One common case for a degenerate spectrum is when corresponding to a graph, perhaps why @AroosaIjaz noticed this through the graph similarity application in the apps layer.
It's quite an unpredictable bug, i.e., not all degenerate matrices result in non unitary output. However, we managed to find that the adjacency matrix of nx.balanced_tree(2, 4) reliably results in takagi() returning a non-unitary matrix. To accommodate for this, we have updated the takagi function to treat real-valued matrices differently, using a simpler approach that correctly returns a unitary matrix.
I think there may still be a chance that highly degenerate complex-valued matrices can cause a problem, but @nquesada is working on updating takagi() even more.
Nice detective work @trbromley @nquesada 
Thank you, guys!
