a few notes for larger simulation when we want to do it in the future
currently we are using Int64 to store the configuration in subspace, which is fine, but this is not safe, and may cause potential errors when the system reach 8x8 scale
currently we hardcoded int64 in our subspace since we didn't consider simulation at 8x8 and beyond scale at the beginning, after recent simulation of the MIS experiment,
it seems quite optimistic to simulate 9x9 system with potential int64 indices support in CUDA and the new native multi-card SpMM implementation in CuRydbergEmulator.
However, in the 9x9 scale, int64 is no longer sufficient and will overflow, we will need to generalize our configuration representation to Int128 and even BigInt
this is something preventing us from doing the graph 100 on 8x8, it's not really an obstacle but more because of historical reason, CUDA only supports Int32 indices until CUDA 11 was released in May, 2020, and the CUSPARSE support in CUDA.jl is not yet updated to CUDA 11, so it would take a few PRs to upgrade the entire wrapper to adapt the new CUSPARSE API changes in CUDA 11.
