While doing multi-GPU benchmarks I noticed that a significant portion of the simulation is spent with the GPUs on ~15% utilization and the CPU using as many threads as the number of GPUs. I believe that during this time Tensorflow transfers the state vector pieces from CPU to GPUs and back. My question is whether Tensorflow does this optimally or if there is any chance we could improve on this.
I have been playing a bit around this but I concluded that Tensorflow manages memory automatically even for the custom kernels so it is not trivial to implement the device communication ourselves (at least with our current code). For example if we define a CUDA custom operator that takes two tensors and one of them is casted on the CPU then Tensorflow will automatically copy it on the GPU first and then apply the operator. The same happens if we attempt to pass a tensor casted on GPU to a CPU op, Tensorflow will create the copy automatically and then apply the op. I am not sure how easy and if it is worth in terms of performance to try changing this ourselves.
Obsolete issue, ref. https://github.com/qiboteam/qibojit.
