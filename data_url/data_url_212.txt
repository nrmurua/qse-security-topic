Currently, we build our docker image (cuda-quantum) against OpenMPI.
The MPI support will thus be built and linked against OpenMPI (see cudaq::mpi namespace).
HPC centers often require specific MPI implementation (e.g., MPICH). For example, we couldn't use the cuda-quantum docker image with full MPI capability if MPICH is required.
There are a couple of solutions to this:
Provide Docker images for all MPI flavors (OpenMPI, MPICH, HPC-X, etc.)
Decouple MPI dependency via a plugin-like interface.
Pros and Cons:
(1) is straighforward and requires the least initial effort. The downside is CI runners's time and container registry storage space.
With (2), libcudaq becomes independent from MPI (portability) but requires code changes.
A plugin-like interface involves:
Some high-level C-API wrapper of MPI API (as a list of function pointers), e.g., this cuquantum MPICommunicator struct
We could provide the plugins for some commonly used MPI libraries. Otherwise, this plugin code can be compiled natively on the host and injected into cuda-quantum via dynamic library loading.
If possible, it will be great to have this available before the workshop at nersc next week. Thanks!
We are going for Option 2 here, and will specifically provide the plugins for OpenMPI and MPICH (included in cuquantum). cuQuantum uses a separate plugin interface for custatevec and cutensornet. We will create a single CUDA Quantum plugin interface that contains all of what CUDA Quantum or any of our backends need, and is also used/usable as the plugin for the respective cuQuantum backend.
