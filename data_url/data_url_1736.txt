Recently @balopat and I discussed the possibility of adding CI tests to Cirq to check changes in TFQ compatibility as new PRs come in.
We were brainstorming something along the lines of a github action in Cirq that would pull from TFQ master and then bazel test all of TFQ with the changes from an incoming Cirq PR to identify any breakages that might come up. Right now It's pretty time consuming leading up to releases to comb through changes and find breakages on both the Cirq and TF dependencies that TFQ has.
Thoughts @dstrain115, @mpharrigan , @viathor ?
We should release cirq at a more regular cadence, document breaking changes, and use this pain as a good measure for how much we're impacting our (outside) users with churn.
If cirq is successful, we can't add every descendant to the CI
We should release cirq at a more regular cadence,
+1 on that - we are 2 months late with my original quarterly plan...
If cirq is successful, we can't add every descendant to the CI
While I agree that we can't do that across all projects, but I think at least within the quantum AI projects (qsim, tfq, openfermion) we could have a bit more feedback / automate the confidence in integrations. I wouldn't think this would be a blocking thing, but more of a monitoring one. That can be actionable by the time we release.
Where these tests would live is a question - we could add them in subprojects, or we could host them here. Maybe you're right about that that integration with cirq-master could be a pattern easier done in dependent projects - as otherwise we'd have to maintain all these separate projects' infrastructure. The benefit of having all of them here would be that with a single pane of glass you could have confidence that the Quantum AI projects (the Cirq platform) are working well together.
Some more food for thought: Sometimes the breakages we find in TFQ don't occur on nice API boundaries to Cirq. So even if you guys did a 100% flawless job of deprecating things and documenting breaking changes and tracked churn it might not translate over all that well to the kinds of problems we have seen in the past on TFQ. Two such examples are:
Part of the fun (horror) of having TFQ sandwiched between Cirq and TensorFlow is trying to keep compatibility with both sides so our builds, CI, docs infra etc. keep working. If we aren't huge fans of adding a CI test, I do hope we can find at least some way to know when the breakages come up in real time so we get the chance to deal with them (or not) as they happen as appose to having to comb through TF history and Cirq history and try to guess and check where it all went haywire or worse having to try and orchestrate some kind of rollback/compatibility fix that could've been easily avoided if we caught the break earlier.
Luckily I played around in the last couple of days with TFQ with the latest version of Cirq - it was very fruitful right before the release:
However, I am now pretty convinced that the best way to test whether TFQ works (i.e. builds and runs without error) with the latest Cirq master is to run the scripts/test_all.sh against the latest Cirq master commits. During my testing I was using
instead of the regular cirq 0.9.1. rule. Are rules something you can easily override somehow in Bazel for CI? (I'm really no Bazel expert). If yes, you could just create a Github Action that runs scripts/test_all against the cirq master daily.
I would suggest moving this issue to the TFQ repo and tracking this work there.
Follow up from the sync is:
Action items have been addressed in TFQ - closing issue.
