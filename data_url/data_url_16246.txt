When calling execute on a list of quantum circuits of length greater than one, the __main__ running process gets duplicated x9.
Run the following script:
This will print Job successfully run x9 times. If circuits = [test], the issue disappears, while increasing the length of the list produces the same x9 duplicity. This behavior is persistent on different backends, including non-simulators.
The script should only run once. This bug might be unnecessarily overloading the backends.
My assumption is that is another instance of Python 3.8 multiprocessing on macOS switching the default method for launching processes to spawn from fork. In general python multiprocessing with macOS >=10.14 (especially on 10.15) has proven to be quite problematic. My guess is here that your script not having if __name__ == '__main__' in it is getting rerun on each worker spawn as part of transpile() and assemble()'s inner parallel_map call since the newly spawned python process can't differentiate between what should be run in the main processes and the sub processes. Can you try changing your code to be something like:
and see if that addresses what you were seeing.
Unfortunately there's not much we can do around this since the issue is in python and macOS. On master we've changed the default on this to not use multiprocessing by default on macOS because of issues like this see: #5324.
That solves it, yes. And I confirm that python3.7 does not reproduce the issue.
I just tested this with current main on MacOS 11.6 with Python 3.8.2 and the Job successfully run line shows only once. Assuming this was fixed by #5383 and closing.
@1ucian0 fwiw it's debatable whether this is actually fixed or not. #5383 changed the default to not run in parallel on macOS with python >= 3.8. So it works for you locally because it's not trying to run in parallel at all. You should be able to reproduce it (for certain types of installs of python, homebrew vs conda vs upstream, but not all) by setting the QISKIT_PARALLEL=TRUE env variable to force it to run in parallel by default.
I'm actually fine with closing this, I was just commenting it's not as simple as being "fixed" by #5383. There's not much we can actually do here besides potentially improving the documentation. It's a limitation with macOS on python >=3.8 (and windows on all python versions) where it uses spawn for multiprocessing which has additional concerns around it's use (vs fork which was used on python < 3.8 on macOS).
