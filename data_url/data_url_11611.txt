For chemistry module though we interpret and return a result at that level, we also include the complete original result AlgorithmResult from Aqua so a knowledgeable user can get access and see aspects that may be useful
def algorithm_result(self) -> AlgorithmResult:.
It would appear for optimization that the 'results'  field might contain this. A comment was raised by Bryce in public aqua Slack channel recently asking about access to info around how long QAOA ran. It seems that 'results' for the Min Eigen Optimizer, as below, is though set to a qubo convertor rather than say the min eigen solver result from Aqua.
https://github.com/Qiskit/qiskit-aqua/blob/0d216b1212e56a88c260f0b563e93af8e1350e42/qiskit/optimization/algorithms/minimum_eigen_optimizer.py#L175
It would be good to unify this access across the application modules so its done in a common way for users and do so appropriately too for iterative algorithms where Aqua algorithm(s) may be run many times for a single end user problem.
One location where the information returned to the user can be improved is in the recursive_minimum_eigen_optimizer. These optimizers run, for instance, many different QAOAs. However, the result object could contain more information than what is currently returned
https://github.com/Qiskit/qiskit-aqua/blob/854bd820b43a0c3f981fae588d1a0323a6e3486d/qiskit/optimization/algorithms/recursive_minimum_eigen_optimizer.py#L212-L215
So far, the user only receives the result at the end of the optimization. Here, it would be useful to have access to information that happened at each iteration of the recursive optimization. For instance:
In discussions with clients, there have been requests for more timing information within the optimization results object.
Clients are keen on benchmarking wall-clock time for hardware and simulator experiments.
I realize that there are many executions that occur within something like a QAOA run. Perhaps it's not necessary to return the raw results object of all hardware runs (which could be tens of thousands of iterations).
A few possibilities are to return something along the lines of:
For each energy evaluation, return the average and stddev
This information is already available via the callback that Aqua has on variational algos. When the algo is created the callback can be provided which will receive that information when it runs within the optimization module. The wallclock time it takes for each eval is evidently the time between each call (only the time for the very first eval would not be known since the callback is made once its complete).
total wall clock time would be better than nothing
Wallclock time, total evals and other info is in the Aqua algorithm result, i.e. the QAOA, or VQE result etc. The actual execution time, excluding queuing time, has often been asked for but I still do not believe that information is passed back anywhere still.
I have reviewed the optimizers and tried different approaches, so I want to summarize my findings before diving deep in the actual implementation. There are a lot of things we may additionally return from optimizers. The problem is that I can't propose a common data structure that covers all possible results, they are too different, without introducing something like a Dict or Any.
In my opinion there are two ways:
For (almost) each optimizer we may have a dedicated result class with additional strongly type properties (property names and type hints). There may be one additional property or a few of them instead of plain raw_results. In case of MinimumEigenOptimizerResult it will be an additional eigensolver_result: MinimumEigensolverResult. In case of RecursiveMinimumEigenOptimizerResult, a new result class that will include history: Optional[List[MinimumEigenOptimizerResult]] with the full history from the iterations. And so on. The benefit of this approach is that all the properties are well defined in the corresponding classes and there's no need for the raw_results property. The drawback is pretty obvious you need dedicated result classes (almost) every time.
We may consider 1) as a mainstream and try to keep raw_results empty, optional, leaving it for other not covered cases. The positive side is that this approach is flexible but from other side somebody may put everything to raw_results instead of thoroughly writing down all the properties.
Put everything that I mentioned in 1) as a raw_result without actually specifying the type of the underlying data, except the documentation. So, it can be an object, a list, or a tuple in the raw_result. Basically it means we keep everything we have now but just add more data as an output The benefit is that you don't need to create a new result class for each optimizer. The drawback is that there are no type hints.
I'm in favor of 1), option 2) is fine also, and I'd like to avoid 3). So @t-imamichi, @woodsp-ibm, @stefan-woerner what do you think?
In the Aqua algorithms we used to return a dictionary, and while it is flexible, as you indicate, it lacks easy discoverability just by looking (or inspecting with an IDE) the result, nor is there type checking and keeping consistency across different implementations is much harder to keep the keys and values consistent.
Hence in Aqua 0.7 we started moving over to properties on a result class with a common base class, AlgorithmResult. which all algorithms can extend. It behaves as a dictionary too, which gave backwards compatibility, and printing the whole result is easy since it has this dictionary form. Sub-classes added their own unique properties. At this level the entities are self-contained if you will.
For chemistry, like the optimization module, it can be asked to solve a problem and returns a result object. The result is built out by interpreting the aqua algorithm result (VQE, QPE etc) and combining that with information the chemistry module has itself to form the chemistry result at that level. As part of this result the "raw" uninterpreted result from the aqua algorithm is also included. The user chooses say which MininumEigenSolver to use (VQE, QPE) etc and we store the result from that too so it's accessible should the user want it. For instance they might want to keep the optimal point returned by VQE to use it in some future calculation; inspect how many evaluations the optimizer passed to VQE took etc. Now if the application module uses an algorithm repeatedly I guess we can consider whether its worthwhile to store each algo result and pass them all back, or perhaps just the final one. Maybe the algorithm uses more than one algorithm, e.g. VQE to get a rough answer then QPE to refine it, in this case it seems to make sense to return both (while we have a notebook that does this we do not have an application in the chemistry stack at present like this)
My take/preference is 1) as it's consistent with what we are doing elsewhere. As far as 2) and adding a raw_results field - if this is done I think it should be a dictionary so fields in there at least have some description. As far as it being abused we all review the code here so can keep an eye on things. If we tried to do without it for now, at least to start, it could always be added later if it was felt it was needed.
Thanks a lot for your detailed comment! I'll update the result classes according to 1)
Iâ€™m wondering why some algorithms include converters as part of OptimizationResult even if the results are already interpreted by the converters,  e.g. MinimumEigenOptimizerResult includes a qubo_converter. What kind of use case is supposed?
@t-imamichi I decided to remove qubo converters considering our discussion on slack :)
I'm in favor of option 1 as it adds discoverability. I also like the idea of having the history in the RecursiveMinimumEigenOptimizerResult as was put forward above. This should allow us, among others, to keep track of final and initial points used by the classical solver at each step of the recursion.
