When using torontonian_sample_state to generate samples from a Gaussian state specified by a covariance matrix cov the time taken to return the same amount of samples from a displaced Gaussian state as opposed to one with zero displacement is significantly more. For example when I specify to generate 250 samples from a set of Gaussian states with average photon number of 3 and max photon number of 15 with no displacement the program finishes in about ~8 hours. When I specify a mu of 0.25 or 0.10 on each mode the program should take longer as there are more probabilities to calculate but not as long as it does.
The program doesn't finish  (>7 days)  when the mu parameter is a 2N list of all 0.25's or all 0.10's. This is confusing as I don't think displacement of this magnitude should have this much of an affect on runtime. Perhaps I am using the mu argument incorrectly?
Two times
No response
No response
Hi @amanuelanteneh, thanks for reporting this.
Can you please post the output of running: import thewalrus; thewalrus.about() ?
Also, if you can post your full code it will make it easier to see if we can reproduce your problem.
Hi @CatalinaAlbornoz,
Here is the output of thewalrus.about():
This is the full function that i use to generate the samples and the only place I use functions from the walrus:
It takes in the adjacency matrix of a graph and uses functions  from the walrus to embed the matrix into a GBS device which i then sample from.  I call this function multiple times for a dataset of graphs. The function seems to run in the expected amount of time when displacement equals 0 but for displacements of 0.1 and 0.25 on each mode the runtime increases very drastically (from about 8 hours to over 7 days) which i don't believe should be the case.
Thank you @amanuelanteneh. We will take a look into this.
@CatalinaAlbornoz Thanks! It might also be that this isn't how the displacement is meant to be specified however I can't find any example code on how to sample from a displaced Gaussian state using thewalrus so I'm unsure.
The slowdown should only be 2x. How many modes are you working with?
@ziofil on average the number of nodes in the graphs I'm looking at is about 25 so around 50 modes on average.
Those are a lot of modes... I'm wondering if the slowdown is due to other effects (like memory swapping and so on). Could you try with fewer modes and see if the slowdown is the same?
also, how sparse is your adjacency matrix?
The adjacency matrices range in sparsity but the majority seem fairly sparse.
Thank you @amanuelanteneh. Does having fewer modes give you the same slowdown? Or do you have the same slowdown with fewer modes?
Hello, I've been running some more simulations and it appears that the issue is no longer a problem. Sorry for the delay in response.
