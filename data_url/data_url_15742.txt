For SPSA as well as QNSPSA you can set learning_rate=None.
When calling the .settings() property of the optimizers after running the optimization, it will return learning_rate=None again.
However, the effective learning_rate is actually changed by the calibrate function. Can self.learning_rate be updated accordingly?
@Cryoris @des137 @mishmash
Yes that's an interesting information, however it's not quite straightforward to resolve because
One possible solution would be to print this information into the logger so that if the automatically calibrated learning rate is used, it would show something like
would that resolve the problem? Or are you really interested in the actual generator without the above information?
Thank you for looking into this, @Cryoris. A (semi-intermediate) solution which can print the learning rate in the logger file sounds like a good idea. Basically, we would like to know (and, possibly store) the learning rate after the calibration step.
Do you want to store it to use it later in the code or just to read out the values?
Just to read out the values of both the learning_rate and the perturbation.
Ok, then I'll add the logger for now üëçüèª
@des137 could you see if the info in #7109 matches your expectations? üôÇ
My apologies - I hadn't noticed that we were waiting for feedback.  Please comment here or open a new issue if #7109 didn't fully satisfy what you wanted.
The changes look great. I did reach out to Julien. Thank you, @jakelishman.
