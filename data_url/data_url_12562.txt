In the hello_many_worlds:
https://www.tensorflow.org/quantum/tutorials/hello_many_worlds
2.4 Input crcuit definition
datapoint_circuits = tfq.convert_to_tensor([
noisy_preparation
] * 2)  # Make two copied of this circuit
There are two copies of the circuit, one for each datapoint.
I am wondering if we had 10000 datapoints, then we need 10000 copies of the circuit?
Or it just runs the same circuit for 10000 times?
If you have 10,000 unique datapoints (each with a different circuit) then yes you would need 10,000 elements in that tensor. If your datapoints aren't unique then you likely don't need a length 10,000 tensor. To clarify in that example the datapoint_circuits represent the data points themselves and there is a different circuit that has these data points prepended to them which does the modeling. Does this answer your question ?
Thanks @MichaelBroughton  for your help. I was referring to the comment of "Make two copied of this circuit". Making copies means that they are creating a list of the same circuit. I am curious about how this is implemented in Google's hardware.
Making copies does indeed mean that those datapoints were the same. If we wanted to run this on a true chip , we would simply execute each full circuit one by one on the hardware.
Thanks @MichaelBroughton ! That helps me to understand it.
