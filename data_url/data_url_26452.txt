My workflow involves a relatively large amount of singlepoint energy/force calculations at the PBE, hybrid, or MP2/RPA level of theory, for hybrid organic-inorganic systems with unit cells of about 150 - 500 atoms. Calculations will only ever be performed on a single node (or less), which means that the host MPI need not be used, and I have full freedom in the container to compile and run CP2K however I want.
I was wondering what the general recommended setup would be in this particular case, in terms of both installation (which external libraries to include during compilation) and execution (MPI/OpenMP; nprocs/ncores/binding/...).
Should I just use the official CP2K 2023.1 image or would it be beneficial to modify it in some specific way?
Did you have a look at this readme? You can try a pre-built fully-featured CP2K v2023.1 container or use/adapt one of the apptainer definition files to build your own container.
Oh I must have missed that, thanks!
When performing single-node calculations, is there a reason not to choose the SSMP version? I.e., does there exist a certain core count above which using multiprocessing is actually faster than multithreading? (For the calculations I mentioned above, i.e. energy/force evaluations at PBE/PBE0/MP2 level of theory)?
While CP2K had MPI support from the start, the support for OpenMP was a late addition. Usually, the loops were simply annotated with $OMP PARALLEL DO without partitioning the data. As a result many parts of CP2K do not scale beyond ~4 threads.
I'm running into the following warning during the SCF: Setting real_kernel for ELPA failed . The corresponding input file is attached. The calculation was executed on 64 AMD EPYC cores; 1 MPI process per core, 1 thread per process.
a520_pbe.txt
Is this something to worry about?
Is this something to worry about?
In my experience not. You can set
in the &GLOBAL section to get rid of that warning.
Does that disable ELPA entirely?
Afaik, yes.
OK, thanks
