Hi!
This issue is somewhat related to #31.
I'm trying to implement a variational circuit that would be optimized to match a target unitary up to a global phase.
I define a loss function to be a fidelity between the two unitaries and trying to tune angles in the ansatz circuit using TF.
I was able to do it for the JAX backend (e.g. basing_hopping(..) optimizer), but failed when trying the same task with TF.
The reason why I'm trying to implement it with TF is that optimization with JAX backend was a bit slow for sufficiently deep circuits (~3-4 qubits ~ 20-30 parametrized U3 gates). So there is a hope that TF optimizer will work faster.
Below is the toy example of tuning of a single U3 gate to match a random SU(2) matrix.
I'm using code from https://quimb.readthedocs.io/en/latest/examples/ex_tensorflow_optimize_pbc_mps.html as an example (the only difference  - I'm optimizing for the unitary, not for the state psi).
It throws an error that is traced back to TF gradient tape.
The error message is:
Any ideas?
Also is there any evidence that TF would potentially work faster then JAX backend?
Thanks!
Hi @yourball,
In general I wouldn't expect better performance from tensorflow than jax, though at some point there was a bug in quimb that didn't compile the backward as well as the forward calculation so it possibly it might be unreasonably slow with your version.
This is fixed, and the optimization generally improved significantly in the tensor_2d branch (which will be a kinda quimb v2.0 release). Here's your snippet updated for that:
There's a few things to note:
The docs will (at some point..) be updated to reflect some of this stuff.
Hi @jcmgray, thanks a lot!
Your snippet worked Ok with tensorflow=2.0.
Important note:
Side note:
Glad you got it working.
with tensorflow v2.1 it seemingly worked (no error messages), however optimization with autodiff_backend='tensorflow' didn't converge to unit fidelity. So I will be using tensorflow v2.0 from now on.
I believe this is indeed the issue with complex gradient through tf.einsum being wrong - there's no error, just bad optimization. From your and my tests it seems like this was broken in v2.1 and v2.2 but OK before and working now (v2.3+).
By the way, if you are interested in eking out performance for larger circuits (and you haven't seen it yet), you should try cotengra for finding the contraction paths (the optimize kwarg to contract).
