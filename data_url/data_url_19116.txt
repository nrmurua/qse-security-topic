Calling optimize_pulses with parallel_map=qutip.parallel.parallel_map, e.g. in the example for the gate optimization (#27) does not result in the expected speedup by a factor of two. Instead, it appears to run much much slower than the non-parallelized version. This could be a bug in qutip, but we'll have to investigate. In any case, we need to make sure there is a way to run the optimization with parallelization, and  the gate optimization seems like a good example where to show this (as well as the ensemble optimization example, #11)
The qutip.parallel.parallel_map routine is working just fine, it's just that the the communication overhead is two orders of magnitude slower than the propagation. This is mostly because in the forward-propagation, we have to synchronize after each propagation time step. When using qutip.parallel.parallel_map for this purpose, all data gets serialized and sent back-and-forth via IPC in every time step. To get around this, I've implemented the entire process and communication management manually, with processes that last through the entire propagation and internally cache as much data as possible, communicating as little as possible. This is extensively documented in the new krotov.parallelization module. The current implementation should be about as good as using MPI in Fortran.
Still, we can't get around sending the propagated states back to the master process, which is relatively expensive. Thus, the parallelization speedup is far from linear. I suspect that the only way to get a true linear speedup is to implement custom parallel_map routines in Cython, where you can release the Global Interpreter Lock and write all data directly, in shared memory (which, in Fortran, would be equivalent to using OpenMP)
I am impressed how quickly you found and implemented this work-around, chapeau!
