The field of quantum computing is more and less split in half when it comes to the choice of qubit ordering convention. These conventions are:
where  $n$ is the number of qubits. In both conventions $q_0$ is considered the least significant qubit, i.e.: the state $|001\rangle$ is interpreted as four by (1) and as one by (2).
In CUDA Quantum, we are currently using (1) both internally and externally (i.e., user facing API where this matters expects users to use convention (1)). However, I propose we internally use (2) to keep consistency with cuQuantum default convention.
Internally the convention will be (2). This will affect:
In user facing API we should allow the user to define which convention will be used.
On the internal aspect at the IR level, qubits are grouped as vectors. The notation is consistent from C++ source code through to the IR. Specifically a vector of qubits, v is indexed such that v[0] refers to the first qubit in the vector. As in the C++ language, this means v[0] is identical to v.front(). Redefining this such that v[0] become v.back() would, I fear, be very confusing to the user.
This extends to any operations in the IR that construct or allow mixing of qubits. Again, consistent with the C++ surface language concatenation of qubits creates a left-to-right mapping of indices. For example, a vector, v, concatenated with another vector, w, as in u = v + w logically implies u[0] is v.front() and u.back() is w.back(). Again, reversing this ordering would be counterintuitive and violate the principle of least surprise.
Hopefully, this RFC does not mean to change these semantics.
