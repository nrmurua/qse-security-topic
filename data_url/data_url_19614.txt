The python code at the end of this message causes PyMatching to crash the python interpreter.
Basically what the code is doing is using (in development features of) stim to generate a surface code circuit, and then reduce the circuit to an error model in terms of possible detection events. I've hard coded the output of that part, so that you should be able to use the public version of stim on pypi.
Anyways, the code takes the circuit and the error model and converts them into a pymatching.Matcher, then samples the circuit's detection events 1000 times and attempts to decode them. Then the process crashes.
I tried to make a smaller repro, but smaller instances were not crashing. It's possible that I'm misusing some of the features (e.g. I translate stim's logical observable concept into pymatching's qubit concept), but of course regardless of any misuse the software shouldn't take down the python interpreter.
Hmm, actually, looking at this again, I can see that it's still using some of the dev features (e.g. reset to X basis). You may have to clone stim's main branch and pip install -e stim_directory into your virtual env to reproduce the failure.
Hi @Strilanc, thanks for flagging this, I hadn't come across this problem before. I'll look into it and fix it soon, but in the meantime I notice that the issue does not occur if num_neighbours is set sufficiently large. For example, using
predicted_observable = m.decode(detectors_only, num_neighbours=40)[0]
appears to work fine. Using exact matching with precomputed shortest paths by setting num_neighbours=None also works fine. Hopefully this helps with your specific problem while I look into fixing the segmentation fault issue.
I also see that you also need multiple connected components, I'll look into supporting that natively (this was also raised in issue #5). What's the reason that your matching graphs are not connected out of interest?
The connected component issue has to do with the graph being automatically generated from the detectors and error mechanisms declared in the circuit. Detectors make nodes, error mechanisms make edges, but in some error models certain detectors are noiseless. So they end up as singleton nodes.
Also there's no rule preventing a circuit from running two separate surface codes.
One thing I don't understand from the paper is what exactly "num neighbors" is for. The graph made up of the detectors as nodes and error mechanisms as edges has degree at most 12, if I recall correctly, so num neighbors 20 should have been more than enough. Are you contracting the detector graph down to a graph between the detectors that actually fired? That's usually more expensive than tweaking the blossom algo to run natively on the detector graph, and forces heuristics that truncate the contracted graph.
Actually now that I think about it more, the issue may be that the perfect stabilizer measurement graph has no timelike errors. So it has one connected component per round.
Even worse, the X and Z stabilizers may form two separate connected components. The only reason I didn't run into that is I didn't remove the Y error at the corner qubits that can connect the two parts.
... I think you should very strongly consider supporting multiple connected components.
Ok sure, very happy to support multiple connected components. Yes the num_neighbours parameter is the same as m in Section 4 of the paper, where I describe the local matching algorithm. I don't tweak the blossom algorithm itself, and do contract the detector graph to what I call a syndrome graph, which only contains nodes of detectors which fired. Rather than constructing a complete graph on the defects (which would be "exact" matching), I include an edge from each defect node to its m nearest defects within the detector graph (this is constructed iteratively through local searches rather than truncating the complete graph). So it is indeed a heuristic that truncates the graph, however the benchmarks in the paper show that it is a very close approximation of exact matching even for small m. Yes I can see why running blossom natively on the detector/matching graph can improve the performance even further, but that's not an optimisation I've looked into.
I don't think that the segmentation fault is a result of more than one connected component (or at least not directly), as I do ensure that it is connected. When I had a look at the bug this morning I saw that it was caused by Lemon appearing to return a self loop in the matching (which broke an assumption I made when indexing an array), but I haven't yet had the chance to work out why this happens.
@oscarhiggott Any updates on this?
@Strilanc I believe it's a bug in Lemon's MaxWeightedPerfectMatching (it matches node 0 to itself on this particular type of matching graph). I can raise it as an exception and prevent the segmentation fault for standard usage, however the Lemon MaxWeightedPerfectMatching.matchingWeight() function itself leads to a segmentation fault when run on the same problem instances. I use matchingWeight() in PyMatching when a user sets return_weight=True (default is False), so preventing the Python interpreter from crashing in that scenario will be trickier. I'd like to better understand what property of the matching graph leads to the crash, and make sure I'm not misusing Lemon, before reporting the bug to Lemon. As mentioned before it only seems to occur for the graphs you constructed for small num_neighbours (e.g. when the matching graph is sparse), and so a current  workaround is to set num_neighbours>40 (the runtime is linear in num_neighbours so this is not too costly). I'll look into this more later in the week, but let me know if you have any suggestions.
I tried to make a simpler repo now that stim v1.4 is out. Unfortunately it seems to have stopped crashing for unknown reasons. So you're stuck with the giant one.
Ok so the problem is definitely caused by Lemon not finding a solution for some specific graphs. I inspected the graphs input to Lemon when it fails more carefully and there is no problem with them (and NetworkX can solve the same matching problems without any issue). However Lemon does flag this (MaxWeightedPerfectMatching.run() returns a bool), so it is easy to catch. I'm not sure why Lemon fails. One way I could handle this when it fails is to retry a couple of times with larger num_neighbours and fall back to NetworkX if the retries also fail. Since it appears to be a rare problem, this shouldn't hurt performance much.
With PR #14, your code no longer crashes the interpreter, and also doesn't throw an exception. If the blossom algorithm ever fails after multiple attempts increasing num_neighbours, it will throw a pymatching._cpp_mwpm.BlossomFailureException rather than crashing the interpreter. Since multiple attempts are made with lemon, I don't see a need to have NetworkX as a fallback (since there is also no guarantee NetworkX will always succeed).
I've reported the problem to the LEMON developers and will update the version used by PyMatching if it's resolved: https://hugo.cs.elte.hu/trac/lemon/ticket/650. Merging the PR now and closing this issue.
Thanks!
Right I've now worked out why lemon did not return a solution. When I inspected the graphs with NetworkX I computed the max weighted max cardinality matching, but double checking now I see that the max cardinality matching for these failed graphs is not a perfect matching. Of course complete graphs on an even number of nodes must have a perfect matching, but when num_neighbours is less than the number of defects for these graphs it only has a perfect matching with high probability. In 80cb686 I now double num_neighbours if the graph does not have a perfect matching, and eventually it is guaranteed to find one once it forms a complete graph. In practice it seems the probability of having a perfect matching is close to 100% once num_neighbours>30, so I doubt it will ever need to retry more than once anyway. I might also change the default num_neighbours to 30. If there are other reasons that lemon can return false then the BlossomFailureException will still be raised. Thanks for flagging this @Strilanc!
