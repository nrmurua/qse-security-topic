The runtime of noise modeling for a given circuit depends on that number of qubits a system has, even though the circuit size is the same.
Consider the following circuit:
When doing noise modeling of the above circuit across multiple system of various sizes, the runtime increases more or less linearly with the number of qubits in the system.  This was measured using job.result().metadata['time_taken'], and averaged over 10 instances of 8192 shots.  This despite the fact that the circuit is the exact same in all the cases (layout was manually fixed to make sure this is the case).

If layout is being specified so that circuit truncation isn't needed I would guess this is likely due to the time taken to deserialize the noise model from JSON in the in the C++ code.
Hmm, I would not expect that to be linear given that things like number of cx is not linear. However, the truncation of a full width circuit down to the subset is something that is likely linear. However, the data would suggest that that truncation is much more time consuming than the noise modeling itself (that should not depend on the system size)
This should be somewhat fixed in 0.9 release which removes the need for truncation of the noise model. Note that there will still be some scaling when the model is converted to kraus or superop form for large numbers of qubits, since the entire model is still converted (but once, rather than truncated and converted for every circuit). This is something that could maybe be further improved upon but would require some structural changes to the C++ noise model.
