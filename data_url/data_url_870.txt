Describe the bug
device.run_batch() fails to validate with S3 when using a named credentials profile to authenticate the boto3 Session. Notably, device.run() works for a single circuit with no change to the authentication procedure. The error specifically is,
There is already a docs page about this error at https://docs.aws.amazon.com/braket/latest/developerguide/braket-troubleshooting-create-fail.html, but the proposed solution is to make sure the S3 resource exists.
To reproduce
When the IAM user has PowerUserAccess attached, the below program fails. Even when the IAM user only has Braket and S3 FullAccess, (EC2 and billing read-only are in the user group as well, but those seem irrelevant here), the IAM policy simulator believes create_task should work correctly (if there's a mistake on my part below, maybe there's a bug in the policy simulator?).
Here's a standalone program that reproduces the bug, assuming profilename is a named profile in ~/.aws/credentials.
Expected behavior
A batch of tasks should run all of the circuits passed to device.run_batch().
System information
Thanks for reporting the issue @eth-n!
Could you please confirm whether the following snippet works correctly for you?
Additionally, another alternative to unblock this use case while we try to fix the issue here, might be updating the AWS_PROFILE to be used. You could execute the command export AWS_PROFILE=profilename to allow the profile profilename to be used by default.
@eth-n were you able to verify the code snippet provided from @kshitijc? Are you blocked on your end? Thank you!
Sorry for the long pause, our focus shifted around a bit, but I'm back to running some experiments on the device.
@kshitijc, your snippet does work.
@christianbmadsen, I was not blocked, I'd just put the tasks in a for loop and pushed the result objects to a list, which worked well enough. I'm not running a very large number of circuits per batch at the moment, but had wanted to have the most streamlined process anyways, especially if I should need to do so in the future.
edit to add: In the workflow, I've mostly found I have to save the task IDs in a code cell as I submit tasks anyways to be able to recall them from S3 and do followup analysis from those result objects. That way if I kill my Jupyter session I can still tell which task is which from the growing task list (double trouble if I want to remember which id is which task from a few months ago...).
Thanks for the feedback @eth-n; it appears that you have been unblocked. If you have any further problems, please feel free to re-open the issue.
