This is a prerequisite for #8
Just a heads-up: I have to prepare for a conference in two weeks, so I won't be able to contribute significantly to the second-order implementation until the last week of January. Until then, maybe @Basilewitsch can keep looking into this, as part of #8?
I implemented second-order Krotov as it is currently implemented in QDYN, i.e., allowing for a static or dynamic calculation of A, although I just implemented the local sigma form for the start. The parameters B and C are actually always zero in the current implementation.
Thanks! Can we calculate the A, B, C inside the sigma function? I'd like to avoid having to pass J_T, A, B, C, and dyn_ABC to optimize_pulses. I understand that in general, the A, B, C are calculated for the next iteration, so we might have to move around where sigma is called (maybe to the beginning of each iteration, where the data from the previous iteration is still available). But in any case, it should be possible to make sigma completely self-contained.
As for initial values, you can do that with a closure:
I see your point but defining all requirements for the entire second-order contribution by just passing sigma to optimize_pulses is not at all (or at least not easily) possible in my opinion. This is due to the fact that we have to distinguish between the function sigma (determining the pulse update contribution) and the function _refresh_A (updating A after each iteration). Thus, sigma has formally nothing to do with updating A. Moreover, the only currently implemented sigma function is the local version, Eq. (13) in JCP 136, 104103 (2012), which also requires the current time t and total time T in addition to A, B, C. Right now, this function is implemented as part of krotov.
Furthermore, sigma is completely independent on dyn_ABC or JT, which are only required in order to specify, if A should be dynamically updated after each iteration and, if so, JT is required to do that. The function _refresh_A that actually recalculates A is right now implemented in krotov as well.
I'm sure we might be able to condense the number of input parameters in a reasonable way but I'm really not sure whether we can condense everything to a single "entity" that contains all second-order information.
The statement that sigma has formally nothing to do with updating A does not make sense! Sigma depends parametrically on A, B and C (possibly B(t), C(t)). This should also be reflected in the data structure!
I'll think about this some more. Worst case, if we really can't calculate the ABC and the resulting value of σ(t) at the same time, we can implement sigma as a class, with a refresh method. But I'll be really surprised if this is necessary. Python is much more flexible than Fortran when it comes to passing around functions, so we should be able to avoid the very long lists of parameters in QDYN that have to cover every conceivable use case.
The sigma function indeed depends on A, B, C but they are just input parameters which do not change as part of evaluating this function. The updates of A are carried out by a separate function, exactly as we do it in QDYN. These updates can be enabled, in which case A gets updated after each iteration (but not as part of evaluating sigma), or it can be disabled if the same A should be used throughout the entire optimization. This is what I meant with sigma having formally nothing to do with updating A.
I'll think about this some more. Worst case, if we really can't calculate the ABC and the resulting value of σ(t) at the same time, we can implement sigma as a class, with a refresh method. But I'll be really surprised if this is necessary. Python is much more flexible than Fortran when it comes to passing around functions, so we should be able to avoid the very long lists of parameters in QDYN that have to cover every conceivable use case.
I completely agree with you that the current implementation can still be improved. I'm just not sure how to do it in the most user-friendly version.
The sigma function indeed depends on A, B, C but they are just input parameters
There's definitely a "starting value", but that's what the closure takes care of.
updates can be enabled, in which case A gets updated after each iteration (but not as part of evaluating sigma), or it can be disabled if the same A should be used throughout the entire optimization.
What's preventing calculating the update to A at the same time as evaluating sigma? The choice whether to update dynamically or not can be made by the user: they can pass a sigma function that updates A, or not.
It may turn that the ABC-refresh and the evaluation of sigma requires significantly incompatible input data (i.e. we'll have to store additional propagated states just to do the calculation in the same place), in which case we'll have to split it up into a "refresh" call and an "evaluate" call.
I would actually not implement the full sigma function (or class, as it may be) in the library, but define it locally in the example notebook: The final choice of sigma is very problem-specific (which of A,B,C to use, what about the ε...?), and should be written out by the user. We can have some of the "building blocks" in a module krotov.second_order, like the commonly used equations for A, B, C (Eq. 25 of Daniel R's paper). These are the "complicated" expressions, sigma itself should then be very easy, with a lot of flexibility.
But again, I'll think about this some more, for a coherent vision.
Defining sigma only in the example notebook looks like a good compromise to me - of course, if you can come up with a simple real solution, feel free to use that instead...
So I looked into the second order again in a bit more detail, and indeed it's not really possible to refresh the parametric dependencies of σ(t), A in our example, at the same time as evaluating σ(t). The reason is that the equation for A requires Δϕ(T), which is only available after the iteration finishes. For the last time step, there is an evaluation at σ(T-dt/2), which then determines the update, which then determines the final-time forward-propagated state, and thus Δϕ(T). So, the recalculation of A has to happen after that, in an extra function call that has nothing to do with evaluation.
Since I'd still like to keep the second-order information as encapsulated as possible, with minimal impact on the interface or the implementation of optimize_pulses, putting the "evaluation" and the "refresh" routines together with the parametric dependencies of σ(t) into a class definitely looks like the way to go. I've defined the following interface in krotov.second_order:
For our concrete example, the appropriate implementation is just directly the equations 30, 31 from the PE paper:
The only thing that is passed to optimize_pulses is then the object sigma(A=0). I've implemented this, and will push shortly.
