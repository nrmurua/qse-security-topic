Let's put the calibration module to the test. The testing should include the following workflow as a minimum.
How does it perform compared to what you can get running single ZNE experiments after some manual tweaking?
Other testing that could be done:
Completion of this ticket will likely be a jupyter notebook in the (private) mitiq-internal repo, along with hopefully some issues opened on GitHub for some potential improvements to the calibration module.
Nate's request: Ideally I'd like someone else on the Mitiq team to take this issue as it will get more eyes on the calibration API, and hopefully then more feedback as well.
I think it would be easier to make progress on this issue if there were concrete deliverable(s) linked to it. Not necessarily a new tutorial or a major overhaul of the existing one. I thought of two options for this issue:
Edit: I'd missed the line above about the notebook on the internal repo, which is also a way to go but maybe a little more cumbersome to track.
Much of what was spelled out in this ticket's description was completed by Nathan in the calibration tutorial, and so I agree that we need something more concrete to strive for moving forward.
I prefer point 1 over 2 as I'm afraid point 2 would be recreating the benchmarking paper. Maybe you have a different set of experiments in mind.
Point 1 sounds good, especially given we have some other big items in the milestone. I will link the PR and we'll track it accordingly. Of course if another contributor wants to take on another aspect of the testing they are welcome to do so!
My apologies that I won't be able complete this in milestone 0.26.0. Happy to pick this up in the next milestone if the next MM agrees.
