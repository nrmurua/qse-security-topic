HQC uses a naive random selection algorithm for creating a random vector of fixed weight which could be improved upon by Fisher-Yates shuffle algorithm. Navie random selection algorithm has a best-case time complexity of Ω(ωr2) ≈ Ω(n) whereas the Fisher–Yates shuffle has the worst-case complexity of O(n)(since random element is selected in O(1)). The downside is that while naive random selection uses O(ωr) ≈ O(√n) memory, Fisher-Yates shuffle uses O(n) memory.
Yes, you are right about the fact fixed-weight vectors are secret.But I don't quite understand how navie selection(which depends on weight) is constant time and Fisher-Yates (depends on n) is not?
Hi @pandasamanvaya, I agree that the upstream HQC code could be improved. You should contact the HQC team directly if that's something you're interested in working on.
However, there are a few issues with your suggestion here.
First, what you've implemented is not Fisher-Yates. See the second paragraph of https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#Implementation_errors. In addition to selecting random_data from the entire [0, N-1] range at each iteration, you are swapping tmp[i] and tmp[random_data] without initializing tmp[random_data] in the event that random_data > i.
The second issue is easy to fix, but the first requires some thought. You have to choose a different rejection threshold for every iteration of the while loop here:

A more fundamental issue is that a naive implementation of Fisher-Yates will leak information through various side channels. This is what cryptojedi means by "not constant-time" (constant- vs variable-time in cryptography refers to the behaviour of the algorithm on inputs of a fixed size, whereas constant-time in complexity theory might refer to the worst-case behaviour of the algorithm as a function of the input size). The issue with Fisher-Yates is that it reveals the permutation that it produces through the sequence of memory accesses that it makes. The existing HQC code takes a small performance hit in favour of executing the same sequence of memory accesses for every possible permutation.
Thanks @jschanck for such a detailed feedback. Totally agree with you that HQC might have got lucky because the weight scales drastically O(√n) with increase in n. So, the naive algorithm succeeds before getting too much collisions(birthday problem).
Here is my understanding on Fisher-Yates(correct me if I am wrong) - Fisher-Yates leaks information while performing the in-place shuffling. Couldn't there be a way to randomize the memory layout(like linked-list implementation instead of array) where one can not figure out the index even after knowing the memory addresses and thus the permutation is hidden? The complexity would be still better O(n2) compared to Ω(n2).
Maybe. HQC tries to do something like that in "fast_convolution_mult" (see: crypto_kem/hqc-128/clean/gf2x.c). But I'm a little skeptical of that approach.
There are other options based on sorting. You can find an example in the AVX2 implementation of McEliece:

I think McEliece also uses a best-case complexity algorithm rather than a worst-case (if I am not wrong). Also I think my previous suggestion might be vulnerable as well because rather than keeping track of memory itself if someone keeps track of no.of memory accesses made then he/she could figure out the permutation. Just curious to know if there is some kind of theorem/conjecture that any worst-case complexity algorithm(especially shuffling) might leak information through side channel attacks? Because it seems like no one uses them.
Any improvement in this section of the HQC algorithms should be discussed with the authors of HQC. I don't think we should start reviewing all the potential spots for optimisation in the implementations that we collect here.
As-is, this change opens up a timing side-channel through memory accesses.
