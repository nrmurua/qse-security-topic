The GPU simulator appears to be using excessive system memory since the 0.8 release. I noticed this running on a HPC cluster and having the jobs killed when they should have been well below the requested memory.
I wrote the following test script and ran it with mprof python test.py
At 30 qubits the maximum needed GPU memory should be ~16 GB.
The reported memory usage on 0.7.6 was very low as one would expect for a GPU simulation:

Running on 0.8.1 with the GPU simulation memory you can see that memory usage is increasing linearly through the computation, maxing out at around ~66GB which is over 4x the memory that would be required for the CPU simulation.

For comparison running this script on the CPU statevector simulator maxes out at the expected 16GB:

The memory usage of 0.8.1 should look approximately the same as 0.7.6 when running a GPU simulation.
There were a lot of changes to the GPU simulator involving the chunk classes in the 0.8 release so my first guess would be there might be a memory leak or bug somewhere there.
