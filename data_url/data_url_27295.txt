I have started with a Stochastic Gradient Descent Optimizer on #24. I think it should be good enough for most use-cases, keeping our intended user-base in mind. @quantshah do you think we should add more optimizers like Adagrad, Adam, etc?
