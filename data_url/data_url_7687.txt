For example, I would like to have
Seems like shared_tags for TNOptimizer/parse_network_to_backend could do the job, but not sure if it can handle the opposite signs.
If it is not trivial, would there be a way to optimize the circuit without the use of the TNOptimizer? I could calculate the loss function with
By trying to use this with jax to do value_and_grad it has some errors in the circuit creating part. Also this is not efficient cause each evaluation will need to rebuild the circuit again. Is there a simple way to update the parameters for the PTensors in the network inplace?
Thanks :)
Notionally you should be able to do something like (if you can identify the gates with tags):
in other words, in the norm_fn (or 'constraint') function, explicitly move the params from one tensor to another, then just specify the first tensor to be optimized in tags.
Note that as yet TNOptimizer doesn't explicitly work on Circuit objects so you would have to construct the lightcone cancelled reduced density matrices and contract them manually.
