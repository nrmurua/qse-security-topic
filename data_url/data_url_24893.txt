When writing a (comparatively large) parameter sweep object to file the size of the .hdf5 file reached above 16 GB before the attempt to write was killed. Direct pickling of the object yielded a file size of ~5 GB.
i'm not necessarily surprised by this - from what I remember pickling uses a pretty efficient binary format. This may be less of a case for the resulting hdf5 files that scqubits generates.
(I'm actually more surprised that you can pickle something above 2GB... at least at some stage in the past that was a maximum - maybe that's changed with latest python releases though?)
I'm not sure there is much to do about that... other that to maybe figure out if there is a more efficient way to store the numpy arrays from what we're using now... (I would think the overhead that comes with the various hdf5 data structures is cheap, space-wise).
Gotcha that makes sense - thanks @petergthatsme !
