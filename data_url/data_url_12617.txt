Comment from @philiptmassey
"We should be calling ParallelFor instead of TransformRangeConcurrently. We can use absl::nullopt or {} as the current estimate of work value, but this would also allow us to make predictions on how much work a circuit takes, which could provide speedups."
Comments from @MichaelBroughton
"When running REALLY deep circuits with lots of symbols through our ops it looks like we may be spending a LOT of time parsing the circuit (sometimes more than actually computing the answer). We should try and make the parse operations that most of ops call right at the very top of their compute functions parallel."
"Maybe we should also make _batch_deserialize_helper in cirq_ops.py parallel as well ?"
This issue has not had any activity in a month. Is it stale ?
Resolved by #283
