Currently readout error mitigation in measure_observables calibrates each observables that it measures. While this is certainly valid, it scales badly -- each observable to be measured needs to be calibrated, so overhead is proportional to the number of observables to be measures (linear in the number of observables).
Instead one should characterize readout imperfections locally, i.e., calibrate how expectation of one body observables are distorted by readout, and extrapolate to many body observables.  In this way the overhead would be independent of the number of observables to be measured, and depend only linearly on the number of qubits involved in the measurement.
This, of course, assumes readout errors are not correlated. But in the presence of two body correlations, one only needs to measure at most a quadratic number of observables. If N body correlations are significant, O(2^N) observables would be needed, but it is reasonable to expect that N is constant and independent of the number of qubits (and almost certainly N=2).
Hi, I'm new here and want to help. (have a light bg in QC w/ Mike+Ike, etc.) I've looked at measure_observables(). Can you help explain this a bit further. What exactly is calibration, and does that happen on line 864? Where does the exponential calibrations happen? Thanks!
These ideas are not explored in Nielse & Chuang, but with some effort it should not be hard for you to follow the argument behind readout error mitigation.
The basic idea is that imperfect measurement introduces biases to the estimate of the expectation value of an observable.  These biases can be estimate independently of the experiment one cares about performing (this is what we mean by calibration in this context -- not be best word to use, but I lacked an alternative). With that information in hand, one can then compensate for the biases and get an estimate of the expectation value of the observable in question with no biases (or much smaller biases).
The issue I am pointing to is that calibration can also be done independently of the observable one wants to measure -- under some assumptions.
Try reading the supplementary material of (this)[https://www.nature.com/articles/nature23879] paper -- it should give you a sense of what readout error mitigation is doing.
