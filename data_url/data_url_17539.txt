From the discussion in #1800, we conclude that we need tests to check properties (in opposition to fixed values). This is a proposal to address that need.
In the past, we used to add randomness to our test suite. For example, here:
This brings the problem of having non-deterministic tests and the retrieval of the falling input when when the test fails. My suggestion is to have a different set of tests (in, for example, test/property_based/) trigged by a different make rule (for example, make property_based_test) using a property-based testing framework, such as hypothesis. This allows to write test in the following way:
This test is more comprehensive than the previous one (test several lengths, for example) and, when fails, triggers the following error:
We could run these tests as not-required in the CI and even submit an issue with the details when failing.
A possible implementation can be found here.
I do not understand why we need this.  We just need to log the random inputs to recover an issue if it arises.  In the above example, the test should vary the length as well.  So the test should be improved, rather than creating a whole new test suite.  Of course, one can, and perhaps should, also add fixed seed tests as well here. I also think that the tests are overly complicated as it stands, and adding an additional package would only make this worse.  For example, NumPy has two required modules for testing (nose and pylint), SciPy has three, and Cirq has two.  We already have five.  Why not have just a single, well-written test suite?
A lot to unpack here. Let me know if I missing any of your points.
This is what I'm trying to address here: a way to recover the input and make sure it does not get lost in void. Alternative solutions are welcomed.
I'm not fully sure if the derivation function is the same in all the platforms/versions. If we can always control other source of determinism, it would be possible, I guess.
We could do it ourselves. But somebody already thought about it. Why not reusing their code?
Because not all the tests have the same goal. Our test-suite tries to make sure that new code is not breaking existing functionality. The kind of test we are discussing here is to find bugs.
log the random input: Instead of using None to seed if not explicitly given, call np.randint(max(int32)), save in logger.
fix seed and log it - The NumPy random is platform independent if the seed is given.  This is why we can fix the seed on all platforms and have the test pass.  The version of the random generator has not changed, and the source itself was last changed in 2005.
do we need a library for doing this - No.
a single, well-written test suite - Why do all the tests have to have the same goal?  Why not have a suite of tests that check for breaking functionality and bugs?
About the last point (which is the one that I feel more strongly about); do we agree with the following premise?
When the tests run as part of the CI in a PR, hitting a broken corner case in a totally unrelated part of the code should not automatically block your PR.
If you agree, I think the best way to implement that is by having them in a different suite. But I'm open to options. Let's close this point before moving to the others...
So I would agree in principle. However, I would say that finding an error is much more important than any inconvenience there might be to merging a PR. If the error is in fact unrelated, it can be logged and an issue created. A subsequent run of the tests should be clean (unless it is a frequent error, which of course could be caught via random testing) and the PR can be merged. So I agree with you. But I view your example as an example of a test suite doing its job.
Closing this as I think we've settled on an initial path forward for property based testing.
