The current gradient framework applies copies and deepcopies in several places. We should revisit these parts of the code and check whether expensive copying is actually necessary or if we could improve the perfomance by not copying.
Some of these have been taken care of, the only occurrences left over are in the parameter shift gradient, where it seems the deepcopy is required and one in the linear combination gradients, where one could remove it but it would require some more thinking and probably a change to ListOp.traverse where one could set indices of the operators to be traversed. Closing this for now, since it doesn't seem to be much of a problem.
