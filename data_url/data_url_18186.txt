The sympy backends were introduced by @liupibm in #217. In the interest of making all simulators behave similarly (and play nicely with the other changes made in #319), the sympy backends were also merged in the SDK. But they lack formal tests.
Include tests in test/python
@ajavadia here is a proposed testing method for the sympy backend (and also other backends):
If we let programmers write assertions for each input qasm, they can only cover a few input qasms. That is, the coverage of the input space is poor.
Instead, we can prepare a large set of qasms, then let three backends (c++, python, sympy) run together and compare their results. If there is discrepancy, some backend must make the mistakes. Then we can focus on that input qasm for debugging.
Existing papers show this method can find more than 1000 bugs in llvm/gcc: http://web.cs.ucdavis.edu/~su/emi-project/
please let me know what you think. Anyway I can manually write a few tests of course.
Comparing among simulators is an effective technique in fuzzing situations. I think the tests should be able to run individually and against a ground truth. They can all have the same set of inputs (if they apply, since the basis gates might vary) and the output should be compared with a known output.llll
@liupibm @1ucian0 can we bring over some of the QASM files that exist in the openqasm repository, and generate ground truths for them? Then we can use them as input to all simulators. The ground truth can be generated using a reliable simulator like the slow python one. Then if anything goes wrong in one of the simulators, we can debug: 1. is the bug in the simulator? 2. is the bug in the ground truth?
I think these QASM files can go in test/python/qasm
sounds good, @ajavadia
@1ucian0  do you want to specify the ground truths which you feel excited about ;)
If not, I can do it.
