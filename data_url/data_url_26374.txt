Is there any reason why we don't test automatically with valgrind anymore? If we accumulate memory issues due to missing testing it will get harder in the future to fix that. Also bugs related to uninitialized variables often don't show up in the testing but then cause segfaults on other systems depending on architecture/compiler.
Maybe we should also have a tester without optimizations (-O0) because access to uninitialized variables may be optimized away.
I totally agree that we should run a Valgrind test. The UZH test stopped working after the libint2 upgrade. My plan is to setup a new Valgrind test at PC2 - hopefully later this year.
Actually, I had a plan to install a Valgrind test on swan too (at least a SSMP version). I've started, but then I went to use valigrind4hpc and found a problem...
Likely I will not test all libraries, we can do that incrementally...
One problem with Valgrind is that it instruments the binary on each startup. Since many of our tests finish very quickly, we actually spend the majority of time instrumenting. Hence, I want to implement a smarter version of the farming mode in the new do_regtest.py.
I found a bunch of false positives, e.g. ai_kinetic.F:173 and atomic_kind_types.F:269 running QS/regtest-gpw-1/Ar-2.inp. They disappear once I switch to -O2. So, apparently GCC got more aggressive with loop unrolling and we need to adopt our arch files.
Turns out the minimal test already runs with -O0 and detected the   problem. However, because the reports were very large the dashboard refused to parse them and only turned gray.
