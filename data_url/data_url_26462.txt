Dear all,
The commit Reorder modules related to MPI (https://github.com/cp2k/cp2k/pull/2647[)](https://github.com/cp2k/cp2k/commit/4db06f03621486fb0a7538132028ae6d12749e28)
seems to be responsible for SEVEREÂ  issues occurring when running on larger numbers of nodes.
For instance a calculation running without problems on 32 nodes of  DAINT-gpu, badly crashes on 64 nodes (see below).
My request is to urgently revert the code to the version prior to these changes and then perform a serious debug of the new implementation, taking into consideration that our regtests are not suitable to explore all the possible issues that might occur on massive parallel jobs.
Thank you for your kind consideration.
Best
Marcella
MPICH2 ERROR [Rank 196] [job id 45170604] [Tue Mar  7 17:27:28 2023] [c3-3c2s7n3] [nid06495] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_PF_INV:CPLTN_DREQ)
MPICH2 ERROR [Rank 192] [job id 45170604] [Tue Mar  7 17:27:28 2023] [c3-3c2s7n3] [nid06495] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_PF_INV:CPLTN_DREQ)
*** Error in /users/mmi/SCRATCH/TMP/cp2k.psmp': free(): invalid size: 0x000000001dcbe710 *** MPICH2 ERROR [Rank 203] [job id 45170604] [Tue Mar  7 17:27:28 2023] [c3-3c2s7n3] [nid06495] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_PF_INV:CPLTN_DREQ) *** Error in /users/mmi/SCRATCH/TMP/cp2k.psmp': munmap_chunk(): invalid pointer: 0x000000001dcae770 ***
Rank 196 [Tue Mar  7 17:27:29 2023] [c3-3c2s7n3] application called MPI_Abort(MPI_COMM_WORLD, 1) - process 196
Rank 203 [Tue Mar  7 17:27:29 2023] [c3-3c2s7n3] application called MPI_Abort(MPI_COMM_WORLD, 1) - process 203
MPICH2 ERROR [Rank 347] [job id 45170604] [Tue Mar  7 17:27:28 2023] [c4-3c1s13n1] [nid06645] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_PF_INV:CPLTN_DREQ)
MPICH2 ERROR [Rank 339] [job id 45170604] [Tue Mar  7 17:27:28 2023] [c4-3c1s13n1] [nid06645] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_PF_INV:CPLTN_DREQ)
MPICH2 ERROR [Rank 342] [job id 45170604] [Tue Mar  7 17:27:28 2023] [c4-3c1s13n1] [nid06645] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_PF_INV:CPLTN_DREQ)
Rank 339 [Tue Mar  7 17:27:29 2023] [c4-3c1s13n1] application called MPI_Abort(MPI_COMM_WORLD, 1) - process 339
MPICH2 ERROR [Rank 337] [job id 45170604] [Tue Mar  7 17:27:49 2023] [c4-3c1s13n1] [nid06645] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_MDD_INV:CPLTN_DREQ)
*** Error in /users/mmi/SCRATCH/TMP/cp2k.psmp': free(): invalid size: 0x000000001c2ee2f0 *** MPICH2 ERROR [Rank 344] [job id 45170604] [Tue Mar  7 17:27:49 2023] [c4-3c1s13n1] [nid06645] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_MDD_INV:CPLTN_DREQ) *** Error in /users/mmi/SCRATCH/TMP/cp2k.psmp': free(): invalid size: 0x000000001ca4ce00 ***
MPICH2 ERROR [Rank 345] [job id 45170604] [Tue Mar  7 17:27:49 2023] [c4-3c1s13n1] [nid06645] - MPIU_ugni_wait_rdma_events(): GNI_CQ_EVENT_TYPE_POST had error (SOURCE_SSID:AT_MDD_INV:CPLTN_DREQ)
*** Error in `/users/mmi/SCRATCH/TMP/cp2k.psmp': corrupted double-linked list: 0x000000001ca9a200 ***
srun: error: nid06728: task 486: Aborted
srun: error: nid06728: tasks 488-489: Segmentation fault (core dumped)
srun: error: nid06728: tasks 482-483: Segmentation fault
srun: error: nid06728: task 485: Aborted
srun: error: nid06728: task 487: Aborted (core dumped)
srun: error: nid06735: task 564: Segmentation fault (core dumped)
srun: error: nid06735: tasks 565,568-570,572-573,575: Terminated
srun: error: nid06635: tasks 216-217,221-222,225-227: Terminated
srun: error: nid06635: task 218: Segmentation fault (core dumped)
srun: error: nid06635: task 219: Aborted
srun: First task exited 30s ago
Can you send me an example input file?
I will revert the PR as soon as possible.
The QS benchmark case H2O-512.inp crashes indeed quickly using 64 nodes (768 cores) on daint-gpu whereas it is running fine on 12 nodes (144 cores).
Short update: The bug was definitely introduced somewhen earlier but the branch support/2023.1 works for me with the given test. So, stick to that one for now. I will continue the bisection.
I found the failing commit and I am currently testing a bugfix (apparently a dangling pointer).
I have added a 512 H2O performance test using 64 and 16 nodes on daint-gpu and daint-mc, respectively. Both test runs are currently crashing and should detect such issues in the future.
