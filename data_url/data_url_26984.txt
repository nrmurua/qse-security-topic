Dear QUICK Team,
I did a Windows port with CUDA support (see https://github.com/MSoegtropIMC/QUICK/tree/windows-cmake). As far as I understand this dows not work with MinGW because according to nVidia nvcc supports only Microsoft Visual C as C compiler with CUDA on Windows - see Cuda Windows. So in short I did the following:
The good news: I can compile all 4 variants (with and without MPI/Cuda) and they run (tstes only the base and the CUDA variant so far). I can reproduce the test results in a few of teh advanced cases (did not test all as yet) for energy, gradient and optimization runs with very minor deviations.
But I also see a few odd effects:
I cannot reproduce the gradients from the hands on tutorial section "HF/DFT gradient calculation) Tutorial. What I get is just completely different. Is it possible that the data in the tutorial is wrong? I find it rather odd that more complicated test cases work and this simple case doesn't. I attached my input and output file.
Gradient.zip
I did run an optimization run for C60 with B3LYP DFT and Dunning cc-pVDZ basis and there is really odd stuff going on in the initial SCF convergence run. After 40 iterations which look reasonable, the energy suddenly drops from -2246 to -4865 and then keeps dropping in large steps from time to time, e.g. from -4639 to -10443 at iteration 48. As far as I can tell an energy around -2250...-2260 is correct. I attached the input and output file of the run. It would be great if you could check if you see the same effect or not. This is a CUDA run (I didn't try the normal version).
C60.zip
One last thing: do you in general appreciate this activity, even if it means to do some minor compatibiliyt changes to the code, or are you already more than busy enough with Linux? I would say the port was quite smooth after I found a combination of compilers which works. Using a Fortran module to interface C - see quick_c_interface.f90 - is I think in general a good idea because it is less error prone and the other changes are quite minor, although the VLA->alloca change is a bit ugly.
Hi @MSoegtropIMC, this is awesome. Unfortunately, I dont have a windows machine at my disposal right now. So I wasn't able to compile your branch and test. But I am sure we can figure out somewhere down the line.
I cannot reproduce the gradients from the hands on tutorial section "HF/DFT gradient calculation) Tutorial. What I get is just completely different. Is it possible that the data in the tutorial is wrong? I find it rather odd that more complicated test cases work and this simple case doesn't. I attached my input and output file.
This is a good catch. The gradients we have in the documentation is from a B3LYP calculation of the same molecule. I will update the documentation later today.
I did run an optimization run for C60 with B3LYP DFT and Dunning cc-pVDZ basis and there is really odd stuff going on in the initial SCF convergence run. After 40 iterations which look reasonable, the energy suddenly drops from -2246 to -4865 and then keeps dropping in large steps from time to time, e.g. from -4639 to -10443 at iteration 48. As far as I can tell an energy around -2250...-2260 is correct. I attached the input and output file of the run. It would be great if you could check if you see the same effect or not. This is a CUDA run (I didn't try the normal version).
Hm.. I think the problem is coming from the ERI engine, specifically from cuda FmT function: https://github.com/merzlab/QUICK/blob/master/src/cuda/gpu_get2e_subs.h#L1735-L1806 or the integer atomic add (for eg: https://github.com/merzlab/QUICK/blob/master/src/cuda/gpu_get2e_subs.h#L1527-L1530). The best way to localize this problem is comparing serial vs cuda ERI energies. I will get back to you on this.
One last thing: do you in general appreciate this activity, even if it means to do some minor compatibiliyt changes to the code, or are you already more than busy enough with Linux?
Of course. This is something we have on our to do list; but unfortunately we have been busy with other features. Since QUICK is now a tool of AMBER, I am pretty sure there will be many users out there who will be benefitted from QUICK/AMBER QM/MM on windows platform. @agoetz Is there someone who can help us? Please add your thoughts.
Hi Madushanka,
fine, then I will put some more effort into my branch so that it can be merged (e.g. test if it still compiles on Linux at least without CUDA and do some cleanup here and there + add instructions).
Regarding the Windows build: in case someone wants to reproduce it, please note that there is a batch file build_VS2019.bat which calls everything - one just would have to adjust paths to the Intel and Ninja installation. The other two batch files don't work (one uses Intel C which is not compatible with nVidia nvcc and one tries Microsoft SLN backend instead of Ninja, which doesn't work either). But I think for the time being we can concentrate on ensuring that my changes didn't mess up the Linux build (which I can't test easily, at least not the CUDA version).
Regarding the C60 issue: I din't fully understand if you want to say:
a) you can reproduce this behavior with the Linux Cuda build and you are looking into it
b) it is an issue specific to my modifications and I should look into it
fine, then I will put some more effort into my branch so that it can be merged (e.g. test if it still compiles on Linux at least without CUDA and do some cleanup here and there + add instructions).
Great! Thanks @MSoegtropIMC.
Regarding the Windows build: in case someone wants to reproduce it, please note that there is a batch file build_VS2019.bat which calls everything - one just would have to adjust paths to the Intel and Ninja installation. The other two batch files don't work (one uses Intel C which is not compatible with nVidia nvcc and one tries Microsoft SLN backend instead of Ninja, which doesn't work either). But I think for the time being we can concentrate on ensuring that my changes didn't mess up the Linux build (which I can't test easily, at least not the CUDA version).
@lizhen62017, @linfranksong Can you guys give this a try?
Regarding the C60 issue: I din't fully understand if you want to say:
a) you can reproduce this behavior with the Linux Cuda build and you are looking into it
b) it is an issue specific to my modifications and I should look into it
I will check if the problem exists in Linux Cuda build and get back to you.
-Madu
@MSoegtropIMC I ran your test case with a Linux cuda build on a V100. See my input and output here:
c60_v100.zip. I dont see any SCF convergence issue. Whats interesting in your output file is that per step operator time is less than on V100. Do you know if your GPU was running some other expensive process while this calculation was going on? Can you rerun this example and confirm?
