When running test_qobj_measure_opt in ci we ocassionally have random failures like:
https://travis-ci.org/Qiskit/qiskit-terra/jobs/451415409#L1748
It looks like there is more variance in the result value than we're expecting and we're occasionally getting results outside our expected range. (In the specific example above we were expecting 500 +/- 80 and got 587)
It happens randomly based on the nature of the test. You could run things in a loop like with stest run --until-failure test_qobj_measure_opt but there is no 100% reproduce (which just runs the test in a loop until it fails).
The tests is deterministic in behavior and always passes
Increase the acceptable range of valid results?
In general there should be a way to retrieve the random seed of a failing test, to make it easily reproducible.
Oh I fully agree, but it doesn't look like we have that in the current ci results though. (I don't see the python hash seed anywhere in the log file) Probably just another reason we should switch to running things under tox in the ci jobs because it always prints that at the start of a test run,
This test is running the qobj in test/python/qobj/cpp_measure_opt.json. I don't understand why we get non-deterministic behavior, as the seed in that qobj has been set. Maybe this points to another bug where the seed is not correctly used?
As a general philosophy, when testing with simulation, we should fix a seed. We should not merely increase the tolerance threshold when things go wrong, because we can easily get deterministic behavior by just fixing the seed.
With the device tests, however, often it is fine to just increase the tolerance threshold a bit.
The seed of package random is different from the seed of numpy. So if you fix one then the other is still not deterministic. Perhaps this explains what you witness. The same goes also if you have in addition a C++ seed.
I am going to close this as we the updates to the simulators i believe this is fixed. Also these test will go into aer when aer comes out.
