Recently, we have seen a number of issues with the limitations imposed by using int32 for the sparse matrix indices in QuTiP (#845, #842, #828, etc).  Supporting both int32 and int64 indices is quite a bit of work, and we really do not have the resources to make such a big change.  Instead, might it not be better to switch everything over to int64?  This would result in a ~20% increase in the size of the sparse matrices, but would allow for handling the really large problems that people are looking into these days.  @ajgpitch, @Ericgig, @sahmed95, what do you think?
There have been quite a few people raise these issues, as you say.
Seems like we have to go int64
As I mentioned in #853, I wonder if it's possible to completely encapsulate any use of sparse matrices to allow for different implementations, assuming a common interface. This would support both int32 and int64, as well as any other specialized implementation anyone might want to use. Obviously, this falls into the "quite a bit of work" category, assuming it's possible at all.
Personally, I also want to clarify that in #853, I wasn't thinking clearly when I titled it "moderately big tensor products" (obviously, it was an insanely large product). I don't actually foresee needing anything bigger than int32.
Is there a CPU-time penalty for switching to int64? If so, I would consider that more of a downside than an increase in memory usage.
Thank you for creating this thread and encouraging to see that there are more people raising the same issues. I saw it one week ago and came to the conclusion that my research group would be much helped if QuTiP used int64 for the sparce matrix indices.
I have the following questions:
It depends on the size of the row and column indices.  If they cant fit in int32, the matrix must use int64 indices.
As with most things, someone just has to find the time .
@goerz we have been discussing encapsulating the matrix type to store qobj.data for some time, see #437
It definitely falls into the quite-a-bit-of-work category.
This is a legitimate concern though. I guess we should run some tests comparing times for indexes across arrays with int32 and int64 before we launch into anything.
Do we know whether scipy are already using int64 for the indexes of their sparse matrices?
Yeah, in the context of #850, I was only thinking of a global switch for the storage format. But #437 makes a good point about using full storage. I'd add to that the Lapack banded storage format which is extremely efficient for diagonal or tri-diagonal operators (or scipy's DIA, which I think is mostly equivalent). I've actually had to solve the equivalent problem of allowing multiple internal formats in the Fortran QDYN package, so I'm not sure why I didn't think of that in the first place ;-). I can definitely vouch for the efficiency gained by being able to switch internal sparse representations.
So yeah, you'd definitely have the most flexible solution by allowing Qobj.data to use varying storage classes from object to object. Maybe the "protocol" could be for Qobj.data to be any subclass of the scipy spmatrix base class? Then the Qobj constructor would probably need an additional parameter format for the class that should be used to convert the inpt argument to the data attribute. The onus would then be on Scipy to provide a full matrix that is a subtype of scipy.spmatrix (just to keep the interface contract), and also to have int64 versions of all their existing sparse classes. From a community standpoint, it would seem like one would get much more bang for the buck to have this problem solved inside SciPy, instead of doing a lot of low-level stuff in QuTiP -- at least if they're amenable to pull requests, but worst case you can still define necessary new scipy.spmatrix subclasses externally.
Take all of my musings with a little grain of salt... I never really looked too deep into QuTiP's low-level internals, specifically where QuTiP is using Cython. Naively, I would sort of think that Cython would remain at the level of sparse linear algebra operations (and thus mostly in scipy) and that QuTiP could be mainly pure Python. Obviously, once you start implementing higher level things like time propagation or even optimal control in Cython, everything becomes a bit of a mess, because then you really do have to manually implement everything for every possible storage format (again, something I'm familiar with from Fortran, and it's a lot of work). Generally, though, I've personally come to the conclusion that the lowest-level thing to really optimize for quantum dynamics simulations (in any language) is the application of a (time-dependent) operator to a Hilbert space state, respectively the commutator with a density matrix (in QuTiP, the operators would be the nested [H0, (u(t), H1), ...] lists). Everything higher level is probably fine at Python speed, as long as the algorithms are sound and you avoid allocating temporary storage as much as possible.
So, while I agree that having a flexible storage format is a good way to go, the truth of the matter is that no one has time to implement such a big change.  SciPy supports both 32 and 64-bit ints, and checks to see which ones are needed at runtime. e.g. a tensor product of two sparse matrices with int32, may need int64 in the result since their shape is larger.  We could also do a similar thing, but it is a lot of work.
The reason why we no longer use standard SciPy sparse objects is: 1) They tend to have a sizeable overhead when being created and/or manipulated do to redundant safety checks. 2)  Many of the sparse operations are not well optimized.  Because SciPy supports many different sparse formats,  they had to generalize many of their core operations, making them slower.  For example, the sparse kronecker product first converts to COO format, does the tensoring, and then converts back to the original format.  Our method is much faster.
Supporting more than just the current CSR format is also problematic.  As already mentioned,  there would be type checks, conditionals, and format conversions everywhere.  e.g. what happens when DIA * CSR.
As usual, available time is the limiting factor here.  Support for both int32 and int64 is not that hard, but would take some time.  Just int64 would make smaller matrices larger, but would allow for much greater system dimensions.  In this later case, it would be a simple switch int -> int64 and size_t -> uint64 in the Cython code, and the fastsparse module (plus probably other places.
I understand completely!
I'd probably just go to int64 with your internal CSR implementation for the moment. Would it be very easy to have the int32/int64 boiled down to a single compile-time flag, for those people that compile QuTiP manually and feel they really need the smaller int32 matrices? The pre-compiled pip/conda releases could be int64. That might be a simple way to provide some support for "both".
Somehow, I thought that SciPy would put a lot more emphasis on performance, and do things like Kronecker products without conversion. It would seem that SciPy should be the place to really optimize the sparse linear algebra implementation, including all the possible combinations like CSR * DIA. Then everyone could build on top of that, making custom sparse-matrix implementation unnecessary.
Maybe when someone gets some serious funding for working on QuTiP, encapsulating all of this better would be doable. So, just something to keep in mind before bolting on too many things onto the existing custom implementation (like multiple sparse and full storage formats). Once you open up that can of worms, it just might be better to re-design Oobj.data to have a very broad base (e.g. SciPy), and keep the custom CSR as an optional, highly efficient implementation for the special case, maybe in a separate package. Either way, it's going to be a lot of work, and not something for a minor-version release.
Michael's compile time flag seems like a good idea. Bit more work than a straight find/replace, but I think it is possible with Cython
Any news on this? I'm also experiencing overflow issues, especially for implementing shallow quantum circuits with a large number of qubits the int64 sparse arrays could be extremely useful.
@nonhermitian, @goerz: The lack of optimization in some scipy.sparse methods is indeed due to having a lot of formats to support and limited developer time.
That said, we're always happy to accept faster implementations! The optimization for multiplying DIA * some sparse matrix would be especially nice.
I have a branch with int64 matrice that pass all tests except the mkl ones which are skipped.
https://github.com/Ericgig/qutip/tree/long
int64 indices are now achievable in dev.major with a 3-line change to core/data/base.pxd and base.pyx.  I'll leave this (and all associated issues) open for now as a reminder that we may want to put in a compile-time switch for that, but in principle QuTiP 5.0 will allow fairly simple swapping for those willing to recompile.
I'm facing the same issue, can you elaborate a little bit on the solution? Thanks
I couldn't find @Ericgig's branch (maybe he tidied it up) so I made #1727. It compiles for me, but I didn't really try it out yet. Feedback appreciated.
Eric's branch was from before the data layer, so more like the current master.  There it's basically impossible to have a command-line switch, because the assumption that everything is done in 32-bit integers is hardcoded everywhere in the code base.
#1874, which was an alternative implementation to #1727 has been merged in dev.major.
If someone here could try out the new feature (available in the dev.major branch) that would be greatly appreciated.
