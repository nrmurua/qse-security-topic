Even when max_parallel_experiments flag is passed, the StatevectorSimulator only uses one core when asked to execute a large number of circuits, as can be easily verified using htop.
The following example computes the statevectors of 10,000 10-qubit states composed of Haar-random single qubit states.
Same behavior occurs when using the StatevectorSimulator backend directly:
Multiple cores should be used as specified by max_parallel_experiments flag.
N/A
Curiously, on a different machine (Intel Xeon E5-2695v4) the simulator uses all cores irregardless of the values of max_parallel_experiments or max_parallel_threads. A slightly more complicated example that uses deeper circuits  crushes with segmentation fault despite only using ~20% of the available 126 GB of memory at peak (see #1289). Repeating the example that uses all 36 cores and segfaults despite setting max_parallel_experiments=2, max_parallel_threads=2:
StatevectorSimulator does not ignore max_parallel_experiments. I believe that your code is struggling before simulation.
Multiple circuits are transferred from python to C++, and then run simulation. If the number of circuits is large, transpilation (python), assembly to qobj (python), and deserialization of qobj(C++) become the bottleneck before simulation.
I'm not sure that 10000 circuits are realistic number as a request. However, I believe that #1266 will help in the near future.
@hhorii just to confirm, you are correct, the flag is not ignored in the original example in macOS. The first code gives the following performance (all time in seconds):
and
On Linux though, the simulator does appear to use all cores irregardless of the passed flags, but that is a problem for another issue.
