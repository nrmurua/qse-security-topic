Is there any sort of UART/USART driver in ARTIQ? I've looked through documentation and coredevices, but can't see anything similar. Currently trying to hack SPI to work as a UART, but it's not ideal (getting the delays right so no timing delay between writes is an issue, and the SPI buffer is too small to output the data words that I want at once). Writing something like coredevice/shiftreg.py could be an option, but I get consistent RTIO Underflow errors when trying to send data at ~ 5 MHz (on kc705).
Related question: What is going on in the coredevice that it can't queue a FIFO buffer with ~150 GPIO/TTL toggles 20 seconds in the future (tested on ARTIQ 2, raises RTIOUnderflow)? Something like:
My best understanding is that ARTIQ should compute the GPIO timeline during the 20 second delay, and then output it when the 20 seconds have elapsed. Is this correct? Would this work better on ARTIQ4 where the for-loop could theoretically be per-computed and replayed via DMA?
Regarding your related question, the output event FIFOs only have a certain depth, and when they are full, CPU execution blocks until there is space available again. In your example, the CPU would block for ~20 s in the first on/off call not to fit in the buffer, and then proceed to push events once they are drained, at which point it soon can't generate them quick enough to keep up. This would indeed work better with DMA, where the steady-state event throughput is higher.
The gateware could start filling another lane but that would reduce the number of allowed event reorderings. IMO it would be ok to use up to half the number of lanes for this purpose.
If you really need an RTIO uart, you can write one. There is a generic uart in misoc that could be adapted.
@jordens if this was being done in ARTIQ 2, then there are no lanes -- this would just be something about the FIFO depth for this channel on the particular gateware being used, which IIRC is 128 events for KC705.  If so, this would explain the underflow, since @drewrisinger is trying to put 140 events into the queue.  Thus the CPU blocks until you push out the first event, at which point you have 64*200ns ~ 12 us to program the remaining 12 events into the queue.  Since the delays are in nanoseconds, there is floating point arithmetic being done for each event and this will clearly give underflows.
@drewrisinger this sort of thing can be pretty easily debugged if you use the core device analyzer tool; it will tell you about the slack at all time points, and in this instance you ought to see the slack go from ~20 s to ~12 us on either side of the delay because you have filled the FIFO and are blocking the CPU as a result.
You can test this by making your loop do only 64 (or fewer) iterations instead of 70, and see if you still get underflows.
Yes. My suggestion applies to sed. Buy it's worth considering there.
