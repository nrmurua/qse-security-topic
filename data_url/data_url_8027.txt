I have a simple task of maximizing $\text{Tr}(O \rho(T))$ using GRAPE where $O$ is a given fixed Hermitian matrix and $\rho(T)$ is a time-evolved quantum state under given Hamiltonian and time interval $[0,T]$. You can also think about this as maximizing trace overlap between two quantum states, if we set $O$ to be another density matrix. I've used all three different fidelitycomputers, but apparently all of them failed to optimize the cost function properly. I'd appreciate it much if anyone can help me understanding what I've been doing wrong! I attached a simple example code below with explanations:



I'd be great if someone points out what I'm missing here. One of my guesses of why this is happening is perhaps because we evolve a ket statevector while the target observable is in full matrix form. If so though, I'm not sure how to cure this problem.
Thank you in advance!
I ended up figuring out the problem by calculating analytic gradients of $\text{Tr}(O\rho(T))$ and made a sub-class under the fidelity computer superclass.
@Leeseok-628 Nice!
