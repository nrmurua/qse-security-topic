The following circuit:
when optimized with level 2, returns:
i.e. it does not perform an obvious CX cancellation. The reason is that it internally repeats the optimization loop until circuit depth stays constant. However it should care about circuit size too.
To reproduce the circuit:
Hey @ajavadia, I was a bit new to Transpiler optimization but would love to work on this issue. Is there any good starting point to learn how CX cancellation pass works , before diving into the code?
@kdk, would specific benchmarking be needed before this change can be added to the transpiler? I was thinking that searching over an additional optimization for the size of the circuit would have some kind of time overhead.
Additional benchmarking is good - we already have it running at https://qiskit.github.io/qiskit/, so you can see things there.  It doesn't hurt to run asv locally - the benchmarks are in the Qiskit/qiskit repo, and you can use asv dev to test your commit against the main branch.  You'll likely want to read up on its docs so you can limit the benchmarking to only the suites you care about; the whole suite takes quite a long time to run.
Some performance regression is acceptable here, provided that the "track" benchmarks improve; it's ok if we're taking a little bit more time, but getting better results (within reason - that's why we have different levels as well).
Hey @jakelishman, so I ran the track_depth benchmarks on my device. Although the benchmarks weren't very different from the main branch, these 3 entries particularly grabbed my attention -
The final result that I got after running the benchmarks for the track_depth suite was that BENCHMARKS NOT SIGNIFICANTLY CHANGED. Would this require further testing?
We recently fixed the transpiler seeds in all benchmarks, because before there was some element of randomness to several of them, but I don't think that affected any of these.  I'm not sure where your numbers come from entirely, because your "original branch" numbers don't seem to line up with anything in our benchmark history, and when I run the tests, I get no changes in depth at all between main and your PR, which is expected, because you there shouldn't be any effect on the depth, only the size.
The benchmarking results we'd need to look at first are the ones that are marked time_ for the same sorts of transpiler benchmarking, and given that we care about depth as well as size, we might want to add duplicates of track_depth to track_size as well (I don't think asv allows us to track more than one thing off the same benchmark run, but that's worth trying as well). @kdk and @mtreinish may have more to offer about adding benchmarks, though.
The benchmarking results we'd need to look at first are the ones that are marked time_ for the same sorts of transpiler benchmarking, and given that we care about depth as well as size, we might want to add duplicates of track_depth to track_size as well
This is spot on. I wouldn't expect the timing to be too much at risk here, the overhead of a size check should be small, and the time-cost of extra runs through the optimization loop should be offset by the improved circuit output, but I suppose we can see what comes back from the existing benchmarks after this merges.
(I don't think asv allows us to track more than one thing off the same benchmark run, but that's worth trying as well).
There's an open feature request for this ( airspeed-velocity/asv#543 ) but our approach to date has been to duplicate the benchmark for each metric we'd like to track.
