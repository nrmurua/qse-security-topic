Ensure all code is designed such that it can not only be configured at compile time to deal with different CPU feature sets (e.g., presence/absence of AVX2) but also handle those gracefully at runtime, e.g, by automatically falling back to a non-optimised version of the same functionality.
In other words, all code compiled with, e.g., -mavx2 should also run without error on CPUs that do not have support for all features (AVX2 in this example).
To help with this, we will integrate the CPU features library and for starters will add warnings if the library is running on a CPU with a feature set different from the one encountered during build (reducing the surprise when one encounters an illegal instruction exception during execution until this issue is fully addressed :-)
Edited by @dstebila to add:
Do we consider this done? @xvzcf @baentsch
Not yet, but almost there.
After yesterday's discussion on checking all algorithms to use common code, and evaluating the (runtime) cost to suitably "XKCP-guard" all common code entry points, I'd need to ask (surely @xvzcf , everyone else invited to chime in, too) for revisiting the suggested use of XKCP for selecting the properly optimized common code: The runtime feature-availability check for each "common code" invocation, e.g. of SHA3, would add a noticeable, quite probably prohibitive, performance penalty to each algorithm using such shared code as typically, such invocations occur very often during each algorithm execution. This check also is not necessary as such algorithms already did the runtime feature check at invocation: Wouldn't it be a correct assessment that those algorithms need common code (symbols) that are (compiled) as optimized as the baseline algorithms as well as common code (symbols) that are/have been compiled not using specific CPU features and call/link to either variant based on the actual CPU features encountered when executing? In essence, #724 looks like a better solution for the problem from a performance perspective than XKCP, even though it requires introducing additional symbols and code (say, "_avx2") for the same functionality (but different compile-time options).
We could rely on global variables that are set with the correct CPU-dependent wirings on the first call to any CPU-dependent function, and then not have to generate that data structure again on future calls.  I acknowledge that global variables are bad(TM), but I think this would be an okay usage?  I don't think it's possible that different threads within the same process would be running on different CPU architectures.  And it should be possible to implement this in a way that is thread-safe.
We already do something similar to this caching here but this still necessitates function calls & checks, i.e., indirections in the "hot" code path. We'll surely do our best to minimize the performance degradation, but it will be there; so the question is: Are we all OK taking this hit for the "cleanliness" of this solution?
Won't know until we try it and see.  After first run, the overhead should just be an if and some function redirection.
Some crypto libraries require a global "initalize_library" function be called before any use, so if we were willing to demand that then we could reduce the overhead even more.
As agreed in last week's discussion with @xvzcf and @dstebila, I reviewed whether it'd be straightforward to do runtime CPU feature "guarding" as done for the PQClean algorithms for the remaining algorithms listed above. The answer is: "Not really": In some cases I do not see a single place where such switch could easily be introduced (picnic: various places exist that are #ifdef'd to CPU features; BIKE: It seems the method there is to simply use different functions with the same symbol for optimized/"hot" functions (activating the right one for the given feature set). In both cases, I'd consider it hard-to-impossible to create a variant containing both types of code, optimized and ref, to enable the feature this issue calls for). In some cases performance improvement by CPU-feature activation doesn't really yield significant improvements (SIKE, SIDH, Frodo), further making this effort pretty much futile.
In sum, I'd argue we leave things as they are right now, communicate to the teams whose algorithms could benefit from such "portability enhancement" a link to this issue as well as a link to the Wiki article asking for their interest to provide this feature also for their algorithms and either close or put this issue into "future work" state.
Would suggest to check off SHA3 when #957 lands. OK, @jschanck?
All that remains here is SIKE and Picnic. @christianpaquin Is this something you want to pursue or shall we close?
If we close this without moving SIKE and Picnic to the same level, I'd suggest to explicitly document that limitation in the (runtime-) support of those algorithms: They'll always run slower than they could when OQS_DIST_BUILD is set (e.g., in profiling and docker images). We could add a sentence here: Something like
Note that Picnic and SIKE do not support this flag and thus will always run with non-optimized code if OQS_DIST_BUILD is set.
Picnic now performs runtime selection if OQS is built with SSE2/AVX2/NEON support.
Closing issue as the only algorithm not supporting this feature has been removed.
