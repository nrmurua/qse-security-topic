While qibojit is designed to save memory footprint by performing in-place updates for the circuit statevector, this set up is incompatible with QML tasks where the cost function is obtained by evaluating the variational circuit over a set of different initial states, i.e. when each training data sample is loaded directly as an initial statevector, as opposed to parameters of an encoding circuit. This is the case when dealing with quantum data, such as in the qcnn class that we introduced. Classical datasets can be loaded directly as statevectors as well, see for instance the variational quantum classifier script https://github.com/qiboteam/qibo/tree/master/examples/variational_classifier.
To illustrate/reproduce this issue, please refer to the simplified version of our benchmark script is as follows:
Please copy the data files 'nqubits_4_data_shuffled_no0.npy' and 'nqubits_4_labels_shuffled_no0.npy' from https://github.com/qiboteam/qibo/tree/master/examples/qcnn_classifier to the same folder as the script with the above code. The training would run properly when backend is set to numpy, but fails when it is set to qibojit.
The issue is that if a similar routine is required in any algorithm, the user would have to be aware of this backend dependency and add additional steps to duplicate the state specifically for the case of qibojit backend. We could implement this backend check in the qcnn class, but Stefano raised the possibility of introducing a flag to intentionally not use in-place updates when setting qibojit backend for applications that fail because of it.
@jf-kong @ihpcdingwj
Dear @jykhoo1987, I tried to run the script above with both qibojit and numpy backends and it never fails, returning similar results.
Can you please add some information about the error you get?
As you said, qibojit performs in-place update of the state vector during the circuit execution, in order to save memory.
In QML, I guess you need to execute many times the same circuit $\mathcal{C}$ but setting every time a different initial state $|\psi(x_i) \rangle$, in which you encode the external information $x_i$. I have the feeling this is totally compatible with the in-place updating of qibojit, since what you need in the QML procedure is the final state $\mathcal{C}|\psi(x_i)_{\rm} \rangle$. Once the final state associated to $x_i$ contribute to the calculation, you start with a new data (in other words, you should need the initial state only at the beginning of the execution).
Please, correct me if I misunderstood.
Anyway, it is of course possible to implement the possibility to set a flag variable which turn off the in-place updating in qibojit, but at that point we should check if qibojit is still needed or if the performances become similar to the numpy's one.
Hi @MatteoRobbiati thanks for the reply. Basically the issue is that the in place update overwrites the statevector (i.e. the datapoint), so after 1 round of loading all the training data, the training data itself gets altered. Therefore if the routine requires loading many times the same training data, it fails with in-place update.
Hi @MatteoRobbiati thanks for the reply. Basically the issue is that the in place update overwrites the statevector (i.e. the datapoint), so after 1 round of loading all the training data, the training data itself gets altered. Therefore if the routine requires loading many times the same training data, it fails with in-place update.
Okay, now I understand. And is the data pre-processing very computationally expensive? Because a solution can be to re-computing the data encoding every time a $x_i$ is sampled. It can be a good idea to compare the two approaches in terms of speed.
If you think it is better to prepare all the initial quantum states once and then  proceed with the training, for this is for sure necessary to deactivate the in-place updating.
There is no pre-processing. $x_i$ is exactly the statevector used as the input, i.e. $\braket{i|\psi} = x_i$, where $i$ indexes the computational basis. Adapting your suggestion, one way to overcome this is to basically re-load the datapoint from file everytime it is used once the final statevector information is no longer required and can be discarded. Reading from file however is likely going to incur more cost in time as opposed to making a copy of every training data.
On a side note, the VQC tutorial https://github.com/qiboteam/qibo/tree/master/examples/variational_classifier also has this issue so it is not specific to the QCNN model. There is a bug in the code now that basically introduces a datatype mismatch (it loads data from iris dataset as float64) with the default double precision of qibojit (complex128) and so it appears to work with qibojit. It is actually because of the incorrect datatype that forces qibojit to make a copy of the input state instead of doing the in-place update that makes it appear as though that VQC example/tutorial is running properly with qibojit.
There are two options for overcoming this: copy the input, implement not-in-place Qibojit.
Contrary to what I said in the meeting, the first option should be implemented on the user side.
I actually believe it should be real user, not even Qibo, since it should happen together with the encoding of the data in the initial state (and where data are encoded is currently fully up to the user). The moment we provide tools for automated data encoding, then in-place backend detection and potential initial state copy could happen there.
It would be useful to make the backends advertise themselves as in-place or not-in-place.
About the not-in-place Qibojit, it would be interesting to disentangle the speed-up in production. Though, a simpler benchmark can already give a large hint about the origin of the speed-up (either coming from in-place operations, or faster operations anyhow).
I would suggest decoupling this second task from the current issue, and open a dedicated issue in Qibojit.
We decide to document how qibojit works and fix example (VQC), cc @MatteoRobbiati.
