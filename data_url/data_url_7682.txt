I recently upgraded to pytorch version 1.8.1+cu102, and now I am getting an error when I run the code that you have helped me with before:
I am not sure why, but it looks as though the optimizer is apply pytorch with GPU settings even though the device is set as CPU.
Would it be possible to post a minimal example that demonstrates the problem?
In the meantime you could try one of ways torch allows to set the default tensor device.
I tried using a modified version of one of the quimb examples:
and from that I got
Does this mean that if you try to run pytorch using the GPU once, it sets that device choice as an environmental variable? If so, then I'm afraid I'm not clear on how to switch back to CPU.
Yeah the problem seems to be that call torch.tensor on e.g. a list no longer propagates the device and requires_grad attributes of the scalars:
need to look into how to get around this.
Here's a workaround for now:
if you call this first, then it should work.
Thank you very much for getting back to me about that. I tried that code in the toy example and it worked, but in my main program, I got the following error:
Again, its much harder to help with only the error traceback, but I suspect you might need to call:
rather than try and call numpy on torch arrays.
Sorry about that. I added the two lines that you suggested, but I got a different error:
My optimization function is defined as
while the loss function is defined as
and the expectation value calculation function is defined as
The key thing is that all numeric/array operations need to be dispatched to the correct backend library (in this case torch), so that the computation can be traced and auto-diffed. That means using autoray (if you want to easily switch backends etc) and avoiding builtin structures like list. Here you probably just need to call:
