A HDF5 dataset uses at least a few kB of disk space while most ARTIQ datasets in practice are smaller than the minimum HDF5 dataset overhead and many (e.g. scalars) are even smaller than their key. This is an inefficient use of disk space.
Could we also add compression to the discussion? As was pointed out on #m-labs, there's support for compression filters for hdf5 datasets. I did some tests to get an impression on readily available and working filters, as well as crude performance.
It seems realistic to me to enable Zstandard compression by default on all array datasets (e.g. images), assuming that smaller data is stored as attributes. But this requires further testing, especially on Windows. More / better testing very welcome.
At least exposing h5py create_dataset options would be great.
