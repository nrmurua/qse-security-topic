I am trying to allocate two 32-qubit circuits in single precision in a 96GB machine. Each vector should need 32GB, making a total of 64 GB, which should fit in memory. However, when the code is run a memory error is raised. Is there any reason why this happens?
Please share a minimal example which raises the memory issue.
This is my script. The circuits are not exactly those, but the structure is.
@AdrianPerezSalinas I don't see anything wrong with Qibo, you are allocating 3 arrays with 2**32 * 2 * 32bit = 34GB each: state1, state1.copy() = state2 and the conj.dot needs another one for the computation, so ~102GB in total for the full computation.
If this is the standard behavior, I guess everything is fine. Somehow I expected the .dot() to not take much memory. Thanks!
Indeed, I also tried to execute this in our machines and the memory usage is as expected. The qibo part allocates two state vectors for state1 and state2 while the overlap calculation allocates another one. I have not checked if the additional vector is allocated due to np.conj or the .dot operation.
@AdrianPerezSalinas, I believe the issue here is whether it is possible to calculate the overlap without creating an additional copy. This is not directly qibo related, but programmatically speaking it should be possible. If the numpy approach you are using is not doing this, a simple work around is to write a numba operator for the overlap calculation:
I tested this in your example and the total memory footprint (including overlap calculation) is only two state vectors, while performance is also acceptable if you compile with parallel=True. This should solve the memory issue.
