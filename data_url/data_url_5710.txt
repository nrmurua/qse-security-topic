Running certain circuits (e.g. the QAOA expectation value of an 18-qubit SKModel instance, see below) with the GPU statevector simulator, AerPauliExpectation, and certain noise models (e.g. 2-qubit depolar error) causes segmentation fault.
Using the CPU statevector simulator does not have this issue.
Some other circuits, for example, the QAOA expectation value of the Maxcut problem, do not have this issue.
Some other noise models, including ideal simulations, Pauli errors, and 1-qubit depolar errors, do not have this issue.
16 qubits or below do not have this issue (the exact number may depend on machine specs).
The simulator should be able to evaluate the QAOA energy without causing segfault.
This issue is caused by gate fusion. Some operations inserted by noise model are fused and fused unitary matrix is larger than matrix buffer allocated on GPU. I will fix this issue by searching max matrix qubits after fusion
Until the fix will be released, please disable fusion by setting fusion_threshold=30 (larger than circuits qubits)
Thanks for the quick help! I can confirm that disabling fusion is an effective workaround.
I would like to create a new issue for segmentation faults due to massive cp gates.
