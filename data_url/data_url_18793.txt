It may be helpful to specify more exactly a procedure for casting angle types to float types.
Angles are fixed-point numbers that represent a value in the range (0,2*pi]. They are expected to be stored as a fixed-point binary value in the range (0,1.0], which is then multiplied by 2pi when casting to float type values.
The current casting specifics PR #268 says the following:
Casting to float[m] is accomplished by first converting the angle value to the nearest float, with ties going to the value with the even least significant bit, and then multiplying the resulting value by 2Ï€ as a float[m] value.
There are two options that the Types and Casting WG came up with for converting the angle value to the nearest float.
The questions are:
Types are pretty well standardized, IIRC:
Floats are 32 bits. Doubles are 64 bits. Quads are 128 bits, and Octuples are 256 bits.
The question of accuracy really depends on what resolution you expect QC hardware to get, bearing in mind that the Planck length is 1.6x10^-35 meters and can be represented in less than 128 bits which is the theoretical maximum accuracy we could achieve in the real world, an oct is probably overkill, a quad will be plenty, a double should be more than adequate for even the most high-accuracy work, and a float should work for the majority of applications.
I personally would vote to add double(), quad(), and oct() to the grammar and use IEEE-754 (or whatever the newest version they came up with is) for the actual implementation (Option 1) that way we're not re-inventing the wheel and it makes it easier to transfer data between languages, so if I have for instance, some C firmware accessing a quantum device I don't need to make helper functions just to encode/decode all the inputs/outputs and when my firmware is accessed by someones Python script, they don't need to do the same thing either.
Just my $0.02 on the subject.
Hi Noah,
QASM 3 as a language already supports all the IEEE 754 floating-point types (the names are float[16], float[32], ..., float[256] for half-, single-, ..., and octuple-precision).  The question here is about exactly how the QASM 3-specific type angle[n] will be converted to those IEEE floating-point types at a bit level; we need to have a completely rigorous definition, but one that is also fast and implementable on FPGA-type hardware, without unnecessary precision loss.  Since angle is not an IEEE 754 type, there are no currently defined semantics for it, which is what we're deciding here - they're particularly important, because as you say, high-level programming languages don't have a similar type, so we'll likely be exchanging floats.
At the moment we don't use the names double, quad, etc, because having several names doesn't fit as nicely with the "feel" of the language - integers take a width argument, for example, because we allow re-interpreting as bit arrays with first-class support - but it's possible we could reconsider it.  Just having the names as special tokens in the language won't affect interoperability, though, because the functionality is already there.
Also, this is a bit beside the point, but with floating-point, we're not just concerned with the maximum absolute scale.  For example, modern research physics almost invariably uses double-precision, not because the final answer needs to be at that precision, but to keep round-off errors at an acceptable level while in the intermediate stages (ML and GPU calculations mean that single-precision and lower is getting more use now, since they make different trade-offs).  A single-precision float can store the Planck length in meters to within about 1e-7 relative error, but the result of "1m + Planck length - 1m" would be exactly 0.  In fact, you'd need octuple-precision to get a non-zero answer, and you'd still have lost about half your binary digits of precision.  That said, most stuff absolutely can be done (and is!) with double or single, but we're don't want to just support the minimum, so the language uses can grow in the future.
Hello... I know it does not matter too much, but here are my thoughts...
I personally prefer the float[N] from working in hardware with C and having to handle hardware with strange number of bits for floats. I like how it is explicit and you do not have to come up with new words to increase the size. Also, it goes with the overall "theme" of the language.
With that being said, I think it would be good to have both options of casting available. Or, at least the issue should be addressed and spelled out. This same issue is in other languages like C. A literal is converted to the "best" floating point representation.
In most implementations I have seen, they choose loss of precision over unknown extra instructions to save precision. So, I would lean towards the default be a loss of precision and provide a specific operator or indicator to ensure better precision. In C, I have seen where people will explicitly encode the floating point number they want to ensure speed&accuracy. Something like that might be a useful addition as well.
Hope this helps some :)
Some folks concerned with this issue might be interested in QAMP Fall 2021 Team No. 23's ways of handling this which will be presented in our Oct. 7 session for QAMP checkpoint.
