The current implementation of Krotov only allows for propagation of Hilbert space kets in the optimization routine. In order to proceed to dissipative systems, propagation with density matrices and the Liouville-von Neumann equation needs to be added. Note that 'mesolve' routine from QuTiP already allows that, i.e., propagation outside the Krotov optimization is possible.
This is a prerequisite for #4, #12.
Can we add a notebook (or failing test) that shows the problem? This is the easiest way to implement the support for density matrices, as it then becomes a matter of just fixing a failing test.
See https://krotov.readthedocs.io/en/latest/contributing.html#how-to-commit-failing-tests-or-example-notebooks
The added example should provide a failing test in a separate issue branch.
I added some details regarding the physical model behind this example.
Note that in the end, we want to optimized for a target state on a subspace of Hilbert, respectively Liouville space. I guess that's already feasible with the possibility to pass one's own chi_constructor, right? However, I think it's easier to resolve the issue by the example in its current form and I will change it later on.
Thanks for the example! That looks great!
So it's just that the expm propagator is being overly careful and raises a NotImplementedError.  Since your H is actually a superoperator, in theory it should work already (if you comment out the line that raises the error). That would just exponentiate the superoperator and multiply it to the state. It looks like there's something tricky with the superoperator-state-multiplication that I haven't figure out yet (it's getting late). Maybe have a look at http://qutip.org/docs/3.1.0/guide/guide-states.html#superoperators-and-vectorized-operators to see if you can find out why it's complaining about incompatible shapes.
Incidentally, the liouvillian routine will make it really easy to also make the expm work for a Hamiltonian plus Lindblad operators.
I think the problem is what I would consider an oversight in QuTiP: qutip/qutip#939
So I committed the workarounds for the QuTiP issues, and now the optimization runs through.
However, the pulse update is zero. This is because of a mismatch between how QuTiP (and most people) define the Liouvillian, as ρ̇(t) = L[ρ(t)], whereas in QDYN we use ρ̇(t) = -i L[ρ(t)]. QDYN's convention is to make the equation of motion look the same in Liouville space and Hilbert space, which then makes the update equation look the same. However, the term (∂H/∂ϵ) in the update doesn't actually mathematically refer to the Hamiltonian, but to the r.h.s of the equation of motion, which must be in the form v̇(t) = -i H[v(t)] for an abstract state v and an abstract operator H. Since QuTiP is missing the -i, the resulting ⟨χ|(∂H/∂ϵ)|ψ⟩ is real, and when we then take the imaginary part, the update becomes zero.
Since we can't (or at least shouldn't) make very strong assumption about what a user might want to put in the objectives, we actually need to make μ≡∂H/∂ϵ a user-supplied parameter that defaults to the correct behavior for a Hamiltonian (or objective.H being a Liouvillian using QDYN's convention). If a user wants to store a Liouvillian in QuTiPs/most people's convention in objective.H (as we do in the example notebook), they will have to supply a custom μ function that multiplies with (i).
Having an optional user-supplied custom μ also opens up the possibility of any kind of "weird" equation of motion: as both the propagator and μ can be user-supplied, anything is possible (the two routines just have to be compatible).
So mu is now a user-defined function. Not really sure why the optimization isn't converging now.
The qdyn assumption for the equation of motion is against the standard convention of the whole community. I suggest that for the python implementation, we do adopt the standard (and qutip) convention. If the mu-workaround is sufficient, then that's fine, otherwise we need to modify our update rule.
So mu is now a user-defined function. Not really sure why the optimization isn't converging now.
The non-converging optimization is most likely due to my use of a preliminary functional in order to examine the (now solved) problem. I will check the the test with the correct functional and transform it afterwards into a proper example.
I agree completely that a user should only ever be confronted with the standard definition of the Liouvillian. Having to pass a custom mu function for open system dynamics wasn't really a permanent solution either, so I'm now handling Liouvillians correctly in the default mu function. The option for a custom mu is still there, but it would have to be an extremely unusual system where someone would need to use it - like something where the derivative still depended on the states, which I think doesn't even happen for Gross–Pitaevskii.
The more likely use case for supplying a custom mu function is probably efficiency: instead of the default, which carries out the derivative and constructs an explicit μ operator in every iteration, someone might want to supply a mu that directly applies the problem-specific, pre-calculated derivative.
Great! It sounds like a very good solution to have the standard for everyone and the 'mu' function for experts... Thanks, Michael!!
I agree completely that a user should only ever be confronted with the standard definition of the Liouvillian. Having to pass a custom mu function for open system dynamics wasn't really a permanent solution either, so I'm now handling Liouvillians correctly in the default mu function. The option for a custom mu is still there, but it would have to be an extremely unusual system where someone would need to use it - like something where the derivative still depended on the states, which I think doesn't even happen for Gross–Pitaevskii.
The more likely use case for supplying a custom mu function is probably efficiency: instead of the default, which carries out the derivative and constructs an explicit μ operator in every iteration, someone might want to supply a mu that directly applies the problem-specific, pre-calculated derivative.
I changed the example such that it coincides with the code I have in qdyn for the same problem. Unfortunately, the example is still not working correctly, i.e., Krotov is non-monotonic and the functional keeps fluctuating. I cross-checked the different steps of the Krotov iterations with the one's from qdyn. Forward propagation and construction of chi works correctly, however, the backwards propagated chi differs from what qdyn returns. I spent some time trying to figure out where the problem might be but haven't found any hint to far. Maybe it is due to usage of expm for the superoperator of the Liouvillian or something like that.
Note that right now, for the sake of debugging, I'm even propagating without Lindblad operators, so the time evolution is unitary.
Probably a bug in using the adjoint of the Lindbladian in the propagation correctly. Could you extract the result of the backward propagation from QDYN into a file and add a test to Krotov? See the tlist_control fixture in test_objectives.py for an example on how to read data from a file in a test.
Looks good to me! Can I merge the branch into master, and close #22 and #4?
