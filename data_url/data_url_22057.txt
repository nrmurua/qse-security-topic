Compilation of multidimensional arrays that will be accessed by kernel functions is very slow and super-linear in time.
May have some overlap with issue  #709, but occurs when defining the list as an attribute rather than only when passing a list as an argument to the function.
A one dimensional list with 5000*16=80000 elements takes ~1s to compile.
Two dimensional list with 5000 by 16 elements takes >20s, versus <5s for 2500 by 16.
Strangely, three dimensional list with 100 by 50 by 16 takes 5s, less than the two dimensional case with same total number of elements
This is basically expected because of Python's dynamic typing. @sbourdeauducq, can we say that only large numpy arrays will be compiled quickly?
We can do better than that. Checking that all the elements in that array have the same type takes 36ms on my slow Intel Atom computer.
Casting as numpy array speeds things up a little but still slow and non-linear
The numpy array type inference being slow is a bug, the Python array type inference being slow is more or less by design. I'll think about what I can do.
We'd be happy if explicitly casting with np.asarray makes it work - no problem if the Python type inferencing remains slow.
It should be a lot faster after #1709. The current time is:
