Currently it only works for LinearOperators, but it should also work for sparse matrices, right?
Absolutely, mostly within 20 iterations for dimension at the order of 10^6 or even higher.
Actually it works for not only sparse matrices, but also for more general matrices when they're diagonal dominant, so that the approximation (H - \lambda I)^{-1} \approx (D_H - \lambda I)^{-1} is valid.
-- Seems no easy way to insert equations in discussion :(
Davidson's key inputs are:
With a sparse matrix as the starting point, I'd suggest converting it to a LinearOperator as well, just as the QubitOperator case, and then they can share the same underlying Davidson Algo implementation, which had better be decoupled for clarity.
