Currently, the top-level compiler driver of CUDA Quantum (nvq++) is a bash script invoking various other tools from CUDA Quantum as well as LLVM.
For example, an MLIR-enabled compilation pipeline generated by nvq++ looks like this:

This scripting approach has pros and cons as described in #505.
High-level diagram

Design considerations/ideas:
(1) Utilize Clang's Driver facility to drive the main compilation workflow
nvq++ driver to contain a clang::driver::Driver object, i.e., no need to modify the LLVM source code.
Pre-filter command line arguments to handle CUDA Quantum-specific args, the rest will be forwarded to the clang driver to build the compilation pipeline (e.g., with or without the link step, which frontend action (full object code compilation or just LLVM IR) was requested, etc.)
(2) Intercept the frontend tool (cc1) of clang driver (CC1Main) to inject our frontend pipeline.
Utilizing the same dual AST consumer strategy as cudaq-quake (CudaQAction) but also taking into account the frontend action requested (e.g., the most common one is object code compilation, EmitObjAction) to construct the conventional AST consumer.
For the Quake path (the bridge), IR optimization, lowering, and codegen are handled internally, without the need to serialize/deserialize the IR via file I/O.
#600
I'd like a bit more clarity on what we mean by "using the driver".
We have to be able to compile "regular, classical C++" and the "CUDA Quantum C++" and should plan on having "regular CUDA" kernels as well. How does that fit together. One important observation is that CUDA Quantum kernels necessarily affect the "regular" C++ code that will be generated. How does that fit in the proposed pipeline without changes to clang/LLVM?
Code/compile/test cycle impact? Building and linking clang (with debug) is a resource hungry and time sinking proposition. Our current setup with separable small tools attenuates that cost by keeping the libraries linked against fairly distinct. What sort of impact does it have on build times and disk space to build everything into one big enchilada? Corollary: what happens to our existing tools, which some may find to be highly productive in terms of development and testing?
By the way, thanks for getting this discussion started, Thien.
Thanks for all the feedback, Eric!
That's right. We're proposing to build a nvq++ driver, wrapping around a clang driver.
This design is actually 'inspired' by cudaq-quake in the sense that it aims to extend the cudaq-quake to handle an end-to-end compilation workflow (bridge, optimization, lowering, codegen).
As mentioned in the draft PR, it's not intended to replace all those tools (cudaq-quake, cudaq-opt, cudaq-translate) but to refactor the code so that nvq++ can re-use the steps (e.g., constructing the pass pipeline). This will make the compilation more efficient (no need to read/write the IR to file) while still keeping those sub-tools for other purposes, such as unit testing for example.
I don't have a good answer for this TBH. My initial thoughts were that we don't want to incur the labor/resource cost of maintaining a Clang/LLVM downstream.  I guess we could follow that approach and it will probably produce a more performant/efficient solution.
The current nvq++ bash script is functioning as a full compiler driver (e.g., also performs linking to generate executable/libraries, etc.), hence I was thinking a clang-driver based solution is leaner in the sense that we don't need to have another wrapper layer.
The first two, I guess, are straightforward for the current bash and the proposed clang-driver based solutions. I don't know enough about CUDA kernel compilation to give a clear answer for the last one. On one hand, we might use the built-in CUDA support in Clang to get a quick start. On the other hand, figuring out a solution to incorporate nvcc into the workflow requires a separate, more extensive design effort I'd think.
One important observation is that CUDA Quantum kernels necessarily affect the "regular" C++ code that will be generated. How does that fit in the proposed pipeline without changes to clang/LLVM?
From what I understand, the "regular" C++ code is currently handled by the default LLVM frontend action. If later on we need to customize this path, one solution is to implement a different frontend action then used it in the compilation workflow.
For development and testing purposes, it's worthwhile keeping those tools. We would refactor the code in those tools (e.g., minus the file I/O) so that nvq++ can reuse it as described in the TODO list of the draft PR, hence amortizing the build time somewhat. Regarding linking against clang, we could follow the same build configuration (no debug info) to save disk space as we don't modify clang code itself. We didn't need to build any extra LLVM components to enable this Clang-wrapper solution since all the necessary dependencies have been built as LLVM dependencies.
Sorry for the belated reply. Thanks for your detailed answers, Thien.
We will need some way to resolve the linking issues. (These arise because the quantum compiler replaces entry point kernels, by name, with routines that invoke the quantum kernel via the runtime. These functions do not necessarily have a linkage that works "out of the box".)
As far as mixing CPU, GPU, and QPU code together, we need to have some design discussions to bring that more into focus.
We will need some way to resolve the linking issues. (These arise because the quantum compiler replaces entry point kernels, by name, with routines that invoke the quantum kernel via the runtime. These functions do not necessarily have a linkage that works "out of the box".)
I've also noticed the extra step that we did to prevent the potential multi-definition linkage errors.
I think there are a couple of possible alternatives in the proposed workflow:
(1) Inject -z muldefs to the linking step (along with other link flags that we need to add anyway) and put the Quake object file before the 'classic' one to make sure the overridden entry point function is picked up by the linker.
(2) Weaken or completely remove the symbol of the entry point function in the classic object code using the llvm-objcopy tool  (--weaken-symbol or --strip-symbol). Weakening the symbol is somewhat equivalent to what we did at the .ll file level before recompiling it. Completely removing the symbol, on the other hand, could potentially help to prevent subtle bugs when the library mode code is silently selected if there are symbol mismatches (like #577).
Depends on the conclusion for #505.
